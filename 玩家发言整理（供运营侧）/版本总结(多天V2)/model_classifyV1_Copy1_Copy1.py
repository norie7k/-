from __future__ import annotations
import json, time, typing as T
import pandas as pd
import requests
import math
from collections import defaultdict
from pathlib import Path
from typing import List, Dict, Any, Optional
import re, unicodedata
from datetime import datetime
from json import JSONDecodeError

# è¿›åº¦æ‰“å°ï¼ˆparse_model2_output_to_json_list ç”¨åˆ°ï¼‰
from tqdm import tqdm

# --- openpyxl æ ·å¼/å·¥å…· ---
from openpyxl import Workbook, load_workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, NamedStyle

# --- docx æ ·å¼/å·¥å…· ---
from docx.oxml import OxmlElement
from docx import Document
from docx.shared import Pt
from docx.oxml.ns import qn


################ æ¨¡å‹è°ƒç”¨ï¼Œå‡ºç»“æœ ###################

def load_system_prompt(path: Path) -> str:
    if not path.exists():
        raise FileNotFoundError(f"Prompt file not found: {path}")
    return path.read_text(encoding="utf-8")


def build_user_prompt_filter(json_lines: T.List[str]) -> str:
    # æ¨¡å‹#1ï¼šç­›æ‰éæ¸¸æˆç›¸å…³ï¼Œåªè¾“å‡ºç›¸å…³ JSON è¡Œï¼ˆåŸæ ·ï¼‰
    return (
        "ä»¥ä¸‹æ˜¯è‹¥å¹²ç©å®¶/å®¢æœ/ç ”å‘çš„å‘è¨€è®°å½•ï¼Œè¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­è§„åˆ™ï¼Œ"
        "åˆ¤æ–­å“ªäº›æ˜¯ã€ä¸æ¸¸æˆå†…å®¹ç›¸å…³ã€‘çš„å‘è¨€ï¼Œä¿ç•™è¿™äº› JSON è¡Œï¼Œä¸ç›¸å…³çš„å¿½ç•¥ã€‚"
        "è¯·ä»…è¾“å‡ºã€ç›¸å…³å‘è¨€çš„åŸå§‹ JSON è¡Œã€‘ï¼Œä¸¥æ ¼ä¿æŒæ ¼å¼ä¸å˜ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + "\n".join(json_lines)
    )


def build_user_prompt_clsuter(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )


def build_user_prompt_cluster_agg(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )


def build_user_prompt_version_agg(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )


def build_user_prompt_subcluster_opinion(
    topic_id: str,
    discussion_point: str,
    dialogs: List[Dict[str, Any]],
) -> str:
    """
    æ¨¡å‹#4ï¼ˆè§‚ç‚¹åˆ†æï¼‰ç”¨æˆ·æç¤ºè¯æ„é€ ï¼š
    ç¬¬ä¸€è¡Œæ˜¯è¯é¢˜ç°‡å…ƒä¿¡æ¯ï¼Œå…¶ä½™æ˜¯åŸå§‹å‘è¨€ JSON è¡Œã€‚
    """
    lines: List[str] = []

    meta = {
        "è¯é¢˜ç°‡ID": topic_id,
        "è®¨è®ºç‚¹": discussion_point,
    }
    lines.append(json.dumps(meta, ensure_ascii=False))

    for row in dialogs:
        lines.append(json.dumps(row, ensure_ascii=False))

    jsonl_block = "\n".join(lines)

    return (
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘æœ¬æ¬¡æ‰€æœ‰æ•°æ®ï¼ˆJSONLï¼Œç¬¬ä¸€è¡Œä¸ºè¯é¢˜ç°‡ä¿¡æ¯ï¼Œå…¶ä½™ä¸ºå‘è¨€ï¼‰ï¼š\n"
        + jsonl_block +
        "\n\nä»…æ ¹æ®ä»¥ä¸Šå†…å®¹ï¼Œå¹¶éµå¾ªç³»ç»Ÿæç¤ºè¯ä¸­çš„è§„åˆ™å’Œè¾“å‡ºæ ¼å¼ï¼Œ"
        "ç›´æ¥è¾“å‡º 1 ä¸ª JSON å¯¹è±¡ä½œä¸ºæœ€ç»ˆç»“æœã€‚"
    )

def build_user_prompt_version_opinion(
    discussion_point: str,
    json_lines: List[Any],   # å®é™…ä¼ çš„æ˜¯ list[str]
) -> str:
    dp = (discussion_point or "").strip()
    jsonl_block = "\n".join(
        (line or "").strip() for line in json_lines if line
    )
    return (
        "ä½ å°†æ”¶åˆ°ä¸€æ®µç©å®¶èŠå¤©åŸæ–‡ï¼ˆJSONLï¼Œæ¯è¡Œä¸€æ¡ï¼‰ã€‚æœ¬æ¬¡åªåˆ†ææŒ‡å®šã€è®¨è®ºç‚¹ã€‘ï¼Œå…¶ä»–å†…å®¹å¿½ç•¥ã€‚\n\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡º1ä¸ªJSONå¯¹è±¡ï¼Œç¦æ­¢è§£é‡Šæ–‡å­—ï¼Œç¦æ­¢Markdownä»£ç å—ã€‚\n\n"
        "ã€è¾“å…¥è¯é¢˜ç‚¹ã€‘ï¼š\n" + dp + "\n\n"
        "ã€è¾“å…¥åŸæ–‡JSONLã€‘\n" + jsonl_block
    )
def build_user_prompt_heat_trend(jsonl_block: str) -> str:
    """
    ç»™æ¨¡å‹#7 çš„ç”¨æˆ·æç¤ºè¯ï¼š
    - jsonl_block å¯ä»¥æ˜¯ä¸€è¡Œæˆ–å¤šè¡Œ JSONL
    - æˆ‘ä»¬ç°åœ¨æ˜¯ä¸€è¡Œä¸€è·‘ï¼Œæ‰€ä»¥å®é™…å°±æ˜¯ä¸€è¡Œ
    """
    return (
        "ä»¥ä¸‹æ˜¯ç‰ˆæœ¬å‘è¨€Top5ä¸­çš„ä¸€ä¸ªã€èšåˆè¯é¢˜ç°‡ã€‘åŠå…¶è®¨è®ºç‚¹é‡åŒ–ä¿¡æ¯ï¼Œ"
        "æ ¼å¼ä¸º JSONLï¼ˆæ¯è¡Œä¸€ä¸ªå¯¹è±¡ï¼Œæœ¬æ¬¡åªæœ‰ä¸€è¡Œï¼‰ã€‚\n\n"
        "è¯·ä¸¥æ ¼æŒ‰ç…§ç³»ç»Ÿæç¤ºè¯ã€Šçƒ­åº¦è¶‹åŠ¿æ™ºèƒ½ä½“ã€‹çš„è¦æ±‚ï¼Œåªè¾“å‡º 1 ä¸ª JSON å¯¹è±¡ï¼š"
        "åŒ…å«å­—æ®µã€èšåˆè¯é¢˜ç°‡ã€‘å’Œã€ä¸€å¥è¯æ€»ç»“ã€‘ï¼Œ"
        "ç¦æ­¢è¾“å‡º Markdown ä»£ç å—ï¼Œç¦æ­¢è§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘\n" + jsonl_block.strip()
    )

def call_ark_chat_completions(
    api_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    temperature: float = 0.3,
    max_tokens: int = 32700,
    timeout: int = 600,
    retries: int = 2,
) -> str:
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    last_err = None
    for attempt in range(retries + 1):
        try:
            resp = requests.post(api_url, headers=headers, json=payload, timeout=timeout)
            if resp.status_code != 200:
                raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            last_err = e
            time.sleep(1.2 * (attempt + 1))
    raise RuntimeError(f"Ark API è°ƒç”¨å¤±è´¥: {last_err}")


def extract_valid_json_lines(text: str) -> T.List[str]:
    """
    æŠŠæ¨¡å‹è¾“å‡ºé‡Œçš„çº¯ JSON è¡Œæå–å‡ºæ¥ï¼ˆé²æ£’å¤„ç†ï¼‰ï¼š
    - é€è¡Œåˆ¤æ–­ï¼šä»¥ { å¼€å¤´ ä¸” ä»¥ } ç»“å°¾ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ª JSON å¯¹è±¡è¡Œ
    - ä¹Ÿèƒ½å®¹å¿å‰åå¤šä½™ç©ºè¡Œæˆ–è§£é‡Šæ–‡å­—ï¼ˆä¼šè¢«å¿½ç•¥ï¼‰
    """
    lines = []
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith("{") and s.endswith("}"):
            lines.append(s)
    return lines


def add_index_to_jsonl_lines(jsonl_lines: List[str]) -> List[str]:
    """
    ç»™åŸå§‹ jsonl æ¯æ¡å‘è¨€åŠ ä¸Šä¸€ä¸ªå”¯ä¸€è¡Œå·å­—æ®µ `_idx`ï¼Œ
    è¿”å›æ–°çš„ List[str]ï¼Œæ¯ä¸ªå…ƒç´ ä»ç„¶æ˜¯ä¸€è¡Œ JSON å­—ç¬¦ä¸²ã€‚
    """
    new_lines = []
    for idx, line in enumerate(jsonl_lines, start=1):
        line = line.strip()
        if not line:
            continue
        obj = json.loads(line)
        obj["_idx"] = idx  # æ–°å¢è¡Œå·
        new_lines.append(json.dumps(obj, ensure_ascii=False))
    return new_lines


def count_output_filter_stats(output_filter: str):
    """
    ç»Ÿè®¡æ¨¡å‹#1 è¾“å‡ºä¸­çš„ï¼š
    - æ€»è¡Œæ•° total_linesï¼ˆç©å®¶+å®¢æœ+ç ”å‘ï¼‰
    - ç©å®¶å‘è¨€è¡Œæ•° player_linesï¼ˆç®€å•ç”¨ å‘è¨€äººID é‡Œçš„å…³é”®è¯åŒºåˆ†ï¼‰
    """
    total_lines = 0
    player_lines = 0

    for line in output_filter.splitlines():
        line = line.strip()
        if not line:
            continue
        total_lines += 1

        try:
            obj = json.loads(line)
        except Exception:
            # å¦‚æœè¿™ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±åªèƒ½ç®—è¿› total_linesï¼Œæ— æ³•ç»†åˆ†
            continue

        speaker = (
            obj.get("ç©å®¶ID")
            or obj.get("å‘è¨€äººID")
            or obj.get("è§’è‰²ID")
            or ""
        )

        # ç®€å•æ’é™¤å®¢æœ/GM/å®˜æ–¹/è¿è¥/ç ”å‘ï¼Œå…¶ä½™è§†ä¸ºç©å®¶
        if any(key in str(speaker) for key in ["å®¢æœ", "GM", "å®˜æ–¹", "è¿è¥", "ç ”å‘"]):
            continue

        player_lines += 1

    return total_lines, player_lines


def get_covered_indices_from_cluster_output(output_cluster: str):
    """
    ä»æ¨¡å‹#2 çš„è‡ªç„¶è¯­è¨€è¾“å‡ºä¸­ï¼Œè§£ææ‰€æœ‰ â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[...æ•°å­—...]â€ é‡Œçš„æ•°å­—ï¼Œ
    æ”¶é›†æˆä¸€ä¸ª set è¿”å›ã€‚

    é€‚é…ç±»ä¼¼æ ¼å¼ï¼š
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[88, 90]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[124,125,126]
    """
    covered = set()

    # æ­£åˆ™åŒ¹é… â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]â€
    pattern = r"å‘è¨€è¡Œå·åˆ—è¡¨[:ï¼š]\s*[\[\ã€]([0-9,\sï¼Œ]+)[\]\ã€‘]"

    for m in re.finditer(pattern, output_cluster):
        nums_str = m.group(1)  # é‡Œé¢æ˜¯ç±»ä¼¼ "1, 2, 3" æˆ– "124,125,126"
        # æ›¿æ¢ä¸­æ–‡é€—å·ï¼ŒæŒ‰é€—å·åˆ‡åˆ†
        nums_str = nums_str.replace("ï¼Œ", ",")
        for part in nums_str.split(","):
            p = part.strip()
            if not p:
                continue
            try:
                covered.add(int(p))
            except ValueError:
                continue

    return covered


################################ ä¿®è¡¥ bug â€” è¯é¢˜ç°‡åˆ’åˆ†è§£æ #########################

def _normalize_json_text(text: str) -> str:
    """
    å¯¹æ¨¡å‹è¾“å‡ºåšä¸€äº›å°ä¿®å¤ï¼Œä»¥ä¾¿ json.loads æ­£å¸¸è§£æï¼š
    1ï¼‰å»æ‰æ•´æ®µæœ«å°¾å¤šä½™çš„é€—å·ï¼š{"a":1},
    2ï¼‰å»æ‰ å±æ€§/å…ƒç´  åç´§è·Ÿ } æˆ– ] çš„éæ³•é€—å·ã€‚
    """
    text = text.rstrip()

    # 1) æ•´ä¸ªå¯¹è±¡æœ«å°¾å¤šæ‰“ä¸€é¢—é€—å·ï¼š{"a":1},
    if text.endswith(","):
        text = text[:-1].rstrip()

    # 2) å±æ€§/å…ƒç´ åç›´æ¥è·Ÿ } æˆ– ] çš„é€—å·
    text = re.sub(r",\s*(\n\s*[}\]])", r"\1", text)

    return text


def parse_model2_output_to_json_list(output_cluster: str, batch_idx: int = 0) -> List[Dict[str, Any]]:
    """
    æŠŠæ¨¡å‹#2çš„åŸå§‹è¾“å‡ºè§£ææˆ list[dict]ï¼š
    - æ”¯æŒä¸€è¡Œä¸€ä¸ª JSONï¼š
        {"è¯é¢˜ç°‡1": "...", "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "..."}
    - ä¹Ÿæ”¯æŒå¤šè¡Œæ¼‚äº® JSONï¼š
        {
          "è¯é¢˜ç°‡1": "...",
          "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "..."
        }
    - è‡ªåŠ¨å¿½ç•¥è§£é‡Šæ€§æ–‡å­—ã€markdown åˆ—è¡¨ "- {...}"
    - è°ƒç”¨ _normalize_json_text ä¿®å¤å°¾é€—å·ç­‰æ ¼å¼é—®é¢˜
    - è§£æå¤±è´¥æ—¶æ‰“å°å®Œæ•´å¯¹è±¡åŸæ–‡ï¼Œä½†ä¸ä¸­æ–­æ•´æ‰¹
    """
    objs: List[Dict[str, Any]] = []
    cur_lines: List[str] = []
    depth = 0  # å½“å‰å¤§æ‹¬å·åµŒå¥—å±‚æ•°

    lines = output_cluster.strip().splitlines()

    for idx, raw in enumerate(lines, start=1):
        line = (raw or "").rstrip()
        if not line.strip():
            continue

        # å»æ‰ markdown åˆ—è¡¨å‰ç¼€ï¼ˆå¸¸è§æ ¼å¼ï¼š"- {...}"ï¼‰
        if line.lstrip().startswith("- "):
            line = line.lstrip()[2:].lstrip()

        # å½“å‰è¿˜æ²¡è¿›å…¥ä»»ä½• JSON å¯¹è±¡ï¼Œå¹¶ä¸”è¿™ä¸€è¡Œä¹Ÿçœ‹ä¸åˆ° "{"
        if depth == 0 and "{" not in line:
            # å¤§æ¦‚ç‡æ˜¯è§£é‡Šæ€§æ–‡å­—ï¼Œæ¯”å¦‚ "ä»¥ä¸‹æ˜¯è¯é¢˜ç°‡..."
            continue

        # ç»Ÿè®¡è¿™ä¸€è¡Œçš„å¤§æ‹¬å·æ•°é‡
        open_cnt = line.count("{")
        close_cnt = line.count("}")

        if depth == 0:
            # åˆšåˆšè¿›å…¥ä¸€ä¸ªæ–°çš„å¯¹è±¡
            cur_lines = [line]
            depth = open_cnt - close_cnt

            # depth <= 0 è¯´æ˜è¿™æ˜¯â€œå•è¡Œå¯¹è±¡â€ï¼š{"a":1} æˆ– {"a":1},
            if depth <= 0:
                text = "\n".join(cur_lines)
                text = _normalize_json_text(text)
                try:
                    obj = json.loads(text)
                except JSONDecodeError as e:
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] âŒ JSONè§£æå¤±è´¥ï¼ˆè¡Œ#{idx}é™„è¿‘ï¼‰ï¼š{e}")
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] è¯¥å¯¹è±¡åŸæ–‡ï¼š{text}")
                else:
                    # ç»Ÿä¸€ "è¯é¢˜ç°‡1"/"è¯é¢˜ç°‡2"... -> "è¯é¢˜ç°‡"
                    for key in list(obj.keys()):
                        if key.startswith("è¯é¢˜ç°‡") and key != "è¯é¢˜ç°‡":
                            obj["è¯é¢˜ç°‡"] = obj.pop(key)
                    objs.append(obj)
                cur_lines = []
                depth = 0
        else:
            # å·²ç»åœ¨ä¸€ä¸ª JSON å¯¹è±¡å†…éƒ¨ï¼ˆå¤šè¡Œåœºæ™¯ï¼‰
            cur_lines.append(line)
            depth += open_cnt - close_cnt

            # depth å›åˆ° 0ï¼Œè¡¨ç¤ºä¸€ä¸ªå¯¹è±¡ç»“æŸ
            if depth <= 0:
                text = "\n".join(cur_lines)
                text = _normalize_json_text(text)

                try:
                    obj = json.loads(text)
                except JSONDecodeError as e:
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] âŒ JSONè§£æå¤±è´¥ï¼ˆè¡Œ#{idx}é™„è¿‘ï¼‰ï¼š{e}")
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] è¯¥å¯¹è±¡åŸæ–‡ï¼š{text}")
                else:
                    for key in list(obj.keys()):
                        if key.startswith("è¯é¢˜ç°‡") and key != "è¯é¢˜ç°‡":
                            obj["è¯é¢˜ç°‡"] = obj.pop(key)
                    objs.append(obj)

                cur_lines = []
                depth = 0

    return objs


def fix_model3_line_extreme_axis(s: str) -> str:
    """
    ä¸“é—¨ä¿®å¤æ¨¡å‹#3è¾“å‡ºä¸­â€œæè½´â€ç›¸å…³çš„åæ ¼å¼ï¼š
    1) "æ—¥æœŸ": "æè½´": "2025-12-06"  =>  "æ—¥æœŸ": "2025-12-06"
    2) "æ—¶é—´è½´": "æè½´": "22:30:57-22:33:57"  =>  "æ—¶é—´è½´": "22:30:57-22:33:57"
    3) "æ—¶é—´è½´": "22:34:24-æè½´": "22:38:33" => "æ—¶é—´è½´": "22:34:24-22:38:33"
    """
    s = re.sub(r'"æ—¥æœŸ"\s*:\s*"æè½´"\s*:\s*"([^"]+)"', r'"æ—¥æœŸ": "\1"', s)
    s = re.sub(r'"æ—¶é—´è½´"\s*:\s*"æè½´"\s*:\s*"([^"]+)"', r'"æ—¶é—´è½´": "\1"', s)
    s = re.sub(r'"æ—¶é—´è½´"\s*:\s*"([^"]*?)-æè½´"\s*:\s*"([^"]+)"', r'"æ—¶é—´è½´": "\1-\2"', s)
    return s


def parse_jsonl_text_safe(text: str, label: str = "æ¨¡å‹#3èšåˆè¾“å‡º") -> List[Dict[str, Any]]:
    """
    å°è¯•æŠŠ text æŒ‰è¡Œè§£æä¸º JSON å¯¹è±¡ï¼š
    - æ¯è¡Œç‹¬ç«‹å°è¯• json.loads
    - è§£æå‰å…ˆå¯¹â€œæè½´â€ç­‰å¸¸è§é”™è¯¯åšä¸€æ¬¡æ­£åˆ™ä¿®å¤
    - è§£æå¤±è´¥æ—¶ä¸ä¼šæŠ›å¼‚å¸¸ï¼Œè€Œæ˜¯æ‰“å°å‡ºå…·ä½“è¡Œå·å’ŒåŸæ–‡ï¼Œæ–¹ä¾¿æ’æŸ¥
    """
    objs: List[Dict[str, Any]] = []
    lines = text.strip().splitlines()

    for idx, raw in enumerate(lines, start=1):
        s = (raw or "").strip()
        if not s:
            continue

        # è·³è¿‡ ```json / ``` ç­‰ä»£ç å—æ ‡è®°
        if s.startswith("```"):
            continue

        # â­ å…ˆä¿®å¤â€œæè½´â€ç›¸å…³æ ¼å¼é”™è¯¯
        s_fixed = fix_model3_line_extreme_axis(s)

        try:
            obj = json.loads(s_fixed)
        except JSONDecodeError as e:
            print(f"[{label}] âŒ JSONè§£æå¤±è´¥ï¼šè¡Œ#{idx} -> {e}")
            print(f"[{label}] è¯¥è¡ŒåŸæ–‡(ä¿®å¤å)ï¼š{s_fixed}")
            continue

        objs.append(obj)

    return objs


################################# è¯é¢˜ç°‡å”¯ä¸€ ID #################################

def infer_date_for_batch(
    cluster_json_list: List[Dict[str, Any]],
    batch_lines: List[str]
) -> str:
    """
    ä¼˜å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜ä¸­æŠ½å–æ—¥æœŸï¼ˆå½¢å¦‚ï¼šxxxxï¼ˆ2025-12-02 16:30:01-17:42:41ï¼‰ï¼‰
    è‹¥å¤±è´¥ï¼Œåˆ™å›é€€åˆ°åŸå§‹å‘è¨€ä¸­çš„ `å‘è¨€æ—¥æœŸ` / `æ—¥æœŸ` å­—æ®µã€‚
    """
    # 1ï¼‰å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜é‡Œæ‰¾ YYYY-MM-DD
    for obj in cluster_json_list:
        title = str(
            obj.get("è¯é¢˜ç°‡")
            or obj.get("èšåˆè¯é¢˜ç°‡")
            or ""
        )
        m = re.search(r"(\d{4}-\d{2}-\d{2})", title)
        if m:
            return m.group(1)

    # 2ï¼‰å¦‚æœæ ‡é¢˜é‡Œæ²¡æœ‰ï¼Œå°±ä»åŸå§‹å‘è¨€é‡Œæ‹¿
    for line in batch_lines:
        line = line.strip()
        if not line:
            continue
        try:
            msg = json.loads(line)
        except Exception:
            continue

        date_str = msg.get("å‘è¨€æ—¥æœŸ") or msg.get("æ—¥æœŸ")
        if date_str:
            return date_str

    # 3ï¼‰å†ä¸è¡Œå°±æŠ¥é”™ï¼Œè®©ä½ æ„è¯†åˆ°æ•°æ®æ ¼å¼æœ‰é—®é¢˜
    raise ValueError("æ— æ³•ä»è¯é¢˜ç°‡æ ‡é¢˜æˆ–åŸå§‹å‘è¨€ä¸­æ¨æ–­å‡ºæ—¥æœŸï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼ã€‚")


def assign_global_cluster_ids(cluster_list: List[Dict[str, Any]], date_str: str, batch_id: int):
    """
    ä¸ºæ¯ä¸ªè¯é¢˜ç°‡ç”Ÿæˆå…¨å±€å”¯ä¸€IDå­—æ®µ `_cluster_id`
    æ ¼å¼ï¼šYYYY-MM-DD_BX_XXï¼Œå¦‚ 2025-11-20_B2_03
    """
    for idx, cluster in enumerate(cluster_list, start=1):
        cluster["_cluster_id"] = f"{date_str}_{batch_id}_{idx:02d}"
    return cluster_list


################################# èšåˆæ¯å¤©çš„è¯æç°‡åˆ†æ‰¹è¾“å‡º #################################

def aggregate_cluster_outputs(batch_outputs: List[str]) -> str:
    """
    æŠŠå¤šæ‰¹æ¨¡å‹#2 è¾“å‡ºèšåˆæˆä¸€ä¸ª jsonl å­—ç¬¦ä¸²ã€‚
    """
    all_lines: List[str] = []

    for _, text in enumerate(batch_outputs, start=1):
        if not text:
            continue

        for line in text.strip().splitlines():
            line = line.strip()
            if not line:
                continue

            try:
                obj = json.loads(line)
            except Exception:
                # å¦‚æœæœ‰ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±è·³è¿‡
                continue

            clean_line = json.dumps(obj, ensure_ascii=False)
            all_lines.append(clean_line)

    # èšåˆä¸ºä¸€ä¸ªå¤§çš„ JSONL å­—ç¬¦ä¸²
    return "\n".join(all_lines)


################ èšåˆä¿®å¤â€œæè½´â€ #############################


def _strip_fences(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
    s = re.sub(r"\s*```$", "", s)
    return s.strip()


def _as_list(v):
    if v is None:
        return []
    if isinstance(v, list):
        return v
    return [v]


def _merge_time_axes(time_axes: List[str]) -> str:
    """
    å¤šä¸ªæ—¶é—´è½´å»é‡+ç”¨é¡¿å·æ‹¼æ¥ï¼š"a-bã€c-d"
    """
    seen, out = set(), []
    for t in time_axes:
        t = (t or "").strip()
        if not t or t in seen:
            continue
        seen.add(t)
        out.append(t)
    return "ã€".join(out)


def clean_time_axis(raw: str) -> str:
    """
    æ¸…æ´—æ—¶é—´è½´å­—ç¬¦ä¸²ï¼š
    - åªä¿ç•™ï¼šæ•°å­—ã€å†’å·ã€æ¨ªæ ã€é¡¿å·ã€ç©ºç™½
    - å»æ‰ 'æè½´' ç­‰å¥‡æ€ªå­—ç¬¦
    """
    if not raw:
        return ""
    return re.sub(r"[^\d:ã€\-\s]", "", str(raw))


# åŒ¹é…ï¼šâ€¦â€¦ï¼ˆ2025-12-07 22:40:36-22:41:27ï¼‰ æˆ– â€¦â€¦(2025-12-07 22:40:36-22:41:27)
# ä»¥åŠâ€œ2025-12-07 22:40:36-22:41:27â€è¿™ç§ä¸å¸¦æ‹¬å·çš„å†™æ³•
SUB_TIME_RE = re.compile(
    r"(?:[ï¼ˆ(]\s*)?(\d{4}-\d{2}-\d{2})\s+"
    r"(\d{2}:\d{2}:\d{2}-\d{2}:\d{2}:\d{2})(?:\s*[ï¼‰)])?"
)


def enrich_subclusters_with_datetime(parsed_subclusters: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    ç»™æ¯æ¡å­ç°‡è¡¥å­—æ®µï¼š
    - æ—¥æœŸ: YYYY-MM-DD
    - æ—¶é—´è½´: HH:MM:SS-HH:MM:SS

    æŠ½å–é¡ºåºï¼š
    1ï¼‰ä¼˜å…ˆä» å­ç°‡["è¯é¢˜ç°‡"] å°¾éƒ¨æ‹¬å·é‡Œçš„ï¼ˆæ—¥æœŸ+æ—¶é—´èŒƒå›´ï¼‰
    2ï¼‰å¦‚æœæ²¡æœ‰ï¼Œå†å°è¯•ä» å­ç°‡["å‘è¨€æ—¶é—´"] é‡ŒæŠ½å–ï¼ˆæ”¯æŒ "2025-12-06 22:34:24-22:38:33"ï¼‰
    3ï¼‰è¿˜æ²¡æœ‰ï¼Œå°±ä» _cluster_id é‡Œå–æ—¥æœŸï¼ˆæ—¶é—´è½´ç•™ç©ºï¼‰
    """
    out: List[Dict[str, Any]] = []

    for sc in parsed_subclusters:
        sc2 = dict(sc)

        # 1) å…ˆçœ‹æ ‡é¢˜
        title = sc2.get("è¯é¢˜ç°‡") or ""
        m = SUB_TIME_RE.search(str(title))
        if m:
            sc2["æ—¥æœŸ"] = m.group(1)
            sc2["æ—¶é—´è½´"] = m.group(2)
        else:
            # 2) å†çœ‹å‘è¨€æ—¶é—´
            ft = (sc2.get("å‘è¨€æ—¶é—´") or "").strip()
            m2 = SUB_TIME_RE.search(ft)
            if m2:
                sc2["æ—¥æœŸ"] = m2.group(1)
                sc2["æ—¶é—´è½´"] = m2.group(2)
            else:
                # 3) å…œåº•ï¼šæ—¥æœŸå¯ä» _cluster_id é‡Œå–åˆ°ï¼ˆæ—¶é—´è½´å–ä¸åˆ°ï¼‰
                cid = (sc2.get("_cluster_id") or "").strip()
                if cid and re.match(r"^\d{4}-\d{2}-\d{2}_", cid):
                    # ä½ çš„ _cluster_id æ˜¯ 2025-12-07_B4_06ï¼Œå‰10ä½å°±æ˜¯æ—¥æœŸ
                    sc2["æ—¥æœŸ"] = cid[:10]
                sc2.setdefault("æ—¶é—´è½´", "")

        out.append(sc2)

    return out


def normalize_model3_clusters(
    output_text: str,
    parsed_subclusters: List[Dict[str, Any]],
) -> tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:
    """
    è§„èŒƒåŒ–æ¨¡å‹#3ï¼ˆæ—¥è¯é¢˜ç°‡èšåˆï¼‰è¾“å‡ºï¼š

    è¾“å…¥ï¼š
      - output_textï¼šæ¨¡å‹#3 è¾“å‡ºåŸå§‹å­—ç¬¦ä¸²
      - parsed_subclustersï¼šå­è¯é¢˜ç°‡åˆ—è¡¨ï¼ˆæ¨¡å‹#2 â†’ aggregate åï¼‰

    è¾“å‡ºï¼š
      - normalizedï¼šæ•´ç†å¥½çš„èšåˆç°‡åˆ—è¡¨ï¼ˆæ¯ä¸ªéƒ½æœ‰ è¯é¢˜ç°‡ / å­è¯é¢˜ç°‡åˆ—è¡¨ / æ—¥æœŸ / æ—¶é—´è½´ï¼‰
      - parsed_subclustersï¼šè¡¥å®Œæ—¥æœŸ/æ—¶é—´è½´åçš„å­ç°‡åˆ—è¡¨
    """

    # 0ï¼‰å…ˆç»™å­ç°‡è¡¥é½ æ—¥æœŸ + æ—¶é—´è½´ï¼ˆä»æ ‡é¢˜/å‘è¨€æ—¶é—´/_cluster_id é‡Œæ‰’ï¼‰
    parsed_subclusters = enrich_subclusters_with_datetime(parsed_subclusters)

    # 1ï¼‰å»ºç´¢å¼•ï¼š_cluster_id -> å­ç°‡ï¼ˆå«æ—¥æœŸ/æ—¶é—´è½´ï¼‰
    sub_by_id: Dict[str, Dict[str, Any]] = {}
    for sc in parsed_subclusters:
        cid = (sc.get("_cluster_id") or "").strip()
        if cid:
            sub_by_id[cid] = sc

    # 2ï¼‰â€œå®‰å…¨è§£æâ€æ¨¡å‹#3 è¾“å‡º
    objs = parse_jsonl_text_safe(output_text, label="æ¨¡å‹#3èšåˆè¾“å‡º")

    normalized: List[Dict[str, Any]] = []

    # 3ï¼‰é€æ¡å¤„ç†èšåˆç°‡
    for obj in objs:
        # 3.1 ç»Ÿä¸€è¯é¢˜ç°‡å­—æ®µ
        topic = (
            obj.get("è¯é¢˜ç°‡")
            or obj.get("èšåˆè¯é¢˜ç°‡")
            or obj.get("è¯é¢˜ç°‡3")
            or ""
        )
        topic = str(topic).strip()

        # 3.2 æ‹¿å­è¯é¢˜ç°‡åˆ—è¡¨ï¼Œå…¼å®¹å„ç§ key
        sub_list = (
            obj.get("å­è¯é¢˜ç°‡åˆ—è¡¨")
            or obj.get("å­è¯é¢˜ç°‡")
            or obj.get("å­è¯é¢˜ç°‡idåˆ—è¡¨")
            or obj.get("å­è¯é¢˜ç°‡IDåˆ—è¡¨")
            or []
        )
        sub_list = [str(x).strip() for x in _as_list(sub_list) if str(x).strip()]

        # 3.3 å…ˆç”¨èšåˆç°‡è‡ªå·±çš„ æ—¥æœŸ / æ—¶é—´è½´
        date = (obj.get("æ—¥æœŸ") or "").strip()
        time_axis = (obj.get("æ—¶é—´è½´") or "").strip()

        # 3.4 å¦‚æœç¼ºï¼Œå°è¯•ä»å­ç°‡å›å¡«
        if (not date) or (not time_axis):
            sub_dates: List[str] = []
            sub_axes: List[str] = []

            for cid in sub_list:
                sc = sub_by_id.get(cid)
                if not sc:
                    continue
                d = (sc.get("æ—¥æœŸ") or "").strip()
                ta = (sc.get("æ—¶é—´è½´") or "").strip()
                if d:
                    sub_dates.append(d)
                if ta:
                    sub_axes.append(ta)

            if not date and sub_dates:
                date = sub_dates[0]         # å¤šä¸ªå­ç°‡æœ‰æ—¥æœŸï¼Œè¿™é‡Œç®€å•å–ç¬¬ä¸€ä¸ª
            if not time_axis and sub_axes:
                time_axis = _merge_time_axes(sub_axes)   # å¤šä¸ªå­ç°‡æ—¶é—´è½´ï¼š"a-bã€c-d"

        # 3.5 ç¼ºå…³é”®å­—æ®µçš„ï¼Œç›´æ¥ä¸¢å¼ƒï¼Œé¿å…åé¢æŠ¥é”™
        if not topic or not sub_list or not date or not time_axis:
            continue

        # 3.6 æ¸…æ´—æ—¶é—´è½´ï¼šåªä¿ç•™æ•°å­—ã€å†’å·ã€æ¨ªæ ã€é¡¿å·ã€ç©ºæ ¼ï¼Œå»æ‰â€œæè½´â€ç­‰è„å­—ç¬¦
        norm_time_axis = clean_time_axis(time_axis)

        normalized.append({
            "è¯é¢˜ç°‡": topic,
            "å­è¯é¢˜ç°‡åˆ—è¡¨": sub_list,
            "æ—¥æœŸ": date,
            "æ—¶é—´è½´": norm_time_axis,
        })

    return normalized, parsed_subclusters


################ top5 ç­›é€‰ç›¸å…³ #################

def parse_time_range(date_str: str, range_str: str):
    """
    ä» range_str ä¸­è§£æä¸€ä¸ªæ—¶é—´æ®µï¼š
    - åªè®¤é‡Œé¢çš„ HH:MM:SS æ¨¡å¼
    - å°‘äº 2 ä¸ªæ—¶é—´ç‚¹å°±è¿”å› (None, None)
    """
    if not range_str:
        return None, None

    # æå–æ‰€æœ‰å½¢å¦‚ 16:10:56 çš„æ—¶é—´ç‰‡æ®µ
    times = re.findall(r"\d{1,2}:\d{2}:\d{2}", str(range_str))
    if len(times) < 2:
        return None, None

    start_str, end_str = times[0], times[1]

    try:
        start_dt = datetime.strptime(f"{date_str} {start_str}", "%Y-%m-%d %H:%M:%S")
        end_dt   = datetime.strptime(f"{date_str} {end_str}", "%Y-%m-%d %H:%M:%S")
        return start_dt, end_dt
    except ValueError:
        return None, None


def match_dialogs_by_time(
    messages: List[Dict[str, Any]],
    date_str: str,
    time_axis_str: str,
) -> List[Dict[str, Any]]:
    """
    æ ¹æ® æ—¥æœŸ + æ—¶é—´è½´ï¼Œä» messages ä¸­ç­›é€‰å¯¹åº”çš„åŸå§‹å‘è¨€ï¼š
    - messages é‡Œçš„æ—¶é—´å­—æ®µä¸ºï¼šå‘è¨€æ—¥æœŸ + å‘è¨€æ—¶é—´
    - time_axis_str æ”¯æŒå¤šæ®µï¼š"16:10:56-16:23:00ã€21:00:00-21:10:00"
    """

    if not messages or not date_str:
        return []
    if not time_axis_str or not isinstance(time_axis_str, str) or not time_axis_str.strip():
        return []

    matched: List[Dict[str, Any]] = []

    # å¤šä¸ªæ—¶é—´æ®µç”¨ "ã€" æ‹¼æ¥
    for part in str(time_axis_str).split("ã€"):
        part = part.strip()
        if not part:
            continue

        start_dt, end_dt = parse_time_range(date_str, part)
        if not start_dt or not end_dt:
            continue

        for row in messages:
            # 1) å…ˆè¿‡æ»¤æ—¥æœŸä¸ä¸€è‡´çš„
            if (row.get("å‘è¨€æ—¥æœŸ") or "") != date_str:
                continue

            ts = row.get("å‘è¨€æ—¶é—´") or ""
            try:
                msg_dt = datetime.strptime(f"{date_str} {ts}", "%Y-%m-%d %H:%M:%S")
            except Exception:
                continue

            if start_dt <= msg_dt <= end_dt:
                matched.append(row)

    return matched


def extract_cluster_stats(èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict[str, Any]], åŸå§‹å‘è¨€: List[str]) -> List[Dict[str, Any]]:
    """
    ç»Ÿè®¡æ¯ä¸ªèšåˆè¯é¢˜ç°‡çš„å‘è¨€ç©å®¶æ•° / å‘è¨€æ€»æ•°ã€‚
    """
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    results: List[Dict[str, Any]] = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = cluster.get("æ—¶é—´è½´")
        if not date or not time_axis:
            print(f"âš  èšåˆè¯é¢˜ç°‡ç¼ºå°‘æ—¥æœŸæˆ–æ—¶é—´è½´ï¼Œè·³è¿‡ï¼š{cluster}")
            continue

        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)
        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        result = {
            "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡"),
            "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
            "å‘è¨€ç©å®¶æ€»æ•°": len(players),
            "å‘è¨€æ€»æ•°": len(matched)
        }
        results.append(result)

    return results


def compute_heat_score(U: int, M: int) -> float:
    """
    çƒ­åº¦å…¬å¼ï¼šU * sqrt(M)
    U = å‘è¨€ç©å®¶æ€»æ•°ï¼›M = å‘è¨€æ€»æ•°
    """
    if U == 0 or M == 0:
        return 0.0
    return round(U * math.sqrt(M), 2)


def extract_top5_heat_clusters(
    èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict[str, Any]],
    åŸå§‹å‘è¨€: List[str],
    top_k: int = 5
) -> List[Dict[str, Any]]:
    """
    å¯¹æ¯ä¸ªèšåˆè¯é¢˜ç°‡è®¡ç®—çƒ­åº¦ï¼Œè¿”å› TopKã€‚
    """
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    enriched: List[Dict[str, Any]] = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = cluster.get("æ—¶é—´è½´")
        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)

        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        U = len(players)
        M = len(matched)
        heat = compute_heat_score(U, M)

        enriched.append({
            "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥",
            "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
            "æ—¥æœŸ": date,
            "æ—¶é—´è½´": time_axis,
            "å‘è¨€ç©å®¶æ€»æ•°": U,
            "å‘è¨€æ€»æ•°": M,
            "çƒ­åº¦è¯„åˆ†": heat
        })

    # æŒ‰çƒ­åº¦è¯„åˆ†æ’åºï¼Œå– TopK
    enriched.sort(key=lambda x: x["çƒ­åº¦è¯„åˆ†"], reverse=True)
    return enriched[:top_k]


################ æ·»åŠ è®¨è®ºç‚¹å­—æ®µï¼ˆæŒ‰å­è¯é¢˜ç°‡èšåˆï¼‰ ################

def attach_discussion_points(
    top_clusters: List[Dict[str, Any]],
    subclusters: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    å¯¹æ¯ä¸ªèšåˆç°‡ï¼š
    - æ ¹æ® å­è¯é¢˜ç°‡åˆ—è¡¨(_cluster_id) æ‰¾å›å­è¯é¢˜ç°‡
    - ç”¨ å­è¯é¢˜ç°‡["æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶"]ï¼ˆæˆ–["è®¨è®ºç‚¹"]ï¼‰ ä½œä¸ºè®¨è®ºç‚¹æ–‡æœ¬
    - èšåˆæˆå¤šä¸ªè®¨è®ºç‚¹ï¼Œæ¯ä¸ªè®¨è®ºç‚¹å†…éƒ¨æŒ‚æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨ + å­è¯é¢˜ç°‡åˆ—è¡¨
    """

    # _cluster_id -> å­è¯é¢˜ç°‡ row
    sub_by_id: Dict[str, Dict[str, Any]] = {}
    for row in subclusters:
        cid = row.get("_cluster_id")
        if cid:
            sub_by_id[str(cid)] = row

    result: List[Dict[str, Any]] = []

    for cluster in top_clusters:
        ids = cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨", []) or []

        # ç”¨ point_text èšåˆï¼šåŒä¸€ä¸ªè®¨è®ºç‚¹å¯èƒ½å¯¹åº”å¤šä¸ªå­ç°‡
        point_bucket: Dict[str, Dict[str, Any]] = {}

        for cid in ids:
            sc = sub_by_id.get(str(cid))
            if not sc:
                continue

            point_text = (sc.get("æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶") or sc.get("è®¨è®ºç‚¹") or "").strip()
            if not point_text:
                continue

            d = sc.get("æ—¥æœŸ") or cluster.get("æ—¥æœŸ")
            t_axis = sc.get("æ—¶é—´è½´") or sc.get("æ—¶é—´èŒƒå›´") or sc.get("æ—¥æœŸæ—¶é—´è½´") or ""

            if point_text not in point_bucket:
                point_bucket[point_text] = {
                    "è®¨è®ºç‚¹": point_text,
                    "æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨": [],
                    "å­è¯é¢˜ç°‡åˆ—è¡¨": [],
                }

            point_bucket[point_text]["å­è¯é¢˜ç°‡åˆ—è¡¨"].append(str(cid))
            point_bucket[point_text]["æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨"].append({
                "æ—¥æœŸ": d,
                "æ—¶é—´è½´": t_axis
            })

        # ä¸æ’åºã€ä¸æˆªæ–­ï¼šå…¨éƒ¨è¿”å›
        points = list(point_bucket.values())

        cluster_name = cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥"

        enriched_cluster = {
            "èšåˆè¯é¢˜ç°‡": cluster_name,
            "æ—¥æœŸ": cluster.get("æ—¥æœŸ"),
            "æ—¶é—´è½´": cluster.get("æ—¶é—´è½´"),
            "å‘è¨€ç©å®¶æ€»æ•°": cluster.get("å‘è¨€ç©å®¶æ€»æ•°"),
            "å‘è¨€æ€»æ•°": cluster.get("å‘è¨€æ€»æ•°"),
            "çƒ­åº¦è¯„åˆ†": cluster.get("çƒ­åº¦è¯„åˆ†"),
            "è®¨è®ºç‚¹": points,
        }

        result.append(enriched_cluster)

    return result


####################### å­˜å…¥æ¯æ—¥å‘è¨€ top5 ########################

from typing import Union

def append_daily_top5_to_version_jsonl(
    final_result: List[Dict[str, Any]],
    version_jsonl_path: Union[str, Path],
):
    """
    å°†å½“æ—¥ Top5 è¿½åŠ å†™å…¥ã€ç‰ˆæœ¬ç´¯è®¡ jsonl æ–‡ä»¶ã€‘ä¸­ï¼ˆä¾‹å¦‚ VERSION_TOP5_PATHï¼‰ã€‚

    åŒæ—¶ä¸ºæ¯æ¡è®°å½•è¡¥å……ä¸¤ä¸ªå­—æ®µï¼š
    1ï¼‰_idxï¼šåœ¨ version_jsonl_path ä¸­çš„å…¨å±€é€’å¢è¡Œå·ï¼ˆä»1å¼€å§‹ï¼‰
    2ï¼‰_daily_top_idï¼šå½“å¤©å†…çš„ Top è¯é¢˜ç°‡IDï¼Œå½¢å¼ï¼šYYYY-MM-DD_TXXï¼Œä¾‹å¦‚ 2025-12-02_T05

    è¯´æ˜ï¼š
    - å‡è®¾ final_result å†…æ‰€æœ‰è®°å½•çš„ "æ—¥æœŸ" ç›¸åŒï¼ˆå³åŒä¸€å¤©çš„ top5ï¼‰
    - å¦‚æœæ–‡ä»¶ä¸­å·²å­˜åœ¨åŒä¸€æ—¥æœŸçš„æ•°æ®ï¼Œä¼šåœ¨åŸæœ‰åŸºç¡€ä¸Šç»§ç»­ç´¯åŠ  _daily_top_id çš„ç¼–å·
    - ä¸ä¼šä¿®æ”¹ä»»ä½•å·²æœ‰çš„ `_cluster_id` å­—æ®µ
    """
    if not final_result:
        print("âš  final_result ä¸ºç©ºï¼Œä»Šæ—¥æ—  Top5 å¯å†™å…¥ã€‚")
        return

    date_str = (final_result[0].get("æ—¥æœŸ") or "").strip()
    if not date_str:
        raise ValueError("final_result ä¸­ç¼ºå°‘ 'æ—¥æœŸ' å­—æ®µï¼Œæ— æ³•ç”Ÿæˆ _daily_top_idã€‚")

    path = Path(version_jsonl_path).resolve()
    path.parent.mkdir(parents=True, exist_ok=True)

    print("âœ… writing daily_top5 to:", path)
    print("âœ… cwd:", Path.cwd().resolve())

    global_max_idx = 0
    existing_count_for_day = 0

    if path.exists():
        with path.open("r", encoding="utf-8") as f:
            for line_no, line in enumerate(f, start=1):
                s = line.strip()
                if not s:
                    continue
                try:
                    obj = json.loads(s)
                except json.JSONDecodeError:
                    print(f"âš  å·²æœ‰æ–‡ä»¶ç¬¬{line_no}è¡Œ JSON æ— æ³•è§£æï¼Œå·²è·³è¿‡ã€‚")
                    continue

                # å…¨å±€ _idxï¼šä¼˜å…ˆç”¨å·²æœ‰ _idxï¼Œè‹¥æ— åˆ™ç”¨è¡Œå·å…œåº•
                if isinstance(obj.get("_idx"), int):
                    global_max_idx = max(global_max_idx, obj["_idx"])
                else:
                    global_max_idx = max(global_max_idx, line_no)

                # ç»Ÿè®¡å½“å¤©å·²æœ‰å¤šå°‘æ¡ï¼ˆç”¨äº _daily_top_id ç¼–å·ç»­ä¸Šå»ï¼‰
                if (obj.get("æ—¥æœŸ") or "").strip() == date_str:
                    existing_count_for_day += 1

    # è¿½åŠ å†™å…¥
    with path.open("a", encoding="utf-8") as f:
        for offset, row in enumerate(final_result, start=1):
            row = dict(row)  # é¿å…ä¿®æ”¹åŸå¯¹è±¡å¼•ç”¨

            # 1) å…¨å±€é€’å¢ _idx
            global_max_idx += 1
            row["_idx"] = global_max_idx

            # 2) å½“å¤©å†…é€’å¢ _daily_top_id
            idx_for_day = existing_count_for_day + offset
            row["_daily_top_id"] = f"{date_str}_T{idx_for_day:02d}"

            f.write(json.dumps(row, ensure_ascii=False) + "\n")

        # å¯é€‰ï¼šå¼ºåˆ¶è½ç›˜
        f.flush()
        import os
        os.fsync(f.fileno())

    print(f"âœ… å·²å°†å½“æ—¥ Top5ï¼ˆå« _idx å’Œ _daily_top_idï¼‰è¿½åŠ å†™å…¥: {path}")



def extract_time_axis_from_title(title: str) -> str:
    """
    ä»è¯é¢˜ç°‡æ ‡é¢˜ä¸­æå–æ—¶é—´è½´éƒ¨åˆ†ï¼Œå¦‚ï¼š
    'ç»´æŠ¤è¡¥å¿è¯‰æ±‚è®¨è®ºï¼ˆ2025-11-19 14:21:57-14:34:08ï¼‰'
    æˆ– 'ç»´æŠ¤è¡¥å¿è¯‰æ±‚è®¨è®º(2025-11-19 14:21:57-14:34:08)'
    -> '14:21:57-14:34:08'
    """
    if not title:
        return ""
    title = str(title)

    # æ”¯æŒå…¨è§’/åŠè§’æ‹¬å·ï¼šï¼ˆï¼‰ã€()
    pattern = r"[ï¼ˆ(](\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2}:\d{2}-\d{2}:\d{2}:\d{2})[ï¼‰)]"
    m = re.search(pattern, title)
    if m:
        return m.group(2)
    return ""


# ------- è§£ææ¨¡å‹#4çš„è¾“å‡ºæ–‡æœ¬ ----------------

def parse_and_normalize_opinion_output(
    opinion_output: str,
    topic_id: str,
    discussion_point: str,
) -> Optional[Dict[str, Any]]:
    """
    è§£ææ¨¡å‹#4è¾“å‡ºï¼Œå¹¶åšè½»åº¦è§„èŒƒåŒ–ã€‚
    å‰æï¼šopinion_output æ˜¯ä¸€ä¸ªå•ç‹¬çš„ JSON å¯¹è±¡å­—ç¬¦ä¸²ã€‚
    """

    if not opinion_output or not opinion_output.strip():
        return None

    s = opinion_output.strip()

    # 1ï¼‰æœ€ç›´æ¥æƒ…å†µï¼šæ•´æ®µå°±æ˜¯ä¸€ä¸ª JSON å¯¹è±¡
    try:
        obj = json.loads(s)
    except Exception:
        # 2ï¼‰å…œåº•ï¼šå¦‚æœä¸Šæ¸¸å“ªå¤©åˆåœ¨å‰ååŠ äº†è§£é‡Šæ–‡å­—ï¼Œ
        #    å°è¯•æˆªå–ç¬¬ä¸€å¯¹ { ... } ä¸­é—´çš„å†…å®¹å†è§£æä¸€æ¬¡
        start = s.find("{")
        end = s.rfind("}")
        if start == -1 or end == -1 or start >= end:
            return None
        try:
            obj = json.loads(s[start: end + 1])
        except Exception:
            return None

    # â€”â€” ç”¨æˆ‘ä»¬è‡ªå·±ä¼ è¿›æ¥çš„ä¿¡æ¯è¦†ç›–ï¼Œç¡®ä¿å¯¹å¾—ä¸Š top5_results â€”â€” 
    obj["è¯é¢˜ç°‡ID"] = topic_id          # é€šå¸¸æ˜¯ _cluster_idï¼Œæ¯”å¦‚ "2025-12-02_B2_01"
    obj["è®¨è®ºç‚¹"] = discussion_point    # ä¸Šæ¸¸â€œæ ¸å¿ƒå¯¹è±¡/æœºåˆ¶â€å­—æ®µ

    # â€”â€” ç»Ÿä¸€â€œä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹â€ä¸º list[str] â€”â€” 
    ex = obj.get("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹")
    if isinstance(ex, str):
        obj["ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"] = [ex.strip()] if ex.strip() else []
    elif isinstance(ex, list):
        obj["ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"] = [str(x).strip() for x in ex if str(x).strip()]
    else:
        obj["ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"] = []

    # â€”â€” åˆ†æ­§ç‚¹åšä¸ª stripï¼Œæ–¹ä¾¿åé¢åˆ¤æ–­â€œæ— æ˜æ˜¾åˆ†æ­§â€ â€”â€” 
    diff = obj.get("ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹")
    if isinstance(diff, str):
        obj["ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹"] = diff.strip()

    return obj


# -------------- final_top5_result ---------------------

def build_daily_top5_opinion_records(
    top5_results: List[Dict[str, Any]],
    sub_opinion_map: Dict[str, Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    æŠŠ å­è¯é¢˜ç°‡è§‚ç‚¹ç»“æœï¼ˆsub_opinion_mapï¼‰ åˆå¹¶å› å½“æ—¥top5 ç»“æœã€‚
    """
    final_records: List[Dict[str, Any]] = []

    for cluster in top5_results:
        date = cluster.get("æ—¥æœŸ")
        cluster_name = cluster.get("èšåˆè¯é¢˜ç°‡") or cluster.get("è¯é¢˜ç°‡") or "æœªçŸ¥"

        base = {
            "æ—¥æœŸ": date,
            "èšåˆè¯é¢˜ç°‡": cluster_name,
            "çƒ­åº¦è¯„åˆ†": cluster.get("çƒ­åº¦è¯„åˆ†"),
            "å‘è¨€ç©å®¶æ•°": cluster.get("å‘è¨€ç©å®¶æ€»æ•°"),
            "å‘è¨€æ€»æ•°": cluster.get("å‘è¨€æ€»æ•°"),
            "æ—¶é—´è½´": cluster.get("æ—¶é—´è½´"),
        }

        cid_list = cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨", []) or []

        discussion_items: List[Dict[str, Any]] = []
        all_examples: List[str] = []
        seen_examples = set()  # é˜²æ­¢åŒä¸€å¥åŸè¯é‡å¤

        for idx, cid in enumerate(cid_list, start=1):
            op = sub_opinion_map.get(cid)
            if not op:
                print(f"âš  å­è¯é¢˜ç°‡ {cid} æœªåœ¨ sub_opinion_map ä¸­æ‰¾åˆ°æ¨¡å‹#4ç»“æœï¼Œè·³è¿‡ã€‚")
                continue

            # åŠ¨æ€å­—æ®µåï¼šè®¨è®ºç‚¹1 / è®¨è®ºç‚¹2 / ...
            key_discussion = f"è®¨è®ºç‚¹{idx}"

            item: Dict[str, Any] = {
                key_discussion: op.get("è®¨è®ºç‚¹", ""),
                "ç©å®¶å…±è¯†": op.get("ç©å®¶å…±è¯†", ""),
            }

            # åªåœ¨ã€æœ‰çœŸå®åˆ†æ­§ã€‘æ—¶å†™å…¥â€œç©å®¶ä¸»è¦åˆ†æ­§ç‚¹â€
            diff = op.get("ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹", "")
            if isinstance(diff, str):
                diff_clean = diff.strip()
                if diff_clean and diff_clean != "æ— æ˜æ˜¾åˆ†æ­§":
                    item["ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹"] = diff_clean

            discussion_items.append(item)

            # æ”¶é›†å…¸å‹å‘è¨€
            examples = op.get("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹", [])
            if isinstance(examples, list):
                for ex in examples:
                    if not ex:
                        continue
                    s = str(ex)
                    if s in seen_examples:
                        continue
                    seen_examples.add(s)
                    all_examples.append(s)

        record = {
            **base,
            "è®¨è®ºç‚¹åˆ—è¡¨": discussion_items,
            "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": all_examples,
        }
        final_records.append(record)

    return final_records


# ------------------------ ç‰ˆæœ¬èšåˆè¯é¢˜ç°‡å¼•å…¥ -------------------------

def read_jsonl_file(path: Path) -> List[Dict[str, Any]]:
    """
    è¯»å– jsonl æ–‡ä»¶ï¼Œè¿”å› List[dict]ã€‚
    è§£æå¤±è´¥çš„è¡Œä¼šæ‰“å°å‘Šè­¦ä½†ä¸ä¼šä¸­æ–­ã€‚
    """
    rows: List[Dict[str, Any]] = []
    if not path.exists():
        return rows

    with path.open("r", encoding="utf-8") as f:
        for i, line in enumerate(f, start=1):
            s = line.strip()
            if not s:
                continue
            try:
                obj = json.loads(s)
            except Exception as e:
                print(f"âš  read_jsonl_file: ç¬¬{i}è¡Œ JSON è§£æå¤±è´¥ï¼š{e} | åŸæ–‡å‰200å­—ç¬¦ï¼š{s[:200]}")
                continue
            rows.append(obj)
    return rows
##### å¤šæ—¥è¾“å‡ºæçº¯ï¼Œåªè¦ idã€æ—¥æœŸã€è¯é¢˜ç°‡ã€è®¨è®ºç‚¹ ################################


def _extract_points_min(points_raw: Any, max_points: int = 3) -> List[Dict[str, Any]]:
    """
    è¾“å‡ºç‚¹ç»“æ„ç»Ÿä¸€ä¸ºï¼š
    {"t": "...", "tid": ["...","..."]}
    """
    if not points_raw:
        return []

    pts: List[Dict[str, Any]] = []

    if isinstance(points_raw, list):
        for p in points_raw:
            if isinstance(p, dict):
                # å…¼å®¹è€å­—æ®µ & æ–°å­—æ®µ
                t = (p.get("è®¨è®ºç‚¹") or p.get("point") or p.get("t") or "").strip()

                tid = (
                    p.get("å­è¯é¢˜ç°‡åˆ—è¡¨")
                    or p.get("å­è¯é¢˜ç°‡idåˆ—è¡¨")
                    or p.get("subcluster_ids")
                    or p.get("tid")
                    or []
                )

                if isinstance(tid, str):
                    tid = [tid]
                if not isinstance(tid, list):
                    tid = []

                if t:
                    pts.append({"t": t, "tid": tid})

            elif isinstance(p, str):
                s = p.strip()
                if s:
                    pts.append({"t": s, "tid": []})

    elif isinstance(points_raw, str):
        s = points_raw.strip()
        if s:
            pts.append({"t": s, "tid": []})

    if max_points and len(pts) > max_points:
        pts = pts[:max_points]
    return pts



def build_version_agg_input_jsonl_text(
    daily_top5_rows: List[Dict[str, Any]],
    max_points_per_row: int = 3,
) -> str:
    """
    âœ…åªè¾“å‡ºæœ€å°å­—æ®µï¼šèšåˆè¯é¢˜ç°‡ã€è®¨è®ºç‚¹ã€å­è¯é¢˜ç°‡åˆ—è¡¨
    æ¯è¡Œç»“æ„ï¼š
    {"èšåˆè¯é¢˜ç°‡":"...", "è®¨è®ºç‚¹":[{"è®¨è®ºç‚¹":"...","å­è¯é¢˜ç°‡åˆ—è¡¨":[...]}]}
    """
    out_lines: List[str] = []
    seen_ids = set()

    for r in daily_top5_rows:
        # ç”¨äºå†…éƒ¨å»é‡ï¼ˆä¸è¾“å‡ºï¼‰
        _uniq = (r.get("_daily_top_id") or r.get("id") or "").strip()
        if not _uniq:
            # å…œåº•ï¼šç”¨ æ—¥æœŸ+topic+_idx åšå»é‡keyï¼ˆä»ä¸è¾“å‡ºï¼‰
            date = (r.get("æ—¥æœŸ") or "").strip()
            topic_tmp = (
                r.get("èšåˆè¯é¢˜ç°‡")
                or r.get("è¯é¢˜ç°‡")
                or r.get("è¯é¢˜ç°‡3")
                or r.get("è¯æç°‡")
                or ""
            ).strip()
            idx = r.get("_idx")
            _uniq = f"{date}|{topic_tmp}|{idx}"

        if _uniq in seen_ids:
            continue
        seen_ids.add(_uniq)

        topic = (
            r.get("èšåˆè¯é¢˜ç°‡")
            or r.get("è¯é¢˜ç°‡")
            or r.get("è¯é¢˜ç°‡3")
            or r.get("è¯æç°‡")
            or ""
        ).strip()

        points = _extract_points_min(r.get("è®¨è®ºç‚¹"), max_points=max_points_per_row)

        # å¿…é¡»æœ‰ topic & pointsï¼Œå¦åˆ™è·³è¿‡
        if not topic or not points:
            continue

        obj = {
            "èšåˆè¯é¢˜ç°‡": topic,
            "è®¨è®ºç‚¹": points,
        }
        out_lines.append(json.dumps(obj, ensure_ascii=False))

    return "\n".join(out_lines)




#################### ç‰ˆæœ¬çƒ­åº¦ TopK è®¡ç®— #################

def compute_version_heat_topk(
    version_clusters: List[Dict[str, Any]],
    daily_top5_rows: List[Dict[str, Any]],
    top_k: int = 5,
) -> List[Dict[str, Any]]:
    """
    version_clusters: æ¨¡å‹#4 çš„ç‰ˆæœ¬èšåˆè¾“å‡ºï¼ˆæ¯è¡Œæœ‰ è¯é¢˜ç°‡ / è®¨è®ºç‚¹ / æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨ï¼‰
    daily_top5_rows:  daily_top5.jsonl è§£æç»“æœï¼ˆæœ‰ å‘è¨€æ€»æ•° / å‘è¨€ç©å®¶æ€»æ•° / _daily_top_idï¼‰

    ç‰ˆæœ¬çº§å‘è¨€çƒ­åº¦å…¬å¼ï¼š
      ç‰ˆæœ¬_å‘è¨€çƒ­åº¦ = ç‰ˆæœ¬_å‘è¨€ç©å®¶æ€»æ•° Ã— sqrt(ç‰ˆæœ¬_å‘è¨€æ€»æ•°)
    å…¶ä¸­ï¼š
      ç‰ˆæœ¬_å‘è¨€æ€»æ•°     = è¯¥ç‰ˆæœ¬çº§ç°‡ä¸‹æ‰€æœ‰ daily å­ç°‡ å‘è¨€æ€»æ•° ä¹‹å’Œ
      ç‰ˆæœ¬_å‘è¨€ç©å®¶æ€»æ•° = è¯¥ç‰ˆæœ¬çº§ç°‡ä¸‹æ‰€æœ‰ daily å­ç°‡ å‘è¨€ç©å®¶æ€»æ•° ä¹‹å’Œï¼ˆè¿‘ä¼¼ï¼‰
    """

    # 1) å»º daily_top5 çš„ç´¢å¼•ï¼šid -> {å‘è¨€æ€»æ•°, å‘è¨€ç©å®¶æ€»æ•°}
    id2metrics: Dict[str, Dict[str, int]] = {}
    for r in daily_top5_rows:
        _id = (r.get("_daily_top_id") or r.get("id") or "").strip()
        if not _id:
            continue
        total_msgs = int(r.get("å‘è¨€æ€»æ•°") or 0)
        total_players = int(r.get("å‘è¨€ç©å®¶æ€»æ•°") or 0)
        id2metrics[_id] = {
            "å‘è¨€æ€»æ•°": total_msgs,
            "å‘è¨€ç©å®¶æ€»æ•°": total_players,
        }

    enriched: List[Dict[str, Any]] = []

    # 2) å¯¹æ¯ä¸ªç‰ˆæœ¬çº§èšåˆè¯é¢˜ç°‡ï¼Œç´¯åŠ æ——ä¸‹æ‰€æœ‰ id çš„ U / M
    for vc in version_clusters:
        dt_list = vc.get("æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨") or []
        version_total_msgs = 0
        version_total_players = 0
        used_ids = set()

        for item in dt_list:
            did = (item.get("id") or "").strip()
            if not did or did in used_ids:
                continue
            used_ids.add(did)

            metrics = id2metrics.get(did)
            if not metrics:
                # daily_top5.jsonl é‡Œæ‰¾ä¸åˆ°è¿™ä¸ª idï¼Œå¯ä»¥æ‰“å°å‡ºæ¥æ’æŸ¥
                # print(f"[WARN] æ‰¾ä¸åˆ° daily è®°å½•ï¼š{did}")
                continue

            version_total_msgs += metrics["å‘è¨€æ€»æ•°"]
            version_total_players += metrics["å‘è¨€ç©å®¶æ€»æ•°"]

        # æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡
        if version_total_msgs <= 0 or version_total_players <= 0:
            continue

        # ğŸ” è¿™é‡Œç›´æ¥å¤ç”¨ä½ å·²æœ‰çš„ compute_heat_score
        version_heat = compute_heat_score(version_total_players, version_total_msgs)

        vc_enriched = dict(vc)
        vc_enriched["ç‰ˆæœ¬_å‘è¨€æ€»æ•°"] = version_total_msgs
        vc_enriched["ç‰ˆæœ¬_å‘è¨€ç©å®¶æ€»æ•°"] = version_total_players
        vc_enriched["ç‰ˆæœ¬_å‘è¨€çƒ­åº¦"] = version_heat

        enriched.append(vc_enriched)

    # 3) æŒ‰ç‰ˆæœ¬_å‘è¨€çƒ­åº¦æ’åºï¼Œå– TopK
    enriched.sort(key=lambda x: x.get("ç‰ˆæœ¬_å‘è¨€çƒ­åº¦", 0.0), reverse=True)
    return enriched[:top_k]
#####æ‰“å¹³è¯æç°‡èšåˆ#############

def clusters_list_to_jsonl(version_clusters: List[Dict[str, Any]]) -> str:
    """
    åªæŠŠå¤–å±‚ list æ‰“å¹³æˆ jsonlï¼š
    - æ¯è¡Œä¸€ä¸ª {"è¯é¢˜ç°‡":..., "è®¨è®ºç‚¹":[...]}
    - ä¸æ‹†å¼€è®¨è®ºç‚¹
    """
    lines = []
    for obj in version_clusters:
        lines.append(json.dumps(obj, ensure_ascii=False))
    return "\n".join(lines)

#########ç‰ˆæœ¬è¯æç°‡æ ¹æ®tidæ‰¾åˆ°å¯¹åº”æ—¶é—´è½´##########
def build_tid_time_index(daily_top5_rows: List[Dict[str, Any]]) -> Dict[str, List[Dict[str, str]]]:
    """
    ä» daily_top5 è®°å½•ä¸­æ„å»º:
      tid -> [{"æ—¥æœŸ": "...", "æ—¶é—´è½´": "..."}, ...]
    æ–¹ä¾¿åç»­é€šè¿‡å­è¯é¢˜ç°‡ ID å›æº¯åˆ°å…·ä½“æ—¶é—´æ®µã€‚
    """
    tid_index: Dict[str, List[Dict[str, str]]] = defaultdict(list)

    for row in daily_top5_rows:
        row_date = (row.get("æ—¥æœŸ") or "").strip()
        row_axis = (row.get("æ—¶é—´è½´") or "").strip()
        points = row.get("è®¨è®ºç‚¹") or []

        for p in points:
            tids = p.get("å­è¯é¢˜ç°‡åˆ—è¡¨") or p.get("tid") or []
            if isinstance(tids, str):
                tids = [tids]
            if not isinstance(tids, list):
                continue

            dt_list = p.get("æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨") or []
            if not isinstance(dt_list, list):
                dt_list = []

            for tid in tids:
                tid = str(tid).strip()
                if not tid:
                    continue

                # ä¼˜å…ˆç”¨ è®¨è®ºç‚¹ è‡ªå·±çš„æ—¥æœŸæ—¶é—´è½´ï¼›æ²¡æœ‰å°±ç”¨ row çš„æ—¥æœŸ+æ—¶é—´è½´å…œåº•
                if dt_list:
                    for seg in dt_list:
                        d = (seg.get("æ—¥æœŸ") or row_date).strip()
                        axis = (seg.get("æ—¶é—´è½´") or "").strip()
                        if d and axis:
                            tid_index[tid].append({"æ—¥æœŸ": d, "æ—¶é—´è½´": axis})
                else:
                    if row_date and row_axis:
                        tid_index[tid].append({"æ—¥æœŸ": row_date, "æ—¶é—´è½´": row_axis})

    # å»é‡
    for tid, segs in tid_index.items():
        seen = set()
        uniq = []
        for seg in segs:
            key = (seg["æ—¥æœŸ"], seg["æ—¶é—´è½´"])
            if key in seen:
                continue
            seen.add(key)
            uniq.append(seg)
        tid_index[tid] = uniq

    return tid_index
#################å¯¹å•ä¸ªç‰ˆæœ¬è¯é¢˜ç°‡ï¼šç®— U/M/çƒ­åº¦ + ç”Ÿæˆè®¨è®ºç‚¹info##############
import json
from typing import Set, List, Dict, Any

def compute_version_cluster_heat_and_points(
    cluster: Dict[str, Any],
    tid_time_index: Dict[str, List[Dict[str, str]]],
    raw_jsonl_lines: List[str],
) -> Dict[str, Any]:
    """
    é’ˆå¯¹ä¸€ä¸ªâ€œç‰ˆæœ¬èšåˆè¯é¢˜ç°‡â€ï¼ˆæ¨¡å‹#4 + #5 è¾“å‡ºä¸­çš„ä¸€æ¡ï¼‰ï¼š
      - å¯¹å…¶ä¸‹æ¯ä¸ªã€è®¨è®ºç‚¹ã€‘å•ç‹¬è®¡ç®—çƒ­åº¦ï¼š
          tid -> æ—¥æœŸ+æ—¶é—´è½´ -> åŸæ–‡æ¶ˆæ¯ -> U_pointã€M_pointã€heat_point
      - è¯é¢˜ç°‡æ•´ä½“çƒ­åº¦ = æ‰€æœ‰è®¨è®ºç‚¹çƒ­åº¦ä¹‹å’Œï¼š
          å‘è¨€ç©å®¶æ€»æ•° = Î£ U_point
          å‘è¨€æ€»æ•°     = Î£ M_point
          çƒ­åº¦è¯„åˆ†     = Î£ heat_point
      - è¾“å‡ºç»“æ„æŒ‰ä½ æŒ‡å®šï¼š
        {
          "èšåˆè¯é¢˜ç°‡": ...,
          "è®¨è®ºç‚¹info": [{"t": "...", "æ—¥æœŸ": "...", "æ—¶é—´è½´": "..."}, ...],
          "å‘è¨€ç©å®¶æ€»æ•°": U_sum,
          "å‘è¨€æ€»æ•°": M_sum,
          "çƒ­åº¦è¯„åˆ†": heat_sum
        }

    æ³¨æ„ï¼šè¿™é‡Œä¸ä¼šè·¨è®¨è®ºç‚¹å»é‡ç©å®¶/æ¶ˆæ¯ï¼š
      åŒä¸€ç©å®¶åœ¨å¤šä¸ªè®¨è®ºç‚¹å‘è¨€ï¼Œä¼šåœ¨ä¸åŒè®¨è®ºç‚¹é‡Œåˆ†åˆ«è®¡å…¥ï¼Œç„¶ååœ¨ç°‡çº§åˆ«åšâ€œæ±‚å’Œâ€ã€‚
    """
    discussion_points = cluster.get("è®¨è®ºç‚¹") or []

    # è§£æåŸå§‹å‘è¨€
    messages = [json.loads(line.strip()) for line in raw_jsonl_lines if line.strip()]

    # â€”â€” ç°‡çº§åˆ«çš„ç´¯ç§¯é‡ â€”â€” 
    total_U = 0
    total_M = 0
    total_heat = 0.0

    # è¾“å‡ºçš„è®¨è®ºç‚¹info
    discussion_info: List[Dict[str, str]] = []

    for p in discussion_points:
        t_text = (p.get("t") or "").strip()
        if not t_text:
            continue

        # è¯¥è®¨è®ºç‚¹ä¸‹çš„ tid åˆ—è¡¨
        tid_list = p.get("tid") or []
        if isinstance(tid_list, str):
            tid_list = [tid_list]
        if not isinstance(tid_list, list):
            continue

        # è¿™ä¸ªè®¨è®ºç‚¹å¯¹åº”çš„æ‰€æœ‰ æ—¥æœŸ+æ—¶é—´è½´ ç‰‡æ®µ
        segs_for_point = []
        for tid in tid_list:
            tid = str(tid).strip()
            if not tid:
                continue
            segs_for_point.extend(tid_time_index.get(tid, []))

        # å»é‡è¿™ä¸ªè®¨è®ºç‚¹å†…çš„ æ—¥æœŸ+æ—¶é—´è½´ ç»„åˆ
        seen_seg_point = set()
        uniq_segs_point: List[Dict[str, str]] = []
        for seg in segs_for_point:
            key = (seg["æ—¥æœŸ"], seg["æ—¶é—´è½´"])
            if key in seen_seg_point:
                continue
            seen_seg_point.add(key)
            uniq_segs_point.append(seg)

        # â€”â€” é’ˆå¯¹è¿™ä¸ªè®¨è®ºç‚¹ï¼Œå›æº¯åŸæ–‡ï¼Œç»Ÿè®¡ U_point / M_point â€”â€” 
        matched_msgs_point: List[Dict[str, Any]] = []
        seen_msg_ids_point: Set[Any] = set()

        for seg in uniq_segs_point:
            date_str = seg["æ—¥æœŸ"]
            time_axis_str = seg["æ—¶é—´è½´"]

            seg_msgs = match_dialogs_by_time(messages, date_str, time_axis_str)
            for msg in seg_msgs:
                # åœ¨ã€è®¨è®ºç‚¹å†…éƒ¨ã€‘å»é‡æ¶ˆæ¯ï¼Œé¿å…åŒä¸€æ¶ˆæ¯å› å¤šæ®µæ—¶é—´é‡å¤è®¡æ•°
                key = msg.get("_idx")
                if key is None:
                    key = (
                        msg.get("å‘è¨€æ—¥æœŸ"),
                        msg.get("å‘è¨€æ—¶é—´"),
                        msg.get("ç©å®¶ID") or msg.get("å‘è¨€äººID") or msg.get("è§’è‰²ID"),
                        msg.get("å†…å®¹") or msg.get("æ–‡æœ¬") or msg.get("æ¶ˆæ¯å†…å®¹"),
                    )
                if key in seen_msg_ids_point:
                    continue
                seen_msg_ids_point.add(key)
                matched_msgs_point.append(msg)

        # è®¡ç®—è¯¥è®¨è®ºç‚¹çš„ U_point / M_point / heat_point
        players_point: Set[str] = set()
        for msg in matched_msgs_point:
            pid = msg.get("ç©å®¶ID") or msg.get("å‘è¨€äººID") or msg.get("è§’è‰²ID")
            if pid:
                players_point.add(str(pid))

        U_point = len(players_point)
        M_point = len(matched_msgs_point)
        heat_point = compute_heat_score(U_point, M_point)

        # ç´¯åŠ åˆ°ç°‡çº§åˆ«
        total_U += U_point
        total_M += M_point
        total_heat += heat_point

        # ç”Ÿæˆè®¨è®ºç‚¹infoï¼ˆæŒ‰ä½ æŒ‡å®šæ ¼å¼ï¼Œåªè¦ t + æ—¥æœŸ + æ—¶é—´è½´ï¼‰
        for seg in uniq_segs_point:
            item = {
                "t": t_text,
                "æ—¥æœŸ": seg["æ—¥æœŸ"],
                "æ—¶é—´è½´": seg["æ—¶é—´è½´"],
            }
            discussion_info.append(item)

    # å»é‡è®¨è®ºç‚¹info
    seen_info = set()
    uniq_info = []
    for item in discussion_info:
        key = (item["t"], item["æ—¥æœŸ"], item["æ—¶é—´è½´"])
        if key in seen_info:
            continue
        seen_info.add(key)
        uniq_info.append(item)

    result = {
        "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥",
        "è®¨è®ºç‚¹info": uniq_info,
        "å‘è¨€ç©å®¶æ€»æ•°": total_U,
        "å‘è¨€æ€»æ•°": total_M,
        "çƒ­åº¦è¯„åˆ†": round(total_heat, 2),   # ä¿æŒä¸¤ä½å°æ•°é£æ ¼
    }

    return result
###### å¯¹å•ä¸ªè¯é¢˜ç°‡è®¡ç®—è®¨è®ºç‚¹ åšé‡åŒ–æ•°æ®è®¡ç®—######
import json
from typing import List, Dict, Any, Set
def compute_cluster_point_metrics(
    cluster: Dict[str, Any],
    tid_time_index: Dict[str, List[Dict[str, str]]],
    raw_jsonl_lines: List[str],
) -> Dict[str, Any]:
    """
    è¾“å…¥ï¼š
      - cluster: å•ä¸ªç‰ˆæœ¬èšåˆè¯é¢˜ç°‡ï¼ˆæ¨¡å‹#4 æˆ– #5 è¾“å‡ºçš„ä¸€æ¡ï¼‰
        å…è®¸ "è®¨è®ºç‚¹" å­—æ®µæ˜¯ï¼š
          * list[dict]ï¼Œå¦‚ [{"t": "...", "tid": [...]}, ...]
          * list[str]ï¼Œå¦‚ ["è®¨è®ºç‚¹1", "è®¨è®ºç‚¹2"]
          * æˆ–ä¸¤è€…æ··åˆ
    è¾“å‡ºï¼š
      {
        "èšåˆè¯é¢˜ç°‡": "...",
        "è®¨è®ºç‚¹info": [...],
      }
    """
    # â­ ç»Ÿä¸€æŠŠ cluster["è®¨è®ºç‚¹"] è§„èŒƒåŒ–æˆ [{"t": "...", "tid": [...]}, ...]
    raw_points = cluster.get("è®¨è®ºç‚¹")
    # max_points=0 -> ä¸æˆªæ–­æ•°é‡ï¼Œä¿ç•™å…¨éƒ¨è®¨è®ºç‚¹
    discussion_points = _extract_points_min(raw_points, max_points=0)

    # è§£æåŸå§‹å‘è¨€
    messages = [json.loads(line.strip()) for line in raw_jsonl_lines if line.strip()]

    discussion_info: List[Dict[str, Any]] = []

    for p in discussion_points:
        t_text = (p.get("t") or "").strip()
        if not t_text:
            continue

        # è¯¥è®¨è®ºç‚¹ä¸‹çš„ tid åˆ—è¡¨ï¼ˆå…è®¸ str / listï¼‰
        tid_list = p.get("tid") or []
        if isinstance(tid_list, str):
            tid_list = [tid_list]
        if not isinstance(tid_list, list):
            continue

        # åé¢é€»è¾‘ä¿æŒä¸å˜ â†“
        # æ”¶é›†è¯¥è®¨è®ºç‚¹å¯¹åº”çš„æ‰€æœ‰ æ—¥æœŸ+æ—¶é—´è½´ ç‰‡æ®µ
        segs_for_point: List[Dict[str, str]] = []
        for tid in tid_list:
            tid = str(tid).strip()
            if not tid:
                continue
            segs_for_point.extend(tid_time_index.get(tid, []))

        # å»é‡ï¼šåŒä¸€è®¨è®ºç‚¹å†…éƒ¨çš„ æ—¥æœŸ+æ—¶é—´è½´ ç»„åˆ
        seen_seg = set()
        uniq_segs: List[Dict[str, str]] = []
        for seg in segs_for_point:
            key = (seg["æ—¥æœŸ"], seg["æ—¶é—´è½´"])
            if key in seen_seg:
                continue
            seen_seg.add(key)
            uniq_segs.append(seg)

        if not uniq_segs:
            continue

        # å›æº¯åŸæ–‡ + ç»Ÿè®¡ U/Mï¼ˆä¿æŒä½ åŸæ¥çš„é€»è¾‘ï¼‰
        matched_msgs_point: List[Dict[str, Any]] = []
        seen_msg_ids_point: Set[Any] = set()

        for seg in uniq_segs:
            date_str = seg["æ—¥æœŸ"]
            time_axis_str = seg["æ—¶é—´è½´"]

            seg_msgs = match_dialogs_by_time(messages, date_str, time_axis_str)
            for msg in seg_msgs:
                key = msg.get("_idx")
                if key is None:
                    key = (
                        msg.get("å‘è¨€æ—¥æœŸ"),
                        msg.get("å‘è¨€æ—¶é—´"),
                        msg.get("ç©å®¶ID") or msg.get("å‘è¨€äººID") or msg.get("è§’è‰²ID"),
                        msg.get("å†…å®¹") or msg.get("æ–‡æœ¬") or msg.get("æ¶ˆæ¯å†…å®¹"),
                    )
                if key in seen_msg_ids_point:
                    continue
                seen_msg_ids_point.add(key)
                matched_msgs_point.append(msg)

        players_point: Set[str] = set()
        for msg in matched_msgs_point:
            pid = msg.get("ç©å®¶ID") or msg.get("å‘è¨€äººID") or msg.get("è§’è‰²ID")
            if pid:
                players_point.add(str(pid))

        U_point = len(players_point)
        M_point = len(matched_msgs_point)
        heat_point = compute_heat_score(U_point, M_point)

        dates, axes = [], []
        seen_date, seen_axis = set(), set()
        for seg in uniq_segs:
            d = seg["æ—¥æœŸ"]
            a = seg["æ—¶é—´è½´"]
            if d and d not in seen_date:
                seen_date.add(d)
                dates.append(d)
            if a and a not in seen_axis:
                seen_axis.add(a)
                axes.append(a)

        date_str_agg = "ã€".join(dates)
        axis_str_agg = "ã€".join(axes)

        info_item = {
            "t": t_text,
            "tid": tid_list,
            "æ—¥æœŸ": date_str_agg,
            "æ—¶é—´è½´": axis_str_agg,
            "æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨": uniq_segs,
            "å‘è¨€ç©å®¶æ€»æ•°": U_point,
            "å‘è¨€æ€»æ•°": M_point,
            "çƒ­åº¦è¯„åˆ†": heat_point,
        }
        discussion_info.append(info_item)

    return {
        "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥",
        "è®¨è®ºç‚¹info": discussion_info,
    }

####2.2 å¯¹æ‰€æœ‰ç‰ˆæœ¬è¯é¢˜ç°‡æ‰¹é‡è®¡ç®—######
def compute_all_clusters_point_metrics(
    version_clusters: List[Dict[str, Any]],
    daily_top5_rows: List[Dict[str, Any]],
    raw_jsonl_lines: List[str],
) -> List[Dict[str, Any]]:
    """
    å¯¹æ‰€æœ‰ç‰ˆæœ¬èšåˆè¯é¢˜ç°‡ï¼Œè®¡ç®—å…¶ä¸‹æ¯ä¸ªè®¨è®ºç‚¹çš„ï¼š
      - å‘è¨€ç©å®¶æ€»æ•°ï¼ˆå»é‡ï¼‰
      - å‘è¨€æ€»æ•°ï¼ˆä¸æŒ‰å†…å®¹å»é‡ï¼‰
      - çƒ­åº¦è¯„åˆ†ï¼ˆU * sqrt(M)ï¼‰

    è¿”å›ï¼š
      [
        {
          "èšåˆè¯é¢˜ç°‡": "...",
          "è®¨è®ºç‚¹info": [
            {
              "t": "...",
              "æ—¥æœŸ": "...",          # å¤šæ—¥æœŸæ‹¼æ¥
              "æ—¶é—´è½´": "a-bã€c-d",    # å¤šæ®µæ—¶é—´æ‹¼æ¥
              "å‘è¨€ç©å®¶æ€»æ•°": ...,
              "å‘è¨€æ€»æ•°": ...,
              "çƒ­åº¦è¯„åˆ†": ...
            },
            ...
          ]
        },
        ...
      ]
    """
    # 1) æ„å»º tid -> æ—¥æœŸæ—¶é—´è½´ ç´¢å¼•ï¼ˆæ¥è‡ª daily_top5ï¼‰
    tid_time_index = build_tid_time_index(daily_top5_rows)

    results: List[Dict[str, Any]] = []
    for cluster in version_clusters:
        metrics = compute_cluster_point_metrics(
            cluster=cluster,
            tid_time_index=tid_time_index,
            raw_jsonl_lines=raw_jsonl_lines,
        )
        results.append(metrics)

    return results

#######3ç­›é€‰å‡ºç‰ˆæœ¬å‘è¨€top5è¯æç°‡#######
from typing import List, Dict, Any

def extract_version_top5_clusters_from_point_metrics(
    clusters_with_points: List[Dict[str, Any]],
    top_k: int = 5,
) -> List[Dict[str, Any]]:
    """
    è¾“å…¥ï¼šcompute_all_clusters_point_metrics çš„ç»“æœï¼š
      [
        {
          "èšåˆè¯é¢˜ç°‡": "...",
          "è®¨è®ºç‚¹info": [
            {
              "t": "...",
              "tid": [...],
              "æ—¥æœŸ": "...",
              "æ—¶é—´è½´": "...",
              "æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨": [...],
              "å‘è¨€ç©å®¶æ€»æ•°": U_point,
              "å‘è¨€æ€»æ•°": M_point,
              "çƒ­åº¦è¯„åˆ†": heat_point
            },
            ...
          ]
        },
        ...
      ]

    é€»è¾‘ï¼š
      1ï¼‰å¯¹æ¯ä¸ªè¯é¢˜ç°‡ï¼šæŠŠæ——ä¸‹æ‰€æœ‰è®¨è®ºç‚¹çš„
         U / M / çƒ­åº¦è¯„åˆ† åšæ±‚å’Œï¼Œå¾—åˆ°ç°‡çº§æ€»é‡
      2ï¼‰æŒ‰ã€ç°‡çº§çƒ­åº¦è¯„åˆ†æ€»å’Œã€‘æ’åºï¼Œå– TopK è¯é¢˜ç°‡
      3ï¼‰è¾“å‡ºæ ¼å¼ä¸ºï¼š
        {
          "èšåˆè¯é¢˜ç°‡": topic,
          "è®¨è®ºç‚¹info": [...åŸæ ·ä¿ç•™...],
          "å‘è¨€ç©å®¶æ€»æ•°": U_sum,
          "å‘è¨€æ€»æ•°": M_sum,
          "çƒ­åº¦è¯„åˆ†": heat_sum
        }
    """

    cluster_summaries: List[Dict[str, Any]] = []

    for cluster in clusters_with_points:
        topic = (
            cluster.get("èšåˆè¯é¢˜ç°‡")
            or cluster.get("è¯é¢˜ç°‡")
            or "æœªçŸ¥"
        )
        points = cluster.get("è®¨è®ºç‚¹info") or []

        total_U = 0
        total_M = 0
        total_heat = 0.0

        for p in points:
            try:
                u = int(p.get("å‘è¨€ç©å®¶æ€»æ•°") or 0)
            except (TypeError, ValueError):
                u = 0
            try:
                m = int(p.get("å‘è¨€æ€»æ•°") or 0)
            except (TypeError, ValueError):
                m = 0
            try:
                h = float(p.get("çƒ­åº¦è¯„åˆ†") or 0.0)
            except (TypeError, ValueError):
                h = 0.0

            total_U += u
            total_M += m
            total_heat += h

        cluster_summaries.append({
            "èšåˆè¯é¢˜ç°‡": topic,
            "è®¨è®ºç‚¹info": points,              # â­ ä¿ç•™æ•´ä¸ªè®¨è®ºç‚¹åˆ—è¡¨
            "å‘è¨€ç©å®¶æ€»æ•°": total_U,
            "å‘è¨€æ€»æ•°": total_M,
            "çƒ­åº¦è¯„åˆ†": round(total_heat, 2),   # ç°‡çº§æ€»çƒ­åº¦ï¼ˆæ‰€æœ‰è®¨è®ºç‚¹çƒ­åº¦ä¹‹å’Œï¼‰
        })

    # æŒ‰ç°‡çº§çƒ­åº¦æ’åºï¼Œå– TopK
    cluster_summaries.sort(
        key=lambda x: x.get("çƒ­åº¦è¯„åˆ†", 0.0),
        reverse=True,
    )

    return cluster_summaries[:top_k]
#######ç‰ˆæœ¬top5çš„è®ºç‚¹å’Œæ—¶é—´æ‹‰å¹³æˆrows#########
from typing import List, Dict, Any

def print_mech_time_from_version_top5(
    version_top5_clusters: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    è¾“å…¥ï¼šç‰ˆæœ¬å‘è¨€Top5ç»“æœï¼ˆversion_top5_clustersï¼‰
    è¾“å‡ºï¼šæ¯ä¸ªè®¨è®ºç‚¹ä¸€è¡Œï¼Œåªä¿ç•™ï¼š
      {
        "è®¨è®ºç‚¹": "...",
        "æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨": [
          {"æ—¥æœŸ": "YYYY-MM-DD", "æ—¶é—´è½´": "HH:MM:SS-HH:MM:SS"},
          ...
        ]
      }
    """

    rows: List[Dict[str, Any]] = []

    for cluster in version_top5_clusters:
        points = cluster.get("è®¨è®ºç‚¹info") or []
        for p in points:
            t_text = (p.get("t") or "").strip()
            if not t_text:
                continue

            # ä¼˜å…ˆç”¨å·²ç»ç®—å¥½çš„ æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨
            dt_list = p.get("æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨") or []

            # å¦‚æœæ¨¡å‹è¾“å‡ºé‡Œæš‚æ—¶æ²¡æœ‰è¿™ä¸ªå­—æ®µï¼Œå°±ç”¨ æ—¥æœŸ + æ—¶é—´è½´ å…œåº•æ‹¼ä¸€æ¡
            if not dt_list:
                date_str = (p.get("æ—¥æœŸ") or "").strip()
                axis_str = (p.get("æ—¶é—´è½´") or "").strip()
                if date_str and axis_str:
                    dt_list = [{"æ—¥æœŸ": date_str, "æ—¶é—´è½´": axis_str}]

            # å®Œå…¨æ²¡æœ‰æ—¶é—´ä¿¡æ¯å°±è·³è¿‡
            if not dt_list:
                continue

            row = {
                "è®¨è®ºç‚¹": t_text,
                "æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨": dt_list,
            }
            rows.append(row)

    return rows
######æ ¹æ®è®¨è®ºç‚¹çš„æ—¶é—´è½´æå–åŸæ–‡#######
def get_dialogs_lines_by_dt_list_debug(
    raw_jsonl_lines: List[str],
    dt_list: List[Dict[str, str]],
    debug: bool = False,
) -> List[str]:
    """
    è¾“å…¥ï¼š
      - raw_jsonl_linesï¼šåŸå§‹å‘è¨€ jsonl åˆ—è¡¨ï¼ˆæ¯è¡Œä¸€ä¸ª JSONï¼‰
      - dt_listï¼šå½¢å¦‚ [{"æ—¥æœŸ":"2025-12-03","æ—¶é—´è½´":"10:14:51-10:16:19"}, ...]
    è¾“å‡ºï¼š
      - dialogs_linesï¼šåŒ¹é…åˆ°çš„åŸæ–‡ jsonl è¡Œåˆ—è¡¨ï¼ˆå»é‡åï¼‰
    """

    # 1) è§£æåŸå§‹ jsonl ä¸º dict
    messages: List[Dict[str, Any]] = []
    for line in raw_jsonl_lines:
        s = (line or "").strip()
        if not s:
            continue
        try:
            obj = json.loads(s)
        except Exception:
            continue
        messages.append(obj)

    all_msgs: List[Dict[str, Any]] = []
    seen_msg_ids: Set[Any] = set()

    # 2) é€ä¸ªæ—¶é—´ç‰‡è°ƒç”¨ match_dialogs_by_time
    for seg in dt_list:
        date_str = (seg.get("æ—¥æœŸ") or "").strip()
        axis_str = (seg.get("æ—¶é—´è½´") or "").strip()
        if not date_str or not axis_str:
            continue

        seg_msgs = match_dialogs_by_time(messages, date_str, axis_str)

        if debug:
            print(f"ğŸ” æ—¶é—´ç‰‡ {date_str} {axis_str} å‘½ä¸­ {len(seg_msgs)} æ¡æ¶ˆæ¯")

        for msg in seg_msgs:
            # ä¼˜å…ˆä½¿ç”¨ _idx å»é‡
            key = msg.get("_idx")
            if key is None:
                key = (
                    msg.get("å‘è¨€æ—¥æœŸ"),
                    msg.get("å‘è¨€æ—¶é—´"),
                    msg.get("ç©å®¶ID") or msg.get("å‘è¨€äººID") or msg.get("è§’è‰²ID"),
                    msg.get("ç©å®¶æ¶ˆæ¯") or msg.get("å†…å®¹") or msg.get("æ–‡æœ¬") or msg.get("æ¶ˆæ¯å†…å®¹"),
                )

            if key in seen_msg_ids:
                continue
            seen_msg_ids.add(key)
            all_msgs.append(msg)

    # 3) æŒ‰æ—¶é—´æ’åºï¼ˆå¯é€‰ï¼Œä½†ä¸€èˆ¬å¾ˆæœ‰ç”¨ï¼‰
    def _time_key(m: Dict[str, Any]):
        d = m.get("å‘è¨€æ—¥æœŸ") or ""
        t = m.get("å‘è¨€æ—¶é—´") or ""
        try:
            return datetime.strptime(f"{d} {t}", "%Y-%m-%d %H:%M:%S")
        except Exception:
            return datetime.max

    all_msgs.sort(key=_time_key)

    # 4) è½¬å› jsonl è¡Œ
    dialogs_lines: List[str] = [json.dumps(m, ensure_ascii=False) for m in all_msgs]

    if debug:
        print(f"âœ… å…±åˆå¹¶å‡º {len(dialogs_lines)} æ¡æ¶ˆæ¯ï¼ˆå·²å»é‡ï¼‰")

    return dialogs_lines
#################ç‰ˆæœ¬è¾“å‡ºæ‹†åˆ†###########
#------------------------æ¨¡å‹4è¾“å‡ºæ‹†åˆ†---------------------------------

def parse_opinion_output_to_list(opinion_output: str) -> List[Dict[str, Any]]:
    """
    æŠŠæ¨¡å‹4è¿”å›çš„å­—ç¬¦ä¸²è§£ææˆ List[dict]ï¼š
    - æ”¯æŒï¼š
      * å•ä¸ª JSON å¯¹è±¡
      * JSON æ•°ç»„
      * jsonlï¼ˆå¤šè¡Œï¼Œæ¯è¡Œä¸€ä¸ª JSONï¼‰
      * ```json ä»£ç å— + å¤šè¡Œæ¼‚äº® JSON
    """
    if not opinion_output:
        return []

    s = opinion_output.strip()

    # 0) å»æ‰ ```json / ``` å¤–å£³
    s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
    s = re.sub(r"\s*```$", "", s)
    s = s.strip()

    # 1) å…ˆå°è¯•æ•´ä½“è§£æï¼ˆèƒ½åƒæ‰å•å¯¹è±¡ / æ•°ç»„ï¼‰
    try:
        obj = json.loads(s)
        if isinstance(obj, dict):
            return [obj]
        if isinstance(obj, list):
            return [x for x in obj if isinstance(x, dict)]
    except json.JSONDecodeError:
        pass

    # 2) ç”¨â€œæ‹¬å·æ·±åº¦â€æ–¹å¼ï¼Œä»å¤šè¡Œé‡Œæ‹¼å‡ºä¸€ä¸ªæˆ–å¤šä¸ª JSON å¯¹è±¡
    objs: List[Dict[str, Any]] = []
    buf: List[str] = []
    depth = 0

    for raw in s.splitlines():
        line = (raw or "").strip()
        if not line:
            continue

        # å»æ‰ markdown åˆ—è¡¨å‰ç¼€ï¼Œå¦‚ "- { ... }"
        if line.startswith("- "):
            line = line[2:].lstrip()

        # å¦‚æœè¿™ä¸€è¡Œå®Œå…¨ä¸å« { æˆ– }ï¼Œä¸”å½“å‰æ·±åº¦ä¸º 0ï¼ŒåŸºæœ¬æ˜¯è§£é‡Šæ–‡å­—ï¼Œè·³è¿‡
        if depth == 0 and "{" not in line:
            continue

        # è¿›å…¥/ç»§ç»­ä¸€ä¸ªå¯¹è±¡
        open_cnt = line.count("{")
        close_cnt = line.count("}")

        if depth == 0 and "{" in line:
            # æ–°å¯¹è±¡çš„å¼€å§‹
            buf = [line]
            depth = open_cnt - close_cnt
            if depth <= 0:
                # å•è¡Œå¯¹è±¡ï¼š{...}
                text = "\n".join(buf)
                try:
                    obj = json.loads(text)
                    if isinstance(obj, dict):
                        objs.append(obj)
                except json.JSONDecodeError:
                    pass
                buf = []
                depth = 0
        else:
            # å·²ç»åœ¨å¯¹è±¡å†…éƒ¨
            buf.append(line)
            depth += open_cnt - close_cnt
            if depth <= 0:
                # å¯¹è±¡ç»“æŸ
                text = "\n".join(buf)
                try:
                    obj = json.loads(text)
                    if isinstance(obj, dict):
                        objs.append(obj)
                except json.JSONDecodeError:
                    pass
                buf = []
                depth = 0

    return objs
#############è¡¥å……è®¨è®ºç‚¹å’Œå‘è¨€ç¤ºä¾‹è‡³ç‰ˆæœ¬top5###############
from typing import List, Dict, Any

def merge_version_top5_with_opinions(
    version_top5_points: List[Dict[str, Any]],
    version_opinions: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    å°†ç‰ˆæœ¬å‘è¨€Top5çš„é‡åŒ–ç»“æœï¼ˆversion_top5_pointsï¼‰
    ä¸ æ¨¡å‹#6çš„è®¨è®ºç‚¹è§‚ç‚¹ç»“æœï¼ˆversion_opinionsï¼‰åˆå¹¶ï¼Œè¾“å‡ºä¸ºä½ æŒ‡å®šçš„æ ¼å¼ï¼š

    {
      "èšåˆè¯é¢˜ç°‡": "...",
      "æ—¥æœŸ": "2026-01-02",
      "æ—¶é—´è½´": "19:29:32-20:00:51",
      "è®¨è®ºç‚¹åˆ—è¡¨": [
        {
          "è®¨è®ºç‚¹1": "...",
          "ç©å®¶è§‚ç‚¹": [...],
          "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": [...]
        },
        {
          "è®¨è®ºç‚¹2": "...",
          ...
        }
      ]
    }
    """

    # 1) å…ˆå»ºä¸€ä¸ª map: è®¨è®ºç‚¹æ–‡æœ¬ -> æ¨¡å‹#6çš„è§‚ç‚¹ç»“æœ
    opinion_map: Dict[str, Dict[str, Any]] = {}
    for op in version_opinions:
        key = (op.get("è®¨è®ºç‚¹") or "").strip()
        if not key:
            continue
        # å¦‚æœåŒåè®¨è®ºç‚¹å¤šæ¬¡å‡ºç°ï¼Œåå†™å…¥çš„ä¼šè¦†ç›–å‰ä¸€ä¸ªâ€”â€”ä¸€èˆ¬ä½ çš„æµç¨‹é‡Œä¸ä¼šé‡å¤
        opinion_map[key] = op

    merged_results: List[Dict[str, Any]] = []

    # 2) éå†æ¯ä¸ªç‰ˆæœ¬ Top5 è¯é¢˜ç°‡
    for cluster in version_top5_points:
        topic = (
            cluster.get("èšåˆè¯é¢˜ç°‡")
            or cluster.get("è¯é¢˜ç°‡")
            or "æœªçŸ¥"
        )

        points = cluster.get("è®¨è®ºç‚¹info") or []

        # -------- èšåˆæ•´ä¸ªç°‡çš„ æ—¥æœŸ & æ—¶é—´è½´ï¼ˆå¤šæ—¥ã€å¤šæ—¶é—´æ®µç”¨é¡¿å·æ‹¼ï¼‰ --------
        date_list: List[str] = []
        time_list: List[str] = []
        seen_dates = set()
        seen_times = set()

        for p in points:
            # p["æ—¥æœŸ"] å¯èƒ½æ˜¯ "2025-12-02" æˆ– "2025-12-02ã€2025-12-03"
            raw_date = (p.get("æ—¥æœŸ") or "").strip()
            raw_axis = (p.get("æ—¶é—´è½´") or "").strip()

            if raw_date:
                for d in raw_date.split("ã€"):
                    d = d.strip()
                    if d and d not in seen_dates:
                        seen_dates.add(d)
                        date_list.append(d)

            if raw_axis:
                for a in raw_axis.split("ã€"):
                    a = a.strip()
                    if a and a not in seen_times:
                        seen_times.add(a)
                        time_list.append(a)

        cluster_date = "ã€".join(date_list)
        cluster_axis = "ã€".join(time_list)

        # -------- ç»„è£… è®¨è®ºç‚¹åˆ—è¡¨ï¼šè®¨è®ºç‚¹1 / è®¨è®ºç‚¹2 / ... --------
        discussion_items: List[Dict[str, Any]] = []

        for idx, p in enumerate(points, start=1):
            t_text = (p.get("t") or "").strip()
            if not t_text:
                continue

            op = opinion_map.get(t_text, {})  # æ‰¾ä¸åˆ°å°±ç»™ç©ºå£³

            item = {
                f"è®¨è®ºç‚¹{idx}": t_text,
                "ç©å®¶è§‚ç‚¹": op.get("ç©å®¶è§‚ç‚¹") or [],
                "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": op.get("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹") or [],
            }
            discussion_items.append(item)

        merged_cluster = {
            "èšåˆè¯é¢˜ç°‡": topic,
            "æ—¥æœŸ": cluster_date,
            "æ—¶é—´è½´": cluster_axis,
            "è®¨è®ºç‚¹åˆ—è¡¨": discussion_items,
        }

        merged_results.append(merged_cluster)

    return merged_results

#######æå–ç‰ˆæœ¬top5é‡åŒ–æ•°æ®####################
from typing import Dict, Any, List

def compute_cluster_date_coverage_raw(cluster: Dict[str, Any]) -> Dict[str, Any]:
    """
    ä»ç‰ˆæœ¬Top5çš„ä¸€æ¡è®°å½•é‡Œï¼ŒæŠ½å–æ‰€æœ‰è®¨è®ºç‚¹çš„æ—¥æœŸï¼Œè®¡ç®—ï¼š
      - è¦†ç›–å¤©æ•°
      - èµ·æ­¢æ—¥æœŸ
      - è¦†ç›–æ—¥æœŸåˆ—è¡¨ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼‰
    """
    dates_set = set()

    points = cluster.get("è®¨è®ºç‚¹info") or []
    for p in points:
        # ä¼˜å…ˆç”¨ æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨
        dt_list = p.get("æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨") or []
        if isinstance(dt_list, list) and dt_list:
            for seg in dt_list:
                d = (seg.get("æ—¥æœŸ") or "").strip()
                if d:
                    dates_set.add(d)
        else:
            # å…œåº•ï¼šç”¨å•ç‹¬çš„ æ—¥æœŸ å­—æ®µ
            d = (p.get("æ—¥æœŸ") or "").strip()
            if d:
                dates_set.add(d)

    if not dates_set:
        return {
            "è®¨è®ºè¦†ç›–å¤©æ•°": 0,
            "èµ·å§‹æ—¥æœŸ": "",
            "ç»“æŸæ—¥æœŸ": "",
            "è¦†ç›–æ—¥æœŸåˆ—è¡¨": []
        }

    sorted_dates = sorted(dates_set)
    coverage_days = len(sorted_dates)

    return {
        "è®¨è®ºè¦†ç›–å¤©æ•°": coverage_days,
        "èµ·å§‹æ—¥æœŸ": sorted_dates[0],
        "ç»“æŸæ—¥æœŸ": sorted_dates[-1],
        "è¦†ç›–æ—¥æœŸåˆ—è¡¨": sorted_dates,
    }



def build_cluster_heat_summary(
    data: Union[Dict[str, Any], List[Dict[str, Any]]]
) -> Union[Dict[str, Any], List[Dict[str, Any]]]:
    """
    é€šç”¨ç‰ˆï¼š
    - å¦‚æœä¼ è¿›æ¥çš„æ˜¯ã€å•æ¡ dictã€‘ï¼ˆä¸€æ¡ç‰ˆæœ¬Top5è®°å½•ï¼‰ï¼Œè¿”å›ä¸€æ¡æ±‡æ€» dict
    - å¦‚æœä¼ è¿›æ¥çš„æ˜¯ã€list[dict]ã€‘ï¼ˆversion_top5_points æ•´ä¸ªåˆ—è¡¨ï¼‰ï¼Œè¿”å› list[æ±‡æ€»dict]

    å•æ¡è¾“å…¥ç¤ºä¾‹ï¼ˆåŸæ¥é‚£ç§ï¼‰ï¼š
      {
        "èšåˆè¯é¢˜ç°‡": "...",
        "è®¨è®ºç‚¹info": [...],
        "å‘è¨€ç©å®¶æ€»æ•°": 82,
        "å‘è¨€æ€»æ•°": 586,
        "çƒ­åº¦è¯„åˆ†": 1178.72
      }

    å•æ¡è¾“å‡ºç¤ºä¾‹ï¼š
      {
        "èšåˆè¯é¢˜ç°‡": topic,
        "è®¨è®ºè¦†ç›–å¤©æ•°": "3å¤©ï¼ˆ2025-12-04 ~ 2025-12-07ï¼‰",
        "å‘è¨€ç©å®¶æ€»æ•°": total_u,
        "å‘è¨€æ€»é‡": total_m,
        "çƒ­åº¦è¯„åˆ†": total_heat
      }
    """

    def _summary_one(cluster: Dict[str, Any]) -> Dict[str, Any]:
        topic = (
            cluster.get("èšåˆè¯é¢˜ç°‡")
            or cluster.get("è¯é¢˜ç°‡")
            or "æœªçŸ¥"
        )

        # â€”â€” è®¡ç®—è¦†ç›–å¤©æ•° & æ—¥æœŸèŒƒå›´ â€”â€” 
        cov = compute_cluster_date_coverage_raw(cluster)
        days = cov["è®¨è®ºè¦†ç›–å¤©æ•°"]
        start = cov["èµ·å§‹æ—¥æœŸ"]
        end = cov["ç»“æŸæ—¥æœŸ"]

        if days > 0 and start and end:
            cover_days_str = f"{days}å¤©ï¼ˆ{start} ~ {end}ï¼‰"
        else:
            cover_days_str = "0å¤©"

        # â€”â€” è¯»å–ç°‡çº§åˆ«çš„ U/M/heatï¼ˆæ³¨æ„è½¬æˆæ•°å€¼ï¼‰â€”â€” 
        try:
            total_u = int(cluster.get("å‘è¨€ç©å®¶æ€»æ•°") or 0)
        except (TypeError, ValueError):
            total_u = 0

        try:
            total_m = int(cluster.get("å‘è¨€æ€»æ•°") or 0)
        except (TypeError, ValueError):
            total_m = 0

        try:
            total_heat = float(cluster.get("çƒ­åº¦è¯„åˆ†") or 0.0)
        except (TypeError, ValueError):
            total_heat = 0.0

        return {
            "èšåˆè¯é¢˜ç°‡": topic,
            "è®¨è®ºè¦†ç›–å¤©æ•°": cover_days_str,   # "3å¤©ï¼ˆ2025-12-04 ~ 2025-12-07ï¼‰"
            "å‘è¨€ç©å®¶æ€»æ•°": total_u,
            "å‘è¨€æ€»é‡": total_m,
            "çƒ­åº¦è¯„åˆ†": total_heat,
        }

    # ===== åˆ†ä¸¤ç§æƒ…å†µå¤„ç† =====
    if isinstance(data, list):
        # ä½ ç°åœ¨çš„ç”¨æ³•ï¼šä¼ çš„æ˜¯ version_top5_pointsï¼ˆlistï¼‰
        return [_summary_one(c) for c in data]
    else:
        # å…¼å®¹æ—§ç”¨æ³•ï¼šä¼ å•æ¡ dict
        return _summary_one(data)

#######æŠ½å–ç‰ˆæœ¬top5ä¸­è¯æç°‡å’Œè®¨è®ºç‚¹ä¿¡æ¯ç”¨äºçƒ­åº¦è¶‹åŠ¿åˆ†æ##############
from typing import List, Dict, Any
import json

def extract_topic_and_points_from_version_top5(
    version_top5_points: List[Dict[str, Any]]
) -> List[Dict[str, Any]]:
    """
    è¾“å…¥ï¼šversion_top5_pointsï¼ˆç‰ˆæœ¬å‘è¨€Top5å®Œæ•´ç»“æ„ï¼‰
    è¾“å‡ºï¼šåªä¿ç•™ã€èšåˆè¯é¢˜ç°‡ + è®¨è®ºç‚¹infoã€‘ï¼š
      [
        {
          "èšåˆè¯é¢˜ç°‡": "...",
          "è®¨è®ºç‚¹info": [  # åŸæ ·ä¿ç•™
            {
              "t": "...",
              "tid": [...],
              "æ—¥æœŸ": "...",
              "æ—¶é—´è½´": "...",
              "æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨": [...],
              "å‘è¨€ç©å®¶æ€»æ•°": ...,
              "å‘è¨€æ€»æ•°": ...,
              "çƒ­åº¦è¯„åˆ†": ...
            },
            ...
          ]
        },
        ...
      ]
    """
    results: List[Dict[str, Any]] = []

    for cluster in version_top5_points:
        topic = (
            cluster.get("èšåˆè¯é¢˜ç°‡")
            or cluster.get("è¯é¢˜ç°‡")
            or "æœªçŸ¥"
        )

        points_raw = cluster.get("è®¨è®ºç‚¹info") or []

        results.append({
            "èšåˆè¯é¢˜ç°‡": topic,
            "è®¨è®ºç‚¹info": points_raw,   # âœ… åŸæ ·å¸¦è¿‡å»
        })

    return results


def build_heat_trend_input_jsonl(
    version_top5_points: List[Dict[str, Any]]
) -> str:
    """
    ç»™ã€çƒ­åº¦è¶‹åŠ¿æ™ºèƒ½ä½“ã€‘å–‚çš„ JSONLï¼š
    æ¯è¡Œä¸€ä¸ªè¯é¢˜ç°‡å¯¹è±¡ï¼š
      {"èšåˆè¯é¢˜ç°‡": "...", "è®¨è®ºç‚¹info": [...]}
    """
    cleaned = extract_topic_and_points_from_version_top5(version_top5_points)
    lines = [json.dumps(obj, ensure_ascii=False) for obj in cleaned]
    return "\n".join(lines)
######ç‰ˆæœ¬è¾“å‡ºæ±‡æ€»#######
def merge_version_final_summary(
    merged_opinion_version: List[Dict[str, Any]],
    heated_stats: List[Dict[str, Any]],
    heat_trend_results: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    è¾“å…¥ï¼š
      - merged_opinion_versionï¼šæ¨¡å‹#6 åˆå¹¶åçš„è§‚ç‚¹ç»“æœï¼ˆå« è®¨è®ºç‚¹åˆ—è¡¨ï¼‰
      - heated_statsï¼šæ¯ä¸ªèšåˆè¯é¢˜ç°‡çš„é‡åŒ–ç»Ÿè®¡ï¼ˆè®¨è®ºè¦†ç›–å¤©æ•° / U / M / çƒ­åº¦è¯„åˆ†ï¼‰
      - heat_trend_resultsï¼šæ¨¡å‹#7 ä¸€å¥è¯çƒ­åº¦è¶‹åŠ¿ç»“æœ

    è¾“å‡ºï¼š
      - final_version_outputs: List[dict]ï¼Œæ¯æ¡ç»“æ„ä¸ºï¼š
        {
          "è¯æç°‡æ ‡é¢˜": "...",
          "è®¨è®ºçƒ­åº¦ï¼ˆé‡åŒ–ï¼‰": {
            "è®¨è®ºè¦†ç›–å¤©æ•°": "...",
            "å‘è¨€ç©å®¶æ€»æ•°": X,
            "å‘è¨€æ€»é‡": Y,
            "çƒ­åº¦è¯„åˆ†": Z,
            "çƒ­åº¦è¶‹åŠ¿": "ä¸€å¥è¯"
          },
          "è®¨è®ºç‚¹åˆ—è¡¨": [...æ¨¡å‹6é‡Œçš„è®¨è®ºç‚¹åˆ—è¡¨...]
        }
      å¹¶æŒ‰ çƒ­åº¦è¯„åˆ† ä»å¤§åˆ°å°æ’åºã€‚
    """

    # 1) å»ºç´¢å¼•ï¼štopic -> è§‚ç‚¹ç»“æœ
    opinion_by_topic: Dict[str, Dict[str, Any]] = {}
    for row in merged_opinion_version:
        t = (row.get("èšåˆè¯é¢˜ç°‡") or row.get("è¯é¢˜ç°‡") or "").strip()
        if t:
            opinion_by_topic[t] = row

    # 2) å»ºç´¢å¼•ï¼štopic -> ä¸€å¥è¯è¶‹åŠ¿
    trend_by_topic: Dict[str, str] = {}
    for row in heat_trend_results:
        t = (row.get("èšåˆè¯é¢˜ç°‡") or row.get("è¯é¢˜ç°‡") or "").strip()
        if not t:
            continue
        summary = (row.get("ä¸€å¥è¯æ€»ç»“") or "").strip()
        trend_by_topic[t] = summary

    # 3) ä»¥ heated_stats ä¸ºåŸºå‡†é¡ºåºï¼Œæ‹¼ä¸‰ä»½ä¿¡æ¯
    final_version_outputs: List[Dict[str, Any]] = []

    for stat in heated_stats:
        topic = (stat.get("èšåˆè¯é¢˜ç°‡") or stat.get("è¯é¢˜ç°‡") or "").strip()
        if not topic:
            continue

        op = opinion_by_topic.get(topic)
        if not op:
            print(f"âš  æœªæ‰¾åˆ°è§‚ç‚¹ç»“æœ merged_opinion_versionï¼š{topic}")
            continue

        trend = trend_by_topic.get(topic, "")

        # é‡åŒ–ä¿¡æ¯
        cover_days_str = stat.get("è®¨è®ºè¦†ç›–å¤©æ•°", "")
        total_u = stat.get("å‘è¨€ç©å®¶æ€»æ•°", 0)
        total_m = stat.get("å‘è¨€æ€»é‡", 0)
        try:
            heat_score = float(stat.get("çƒ­åº¦è¯„åˆ†") or 0.0)
        except (TypeError, ValueError):
            heat_score = 0.0

        # è®¨è®ºç‚¹åˆ—è¡¨ï¼šç›´æ¥ç”¨æ¨¡å‹6åˆå¹¶åçš„ç»“æ„
        discussion_list = op.get("è®¨è®ºç‚¹åˆ—è¡¨") or []

        final_item = {
            "è¯æç°‡æ ‡é¢˜": topic,
            "è®¨è®ºçƒ­åº¦ï¼ˆé‡åŒ–ï¼‰": {
                "è®¨è®ºè¦†ç›–å¤©æ•°": cover_days_str,
                "å‘è¨€ç©å®¶æ€»æ•°": total_u,
                "å‘è¨€æ€»é‡": total_m,
                "çƒ­åº¦è¯„åˆ†": heat_score,
                "çƒ­åº¦è¶‹åŠ¿": trend,
            },
            "è®¨è®ºç‚¹åˆ—è¡¨": discussion_list,
        }

        final_version_outputs.append(final_item)

    # 4) æŒ‰ çƒ­åº¦è¯„åˆ† ä»å¤§åˆ°å°æ’åº
    final_version_outputs.sort(
        key=lambda x: x.get("è®¨è®ºçƒ­åº¦ï¼ˆé‡åŒ–ï¼‰", {}).get("çƒ­åº¦è¯„åˆ†", 0.0),
        reverse=True,
    )

    return final_version_outputs

