{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a270f7e-1cbc-488e-b78b-d113cf2b2146",
   "metadata": {},
   "source": [
    "## æ•°æ®å¤„ç†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bcb45d8-1ccc-4600-b1fb-9e30f9b83c8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "import re\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Union, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "import os, re, json, time\n",
    "import typing as T\n",
    "import requests\n",
    "\n",
    "from pathlib import Path\n",
    "from openpyxl import Workbook, load_workbook\n",
    "\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.utils import get_column_letter\n",
    "from openpyxl.styles import Font, Alignment, Border, Side, PatternFill\n",
    "from data_processing import load_and_process,build_jsonl_for_range, save_jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f289cc4f-5fbb-4e00-bf55-1bd0fe391231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11803 lines\n",
      "{\"å‘è¨€æ—¥æœŸ\": \"2025-12-01\", \"å‘è¨€æ—¶é—´\": \"00:00:30\", \"å®¢æœID\": \"é’ç“·æ¸¸æˆå®¢æœ-å“¼å”§(3008694133)\", \"å®¢æœæ¶ˆæ¯\": \"å“¼\"}\n"
     ]
    }
   ],
   "source": [
    "## ç ”å‘å­—å…¸\n",
    "speaker_map = {\n",
    "    \"16186514\":   \"peteræœ¬å°Š\",\n",
    "    \"1655611808\": \"è¿è¥ç»¾ç»¾\",\n",
    "    \"2073820674\": \"æ²™åˆ©æ–‡è€å¸ˆ\",\n",
    "    \"2726067525\": \"milissa\",\n",
    "}\n",
    "## å®¢æœå­—å…¸\n",
    "MAPPING_FILE = \"mappingåœ°çƒ1.xlsx\"\n",
    "\n",
    "##QQçš„txtæ–‡ä»¶\n",
    "pathtxt   = \"1209ã€Šæ¬¢è¿æ¥åˆ°åœ°çƒã€‹æµ‹è¯•2ç¾¤.txt\"\n",
    "\n",
    "# è®¾å®šæ—¶é—´èŒƒå›´\n",
    "start_time = \"2025-12-01 00:00:00\"\n",
    "end_time   = \"2025-12-07 00:00:00\"\n",
    "\n",
    "\n",
    "# 1) æ‹¿åˆ° JSONLï¼ˆåˆ—è¡¨ï¼‰\n",
    "jsonl_lines01 = build_jsonl_for_range(\n",
    "    pathtxt=pathtxt,\n",
    "    mapping_file=MAPPING_FILE,\n",
    "    speaker_map=speaker_map,\n",
    "    start_time=start_time,\n",
    "    end_time=end_time,\n",
    "    return_str=False,   # è¿”å› list[str]\n",
    ")\n",
    "\n",
    "print(len(jsonl_lines01), \"lines\")\n",
    "print(jsonl_lines01[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "840c1e89-cc08-4532-bc6b-1842742fce13",
   "metadata": {},
   "source": [
    "## å¤§æ¨¡å‹åˆ†ç±»"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d8e640-2974-47ea-a008-6c08e71b46bd",
   "metadata": {},
   "source": [
    "## å®šä¹‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e952a6ae-48c3-49f3-9b91-d867f416c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "æ‰¹å¤„ç† 10000 æ¡èŠå¤©æ•°æ®ï¼ˆæ¯æ‰¹ 100 æ¡ï¼‰ï¼š\n",
    "- æ¨¡å‹#1ï¼šè¿‡æ»¤éæ¸¸æˆç›¸å…³ï¼ˆåªä¿ç•™ç›¸å…³ JSON è¡Œï¼ŒåŸæ ·è¾“å‡ºï¼‰\n",
    "- æ¨¡å‹#2ï¼šæå–é«˜è®¨è®ºçš„å‘è¨€å¹¶åˆ†æ\n",
    "- ç»“æœæŒ‰wordæ ¼å¼æ–‡æ¡£è¾“å‡º\n",
    "\"\"\"\n",
    "from model_classifyV1_Copy1_Copy1 import (\n",
    "    load_system_prompt,\n",
    "    build_user_prompt_filter,build_user_prompt_clsuter,call_ark_chat_completions,\n",
    "    extract_valid_json_lines,add_index_to_jsonl_lines,count_output_filter_stats,get_covered_indices_from_cluster_output,\n",
    "    aggregate_cluster_outputs,build_user_prompt_cluster_agg,assign_global_cluster_ids,\n",
    "    extract_top5_heat_clusters,attach_discussion_points,extract_cluster_stats,append_daily_top5_to_version_jsonl,infer_date_for_batch,\n",
    "    match_dialogs_by_time,build_user_prompt_subcluster_opinion,extract_time_axis_from_title,\n",
    "    parse_and_normalize_opinion_output,build_daily_top5_opinion_records,fix_output_cluster_agg_keys\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b7b9c0-b495-4640-b7d1-2c31ef371d3d",
   "metadata": {},
   "source": [
    "## è®¾ç½®å‚æ•°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "70d36c9f-a439-4597-b52a-ca091439031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============= ä½ çš„æ¨¡å‹ä¸æ–‡ä»¶é…ç½®ï¼ˆæ”¹è¿™é‡Œï¼‰ =============\n",
    "API_URL   = \"https://ark.cn-beijing.volces.com/api/v3/chat/completions\" \n",
    "API_KEY = \"de91deb0-aae6-46cb-bac0-17ac3b6107f5\" #API\n",
    "V3_MODEL_ID= \"ep-20251020160142-5d7hp\"#æ¥å…¥ç‚¹\n",
    "V3_1_MODEL_ID = \"ep-20251020160025-9p5tj\"#æ¥å…¥ç‚¹\n",
    "R1_MODEL_ID = \"ep-20251020160103-5n6g2\"#æ¥å…¥ç‚¹\n",
    "\n",
    "PROMPT_MD_PATH01 = Path(\"æç¤ºè¯1.md\") # æ¨¡å‹#1 system æç¤ºè¯ï¼ˆç­›ç›¸å…³ï¼‰\n",
    "PROMPT_MD_PATH02 = Path(\"2è¯é¢˜åˆ†ç±».md\") # æ¨¡å‹#2 system æç¤ºè¯ï¼ˆåˆ†è¯é¢˜ï¼‰\n",
    "PROMPT_MD_PATH03 = Path(\"3æ—¥èšåˆ.md\") # æ¨¡å‹#3 system æç¤ºè¯ï¼ˆæ—¥èšåˆï¼‰\n",
    "PROMPT_MD_PATH04 = Path(\"2è¯é¢˜åˆ†ç±»å’Œæ€»ç»“.md\") #æ¨¡å‹#4 system æç¤ºè¯ï¼ˆè§‚ç‚¹åˆ†æï¼‰\n",
    "VERSION_TOP5_JSONL = \"version_daily_top5_with_opinion.jsonl\"\n",
    "BATCH_SIZE       = 300\n",
    "SLEEP_BETWEEN    = 1   # æ¯æ‰¹ä¹‹é—´çš„é—´éš”ï¼Œé˜²æ­¢QPSè§¦å‘é™æµï¼›æŒ‰éœ€è°ƒæ•´\n",
    "RETRIES          = 2\n",
    "TEMPERATURE      = 0.20\n",
    "MAX_TOKENS       = 16384\n",
    "TIMEOUT_SEC      = 600\n",
    "# ====================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b2dc1-1a41-4ca6-9959-cccbb22adb86",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1.æ£€éªŒæ¯æ—¥è¯æç°‡åˆ†ç±»è¾“å‡º"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ca2f9f-62ca-4df1-aeae-2ead09fb39de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡å¤„ç† 141 æ¡ï¼Œå…± 1 æ‰¹ï¼ˆæ¯æ‰¹ 300 æ¡ï¼‰ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦:   0%|                                                                                                                                     | 0/1 [02:39<?, ?æ‰¹/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[æ‰¹æ¬¡ 1] æ¨¡å‹#1 è¾“å‡ºæ€»è¡Œæ•°ï¼š89ï¼Œå…¶ä¸­ç©å®¶å‘è¨€ï¼š89 æ¡ï¼ˆä¸å»é‡ï¼‰\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦:   0%|                                                                                                                                     | 0/1 [02:46<?, ?æ‰¹/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===== æ‰¹æ¬¡ 1/1 è¯é¢˜ç°‡è¾“å‡º =====\n",
      "{\"è¯é¢˜ç°‡\":\"åœæœç»´æŠ¤è¡¥å¿è®¨è®ºï¼ˆ2025-11-19 15:02:03-15:11:27ï¼‰\",\"æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶\":\"åœæœç»´æŠ¤è¡¥å¿æœºåˆ¶ï¼Œç©å®¶è®¨è®ºç»´æŠ¤æ—¶é•¿ä¸è¡¥å¿æœŸæœ›ï¼ˆ100æŠ½ã€50æŠ½ã€çŸ³å¸ã€ä»£å¸ç­‰ï¼‰\"}\n",
      "{\"è¯é¢˜ç°‡\":\"æ¸¸æˆä¸‹è½½ä¸å®‰è£…é—®é¢˜ï¼ˆ2025-11-19 15:02:22-15:13:20ï¼‰\",\"æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶\":\"æ¸¸æˆå®‰è£…åŒ…ä¸‹è½½ä¸æ›´æ–°æœºåˆ¶ï¼Œç©å®¶è¯¢é—®ä¸‹è½½é“¾æ¥ã€æ˜¯å¦éœ€è¦é‡æ–°ä¸‹è½½ç­‰é—®é¢˜\"}\n",
      "{\"è¯é¢˜ç°‡\":\"è§’è‰²æŠ€èƒ½æ›´æ–°è®¨è®ºï¼ˆ2025-11-19 15:02:58-15:04:02ï¼‰\",\"æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶\":\"çŒ›çŠ¸è±¡å’Œä»™äººæŒè§’è‰²çš„PTSDä¸å¼ºè¿«ç—‡è¢«åŠ¨æŠ€èƒ½æ›´æ–°å®è£…æƒ…å†µ\"}\n",
      "{\"è¯é¢˜ç°‡\":\"å®¢æœäº’åŠ¨ä¸è°ƒä¾ƒï¼ˆ2025-11-19 15:07:27-15:14:50ï¼‰\",\"æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶\":\"å®¢æœæœåŠ¡ä½“éªŒï¼Œç©å®¶ä¸å®¢æœè–ç±³ã€è“æ¡‰çš„äº’åŠ¨è°ƒä¾ƒåŠæ— å…³æ¸¸æˆå†…å®¹çš„é—²èŠ\"}\n",
      "===========================================\n",
      "\n",
      "[æ‰¹æ¬¡ 1] è¯é¢˜ç°‡è¦†ç›–åŸå§‹å‘è¨€æ¡æ•°ï¼š0ï¼Œæœªè¢«è¦†ç›–æ¡æ•°ï¼š141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [02:47<00:00, 167.86s/æ‰¹]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… å…¨éƒ¨æ‰¹æ¬¡å¤„ç†å®Œæˆï¼\n",
      "åŸå§‹è¾“å…¥æ€»æ•°ï¼š141\n",
      "æ¨¡å‹#1 ç­›ç›¸å…³åæ€»è¡Œæ•°ï¼š89ï¼Œå…¶ä¸­ç©å®¶å‘è¨€ï¼š89 æ¡\n",
      "æ¨¡å‹#2 è¯é¢˜ç°‡ç´¯è®¡è¦†ç›–åŸå§‹å‘è¨€æ¡æ•°ï¼š0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "\n",
    "# ===========================================================\n",
    "# =============== ä¸»å¾ªç¯ï¼šæ¨¡å‹#1 + æ¨¡å‹#2 ====================\n",
    "# ===========================================================\n",
    "\n",
    "# è¯»å–ç³»ç»Ÿæç¤º\n",
    "system_prompt01 = load_system_prompt(PROMPT_MD_PATH01)  # ç­›ç›¸å…³\n",
    "system_prompt02 = load_system_prompt(PROMPT_MD_PATH02)  # åšè¯é¢˜ç°‡\n",
    "# 1âƒ£ ç»™åŸå§‹ jsonl æ¯æ¡åŠ  _idx\n",
    "jsonl_lines01_indexed = add_index_to_jsonl_lines(jsonl_lines01)\n",
    "\n",
    "total = len(jsonl_lines01_indexed)\n",
    "\n",
    "PRINT_UNCLUSTERED = False\n",
    "if total == 0:\n",
    "    print(\"æ²¡æœ‰å¯å¤„ç†çš„æ•°æ®ã€‚\")\n",
    "else:\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    print(f\"å‡†å¤‡å¤„ç† {total} æ¡ï¼Œå…± {total_batches} æ‰¹ï¼ˆæ¯æ‰¹ {BATCH_SIZE} æ¡ï¼‰ã€‚\")\n",
    "\n",
    "# ç»Ÿè®¡æ€»é‡ç”¨çš„ï¼ˆå¯é€‰ï¼‰\n",
    "total_filtered_lines = 0         # æ¨¡å‹#1 è¾“å‡ºæ€»è¡Œæ•°ä¹‹å’Œï¼ˆå«å®¢æœï¼‰\n",
    "total_filtered_player_lines = 0  # æ¨¡å‹#1 è¾“å‡ºç©å®¶è¡Œæ•°ä¹‹å’Œ\n",
    "total_covered_idx = 0            # è¢«è¯é¢˜ç°‡è¦†ç›–çš„åŸå§‹å‘è¨€æ€»æ•°ï¼ˆæŒ‰ _idx å»é‡ï¼‰\n",
    "\n",
    "\n",
    "for b in tqdm(range(total_batches), desc=\"ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦\", unit=\"æ‰¹\"):\n",
    "    start = b * BATCH_SIZE\n",
    "    end = min(start + BATCH_SIZE, total)\n",
    "    # æœ¬æ‰¹åŸå§‹å‘è¨€ï¼ˆå·²å¸¦ _idxï¼‰\n",
    "    batch_lines = jsonl_lines01_indexed[start:end]\n",
    "\n",
    "    # æœ¬æ‰¹åŸå§‹ _idx é›†åˆï¼Œç”¨æ¥å’Œè¯é¢˜ç°‡è¦†ç›–åšå·®é›†\n",
    "    batch_original_idx = set()\n",
    "    for line in batch_lines:\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "            batch_original_idx.add(int(obj[\"_idx\"]))\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "    try:\n",
    "        # ========== æ¨¡å‹ #1ï¼šç­›ç›¸å…³ ==========\n",
    "        user_prompt1 = build_user_prompt_filter(batch_lines)\n",
    "        output_filter = call_ark_chat_completions(\n",
    "            api_url=API_URL,\n",
    "            api_key=API_KEY,\n",
    "            model=V3_MODEL_ID,\n",
    "            system_prompt=system_prompt01,\n",
    "            user_prompt=user_prompt1,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            timeout=TIMEOUT_SEC,\n",
    "            retries=RETRIES,\n",
    "        )\n",
    "        if not output_filter:\n",
    "            tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] æ¨¡å‹#1 æ— æœ‰æ•ˆè¾“å‡ºï¼Œè·³è¿‡ã€‚\")\n",
    "            continue\n",
    "\n",
    "        # ç»Ÿè®¡ç­›ç›¸å…³ä¹‹åçš„è¡Œæ•°ï¼šæ€»è¡Œæ•° & ç©å®¶è¡Œæ•°\n",
    "        filter_total_lines, filter_player_lines = count_output_filter_stats(output_filter)\n",
    "        total_filtered_lines += filter_total_lines\n",
    "        total_filtered_player_lines += filter_player_lines\n",
    "\n",
    "        tqdm.write(\n",
    "            f\"[æ‰¹æ¬¡ {b+1}] æ¨¡å‹#1 è¾“å‡ºæ€»è¡Œæ•°ï¼š{filter_total_lines}ï¼Œ\"\n",
    "            f\"å…¶ä¸­ç©å®¶å‘è¨€ï¼š{filter_player_lines} æ¡ï¼ˆä¸å»é‡ï¼‰\"\n",
    "        )\n",
    "\n",
    "        # ========== æ¨¡å‹ #2ï¼šè¯é¢˜ç°‡ ==========\n",
    "        user_prompt2 = build_user_prompt_clsuter(output_filter)\n",
    "        output_cluster = call_ark_chat_completions(\n",
    "            api_url=API_URL,\n",
    "            api_key=API_KEY,\n",
    "            model=V3_1_MODEL_ID,\n",
    "            system_prompt=system_prompt02,\n",
    "            user_prompt=user_prompt2,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            timeout=TIMEOUT_SEC,\n",
    "            retries=RETRIES,\n",
    "        )\n",
    "        if not output_cluster:\n",
    "            tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] æ¨¡å‹#2 æ— æœ‰æ•ˆè¾“å‡ºï¼Œè·³è¿‡ã€‚\")\n",
    "            continue\n",
    "\n",
    "        # æ‰“å°æœ¬æ‰¹çš„è¯é¢˜ç°‡è¾“å‡ºï¼ˆåŸæ ·ï¼‰\n",
    "        print(f\"\\n===== æ‰¹æ¬¡ {b+1}/{total_batches} è¯é¢˜ç°‡è¾“å‡º =====\")\n",
    "        print(output_cluster)\n",
    "        print(\"===========================================\\n\")\n",
    "\n",
    "        # è®¡ç®—ï¼šæœ¬æ‰¹æœ‰å“ªäº› _idx è¢«è¯é¢˜ç°‡è¦†ç›–ï¼ˆä»â€œå‘è¨€è¡Œå·åˆ—è¡¨â€è§£æï¼‰\n",
    "        covered_idx = get_covered_indices_from_cluster_output(output_cluster)\n",
    "        total_covered_idx += len(covered_idx)\n",
    "\n",
    "        # å’Œâ€œæœ¬æ‰¹åŸå§‹å‘è¨€ _idx é›†åˆâ€åšå·®é›†ï¼Œçœ‹æœ‰å“ªäº›æ²¡è¢«è¦†ç›–\n",
    "        unclustered_idx = batch_original_idx - covered_idx\n",
    "\n",
    "        tqdm.write(\n",
    "            f\"[æ‰¹æ¬¡ {b+1}] è¯é¢˜ç°‡è¦†ç›–åŸå§‹å‘è¨€æ¡æ•°ï¼š{len(covered_idx)}ï¼Œ\"\n",
    "            f\"æœªè¢«è¦†ç›–æ¡æ•°ï¼š{len(unclustered_idx)}\"\n",
    "        )\n",
    "\n",
    "        # â­â­ æ–°å¢ï¼šæ‰“å°â€œæœªè¢«è¯é¢˜ç°‡è¦†ç›–â€çš„å‘è¨€ä¿¡æ¯ï¼ˆæœ€å¤šå‰ 20 æ¡ï¼‰\n",
    "        if PRINT_UNCLUSTERED and unclustered_idx:\n",
    "            # å…ˆæŠŠè¿™ä¸€æ‰¹ä¸­æœªè¦†ç›–çš„å‘è¨€å¯¹è±¡æå‡ºæ¥\n",
    "            unclustered_records = []\n",
    "            for line in batch_lines:\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                idx_val = obj.get(\"_idx\")\n",
    "                try:\n",
    "                    idx_int = int(idx_val)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if idx_int in unclustered_idx:\n",
    "                    unclustered_records.append(obj)\n",
    "\n",
    "            # æŒ‰ _idx æ’åºï¼Œé¿å…ä¹±åº\n",
    "            unclustered_records.sort(key=lambda x: int(x.get(\"_idx\", 0)))\n",
    "\n",
    "            print(f\"\\n[æ‰¹æ¬¡ {b+1}] æœªè¢«è¯é¢˜ç°‡è¦†ç›–çš„åŸå§‹å‘è¨€ï¼ˆæœ€å¤šå‰20æ¡ï¼‰ï¼š\")\n",
    "            for rec in unclustered_records[:20]:\n",
    "                idx = rec.get(\"_idx\")\n",
    "                date = rec.get(\"å‘è¨€æ—¥æœŸ\") or rec.get(\"æ—¥æœŸ\") or \"\"\n",
    "                t = rec.get(\"å‘è¨€æ—¶é—´\") or rec.get(\"æ—¶é—´\") or \"\"\n",
    "                speaker = rec.get(\"å‘è¨€äººID\") or rec.get(\"ç©å®¶ID\") or rec.get(\"è§’è‰²ID\") or \"\"\n",
    "                msg = (\n",
    "                    rec.get(\"ç©å®¶æ¶ˆæ¯\")\n",
    "                    or rec.get(\"å‘è¨€å†…å®¹\")\n",
    "                    or rec.get(\"ç©å®¶å‘è¨€\")\n",
    "                    or rec.get(\"æ¶ˆæ¯\")\n",
    "                    or \"\"\n",
    "                )\n",
    "                print(f\"- _idx={idx} [{date} {t}] {speaker}: {msg}\")\n",
    "            print()  # æ¢è¡Œåˆ†éš”ä¸€ä¸‹\n",
    "\n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] âŒ å‡ºé”™ï¼š{e}\")\n",
    "        continue\n",
    "\n",
    "    # é˜²æ­¢ QPS è¿‡é«˜\n",
    "    time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "print(\"\\nâœ… å…¨éƒ¨æ‰¹æ¬¡å¤„ç†å®Œæˆï¼\")\n",
    "print(f\"åŸå§‹è¾“å…¥æ€»æ•°ï¼š{total}\")\n",
    "print(f\"æ¨¡å‹#1 ç­›ç›¸å…³åæ€»è¡Œæ•°ï¼š{total_filtered_lines}ï¼Œå…¶ä¸­ç©å®¶å‘è¨€ï¼š{total_filtered_player_lines} æ¡\")\n",
    "print(f\"æ¨¡å‹#2 è¯é¢˜ç°‡ç´¯è®¡è¦†ç›–åŸå§‹å‘è¨€æ¡æ•°ï¼š{total_covered_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5465d2eb-915e-4133-a3dd-485de2d48577",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## åŠ è®¨è®ºè§‚ç‚¹åˆ†æçš„ç‰ˆæœ¬æµ‹è¯•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "411f43b3-eb1e-4fff-9e58-f758a3848262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡å¤„ç† 2308 æ¡ï¼Œå…± 8 æ‰¹ï¼ˆæ¯æ‰¹ 300 æ¡ï¼‰ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦:   0%|                                                                                                                                     | 0/8 [06:02<?, ?æ‰¹/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[æ‰¹æ¬¡ 1] æ¨¡å‹#1 ç­›åä¿ç•™ 231 æ¡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ                                                                                                            | 1/8 [13:17<45:30, 390.06s/æ‰¹]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[æ‰¹æ¬¡ 2] æ¨¡å‹#1 ç­›åä¿ç•™ 226 æ¡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦:  25%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                             | 2/8 [13:48<41:49, 418.32s/æ‰¹]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ... å‰ç½®ï¼šç³»ç»Ÿæç¤ºã€create_intent_excel_styled(EXCEL_FILE)ã€jsonl_lines01 ç­‰\n",
    "\n",
    "system_prompt01 = load_system_prompt(PROMPT_MD_PATH01)  # ç­›ç›¸å…³\n",
    "system_prompt02 = load_system_prompt(PROMPT_MD_PATH02)  # åšè¯æç°‡\n",
    "system_prompt03 = load_system_prompt(PROMPT_MD_PATH03)  # è¯é¢˜ç°‡èšåˆ / æ ¡æ­£ï¼ˆæ™ºèƒ½ä½“4ï¼‰\n",
    "\n",
    "# ç”¨äºå­˜æ”¾å½“å¤©æ‰€æœ‰æ‰¹æ¬¡çš„è¯é¢˜ç°‡ JSON è¡Œï¼ˆç»™æ™ºèƒ½ä½“4ç”¨ï¼‰\n",
    "batch_cluster_outputs  = []\n",
    "total = len(jsonl_lines01)\n",
    "if total == 0:\n",
    "    print(\"æ²¡æœ‰å¯å¤„ç†çš„æ•°æ®ã€‚\")\n",
    "else:\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    print(f\"å‡†å¤‡å¤„ç† {total} æ¡ï¼Œå…± {total_batches} æ‰¹ï¼ˆæ¯æ‰¹ {BATCH_SIZE} æ¡ï¼‰ã€‚\")\n",
    "\n",
    "written_total = 0\n",
    "\n",
    "for b in tqdm(range(total_batches), desc=\"ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦\", unit=\"æ‰¹\"):\n",
    "    start = b * BATCH_SIZE\n",
    "    end = min(start + BATCH_SIZE, total)\n",
    "    batch_lines = jsonl_lines01[start:end]\n",
    "\n",
    "    try:\n",
    "        # --- æ¨¡å‹ #1ï¼šç­›ç›¸å…³ ---\n",
    "        user_prompt1 = build_user_prompt_filter(batch_lines)\n",
    "        output_filter = call_ark_chat_completions(\n",
    "            api_url=API_URL,\n",
    "            api_key=API_KEY,\n",
    "            model=V3_MODEL_ID,\n",
    "            system_prompt=system_prompt01,\n",
    "            user_prompt=user_prompt1,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            timeout=TIMEOUT_SEC,\n",
    "            retries=RETRIES,\n",
    "        )\n",
    "        \n",
    "        if not output_filter:\n",
    "            tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] æ¨¡å‹#1 æ— æœ‰æ•ˆè¾“å‡ºï¼Œè·³è¿‡ã€‚\")\n",
    "            continue\n",
    "        filter_count = sum(1 for line in output_filter.splitlines() if line.strip())\n",
    "        tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] æ¨¡å‹#1 ç­›åä¿ç•™ {filter_count} æ¡\")\n",
    "        written_total += filter_count   # å¦‚æœä¸éœ€è¦æ€»æ•°ï¼Œå¯ä»¥åˆ æ‰è¿™ä¸€è¡Œ\n",
    "        \n",
    "        # --- æ¨¡å‹ #2ï¼šè¯é¢˜ç°‡åˆ’åˆ† ---\n",
    "        user_prompt2 = build_user_prompt_clsuter(output_filter)\n",
    "        output_cluster = call_ark_chat_completions(\n",
    "            api_url=API_URL,\n",
    "            api_key=API_KEY,\n",
    "            model=V3_1_MODEL_ID,\n",
    "            system_prompt=system_prompt02,\n",
    "            user_prompt=user_prompt2,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            timeout=TIMEOUT_SEC,\n",
    "            retries=RETRIES,\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        if not output_cluster:\n",
    "            tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] æ¨¡å‹#2 æ— æœ‰æ•ˆè¾“å‡ºï¼Œè·³è¿‡ã€‚\")\n",
    "            continue\n",
    "        try:\n",
    "            cluster_json_list = []\n",
    "            for line in output_cluster.strip().splitlines():\n",
    "                line = line.strip()\n",
    "                if not line.startswith(\"{\"):\n",
    "                    continue\n",
    "                obj = json.loads(line)\n",
    "                # å°† \"è¯é¢˜ç°‡1\" â†’ \"è¯é¢˜ç°‡\"ï¼Œä¾¿äºåç»­ç»Ÿä¸€å¤„ç†\n",
    "                for key in list(obj.keys()):\n",
    "                    if key.startswith(\"è¯é¢˜ç°‡\") and key != \"è¯é¢˜ç°‡\":\n",
    "                        obj[\"è¯é¢˜ç°‡\"] = obj.pop(key)\n",
    "                cluster_json_list.append(obj)\n",
    "\n",
    "            # å‡è®¾ä»è¾“å‡ºä¸­æ‹¿æ—¥æœŸ\n",
    "            date_str = infer_date_for_batch(cluster_json_list, batch_lines)  # ä½ å¯ä»¥ä»ä¸Šä¸‹æ–‡è·å¾—æ›´å‡†ç¡®å€¼\n",
    "            batch_id = f\"B{b+1}\"\n",
    "            cluster_json_list = assign_global_cluster_ids(cluster_json_list, date_str, batch_id)\n",
    "        \n",
    "            output_cluster_with_ids = \"\\n\".join(json.dumps(c, ensure_ascii=False) for c in cluster_json_list)\n",
    "            batch_cluster_outputs.append(output_cluster_with_ids)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] âš  æ·»åŠ  _cluster_id å¤±è´¥ï¼š{e}\")\n",
    "            continue\n",
    "\n",
    "       \n",
    "       \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] âŒ å‡ºé”™ï¼š{e}\")\n",
    "        continue\n",
    "\n",
    "    time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "\n",
    "\n",
    "# --- æ¯æ—¥è¯æç°‡èšåˆæˆjsonl ---\n",
    "all_cluster = aggregate_cluster_outputs(batch_cluster_outputs)\n",
    "\n",
    "# --- æ¨¡å‹ #3ï¼šæ—¥è¯æç°‡èšåˆ ---\n",
    "user_prompt3 = build_user_prompt_cluster_agg(all_cluster)\n",
    "output_cluster_agg = call_ark_chat_completions(\n",
    "    api_url=API_URL,\n",
    "    api_key=API_KEY,\n",
    "    model=V3_1_MODEL_ID,           # æˆ–ä½ ç»™æ™ºèƒ½ä½“4é€‰çš„æ¨¡å‹\n",
    "    system_prompt=system_prompt03,  # è¯é¢˜ç°‡èšåˆ/æ ¡æ­£æç¤ºè¯\n",
    "    user_prompt=user_prompt3,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    timeout=TIMEOUT_SEC,\n",
    "    retries=RETRIES,\n",
    ")\n",
    "\n",
    "parsed_subclusters = [\n",
    "    json.loads(line.strip()) for line in all_cluster.strip().splitlines() if line.strip()\n",
    "]\n",
    "parsed_clusters = [json.loads(line) for line in output_cluster_agg.strip().splitlines() if line.strip()]\n",
    "top5_results = extract_top5_heat_clusters(parsed_clusters, jsonl_lines01, top_k=5)\n",
    "final_result = attach_discussion_points(top5_results, parsed_subclusters)\n",
    "\n",
    "# è¾“å‡ºæŸ¥çœ‹ï¼š\n",
    "#for row in final_result:\n",
    "    #print(json.dumps(row, ensure_ascii=False, indent=2))\n",
    "\n",
    "append_daily_top5_to_version_jsonl(\n",
    "    final_result,\n",
    "    version_jsonl_path=VERSION_TOP5_JSONL # è¿™ä¸ªè·¯å¾„ä½ å¯ä»¥æŒ‰ç‰ˆæœ¬å·åŠ¨æ€æ”¹\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e99015f-028b-4df0-b3f0-228f671cacfa",
   "metadata": {},
   "source": [
    "## å¼€å§‹è¿è¡Œ(ä¸å¯åŠ¨ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8c0e8-7351-43b6-9745-d0f23cea3be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‡†å¤‡å¤„ç† 2308 æ¡ï¼Œå…± 8 æ‰¹ï¼ˆæ¯æ‰¹ 300 æ¡ï¼‰ã€‚\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦:   0%|                                                                                                                                                                                         | 0/8 [01:49<?, ?æ‰¹/s]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[æ‰¹æ¬¡ 1] æ¨¡å‹#1 ç­›åä¿ç•™ 68 æ¡\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦:  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                                                                                                                                          | 1/8 [01:59<13:56, 119.49s/æ‰¹]"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ... å‰ç½®ï¼šç³»ç»Ÿæç¤ºã€create_intent_excel_styled(EXCEL_FILE)ã€jsonl_lines01 ç­‰\n",
    "\n",
    "system_prompt01 = load_system_prompt(PROMPT_MD_PATH01)  # ç­›ç›¸å…³\n",
    "system_prompt02 = load_system_prompt(PROMPT_MD_PATH02)  # åšè¯æç°‡\n",
    "system_prompt03 = load_system_prompt(PROMPT_MD_PATH03)  # è¯é¢˜ç°‡èšåˆ / æ ¡æ­£ï¼ˆæ™ºèƒ½ä½“4ï¼‰\n",
    "system_prompt04 = load_system_prompt(PROMPT_MD_PATH04)  # è¯æç°‡ç©å®¶è§‚ç‚¹æ„Ÿå—åˆ†æ\n",
    "\n",
    "\n",
    "sub_opinion_map = {}\n",
    "# ç”¨äºå­˜æ”¾å½“å¤©æ‰€æœ‰æ‰¹æ¬¡çš„è¯é¢˜ç°‡ JSON è¡Œï¼ˆç»™æ™ºèƒ½ä½“4ç”¨ï¼‰\n",
    "batch_cluster_outputs  = []\n",
    "total = len(jsonl_lines01)\n",
    "if total == 0:\n",
    "    print(\"æ²¡æœ‰å¯å¤„ç†çš„æ•°æ®ã€‚\")\n",
    "else:\n",
    "    total_batches = (total + BATCH_SIZE - 1) // BATCH_SIZE\n",
    "    print(f\"å‡†å¤‡å¤„ç† {total} æ¡ï¼Œå…± {total_batches} æ‰¹ï¼ˆæ¯æ‰¹ {BATCH_SIZE} æ¡ï¼‰ã€‚\")\n",
    "\n",
    "written_total = 0\n",
    "\n",
    "for b in tqdm(range(total_batches), desc=\"ğŸ”¥ æ‰¹å¤„ç†è¿›åº¦\", unit=\"æ‰¹\"):\n",
    "    start = b * BATCH_SIZE\n",
    "    end = min(start + BATCH_SIZE, total)\n",
    "    batch_lines = jsonl_lines01[start:end]\n",
    "\n",
    "    try:\n",
    "        # --- æ¨¡å‹ #1ï¼šç­›ç›¸å…³ ---\n",
    "        user_prompt1 = build_user_prompt_filter(batch_lines)\n",
    "        output_filter = call_ark_chat_completions(\n",
    "            api_url=API_URL,\n",
    "            api_key=API_KEY,\n",
    "            model=V3_MODEL_ID,\n",
    "            system_prompt=system_prompt01,\n",
    "            user_prompt=user_prompt1,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            timeout=TIMEOUT_SEC,\n",
    "            retries=RETRIES,\n",
    "        )\n",
    "        \n",
    "        if not output_filter:\n",
    "            tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] æ¨¡å‹#1 æ— æœ‰æ•ˆè¾“å‡ºï¼Œè·³è¿‡ã€‚\")\n",
    "            continue\n",
    "        filter_count = sum(1 for line in output_filter.splitlines() if line.strip())\n",
    "        tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] æ¨¡å‹#1 ç­›åä¿ç•™ {filter_count} æ¡\")\n",
    "        written_total += filter_count   # å¦‚æœä¸éœ€è¦æ€»æ•°ï¼Œå¯ä»¥åˆ æ‰è¿™ä¸€è¡Œ\n",
    "        \n",
    "        # --- æ¨¡å‹ #2ï¼šè¯é¢˜ç°‡åˆ’åˆ† ---\n",
    "        user_prompt2 = build_user_prompt_clsuter(output_filter)\n",
    "        output_cluster = call_ark_chat_completions(\n",
    "            api_url=API_URL,\n",
    "            api_key=API_KEY,\n",
    "            model=V3_1_MODEL_ID,\n",
    "            system_prompt=system_prompt02,\n",
    "            user_prompt=user_prompt2,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            timeout=TIMEOUT_SEC,\n",
    "            retries=RETRIES,\n",
    "        )\n",
    "        \n",
    "        \n",
    "\n",
    "        if not output_cluster:\n",
    "            tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] æ¨¡å‹#2 æ— æœ‰æ•ˆè¾“å‡ºï¼Œè·³è¿‡ã€‚\")\n",
    "            continue\n",
    "        try:\n",
    "            cluster_json_list = []\n",
    "            for raw in output_cluster.strip().splitlines():\n",
    "                line = raw.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                if line in (\"{\", \"}\", \"},\"):\n",
    "                    continue\n",
    "                if not line.startswith(\"{\"):\n",
    "                    continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"[æ‰¹æ¬¡ {b+1}] âš  è§£ææ¨¡å‹#2 è¾“å‡º JSON å¤±è´¥ï¼š{e}ï¼Œè¡Œå†…å®¹ï¼š{line[:120]}\")\n",
    "                continue\n",
    "            \n",
    "            # å°† \"è¯é¢˜ç°‡1\" â†’ \"è¯é¢˜ç°‡\"ï¼Œä¾¿äºåç»­ç»Ÿä¸€å¤„ç†\n",
    "            for key in list(obj.keys()):\n",
    "                if key.startswith(\"è¯é¢˜ç°‡\") and key != \"è¯é¢˜ç°‡\":\n",
    "                    obj[\"è¯é¢˜ç°‡\"] = obj.pop(key)\n",
    "            cluster_json_list.append(obj)\n",
    "\n",
    "\n",
    "            # å‡è®¾ä»è¾“å‡ºä¸­æ‹¿æ—¥æœŸ\n",
    "            date_str = infer_date_for_batch(cluster_json_list, batch_lines)  # ä½ å¯ä»¥ä»ä¸Šä¸‹æ–‡è·å¾—æ›´å‡†ç¡®å€¼\n",
    "            batch_id = f\"B{b+1}\"\n",
    "            cluster_json_list = assign_global_cluster_ids(cluster_json_list, date_str, batch_id)\n",
    "        \n",
    "            output_cluster_with_ids = \"\\n\".join(json.dumps(c, ensure_ascii=False) for c in cluster_json_list)\n",
    "            batch_cluster_outputs.append(output_cluster_with_ids)\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] âš  æ·»åŠ  _cluster_id å¤±è´¥ï¼š{e}\")\n",
    "            continue\n",
    "\n",
    "       \n",
    "       \n",
    "    except Exception as e:\n",
    "        tqdm.write(f\"[æ‰¹æ¬¡ {b+1}] âŒ å‡ºé”™ï¼š{e}\")\n",
    "        continue\n",
    "\n",
    "    time.sleep(SLEEP_BETWEEN)\n",
    "\n",
    "\n",
    "\n",
    "# --- æ¯æ—¥è¯æç°‡èšåˆæˆjsonl ---\n",
    "all_cluster = aggregate_cluster_outputs(batch_cluster_outputs)\n",
    "\n",
    "# --- æ¨¡å‹ #3ï¼šæ—¥è¯æç°‡èšåˆ ---\n",
    "user_prompt3 = build_user_prompt_cluster_agg(all_cluster)\n",
    "output_cluster_agg = call_ark_chat_completions(\n",
    "    api_url=API_URL,\n",
    "    api_key=API_KEY,\n",
    "    model=V3_1_MODEL_ID,           # æˆ–ä½ ç»™æ™ºèƒ½ä½“4é€‰çš„æ¨¡å‹\n",
    "    system_prompt=system_prompt03,  # è¯é¢˜ç°‡èšåˆ/æ ¡æ­£æç¤ºè¯\n",
    "    user_prompt=user_prompt3,\n",
    "    temperature=TEMPERATURE,\n",
    "    max_tokens=MAX_TOKENS,\n",
    "    timeout=TIMEOUT_SEC,\n",
    "    retries=RETRIES,\n",
    ")\n",
    "\n",
    "#æ¯æ—¥æ€»å‘è¨€è½¬å•æ¡jonslæ ¼å¼\n",
    "parsed_subclusters = [\n",
    "    json.loads(line.strip()) for line in all_cluster.strip().splitlines() if line.strip()\n",
    "]\n",
    "\n",
    "\n",
    "# ğŸ”§ å¯¹æ¨¡å‹#3è¾“å‡ºçš„ key åšç»Ÿä¸€è§„èŒƒåŒ–ï¼ˆä¿®æ­£ timeè½´ / æ—¶é—´æ / æè½´ / å­è¯æç°‡åˆ—è¡¨ ç­‰ï¼‰\n",
    "output_cluster_agg = fix_output_cluster_agg_keys(output_cluster_agg)\n",
    "\n",
    "# è§£æåŸå§‹å‘è¨€ jsonlï¼Œä¸ºæŒ‰æ—¶é—´è½´ç­›é€‰å¯¹è¯ç”¨\n",
    "parsed_msgs = [json.loads(line.strip()) for line in jsonl_lines01 if line.strip()]\n",
    "\n",
    "#æ¯æ—¥èšåˆåè¯æç°‡è½¬å•æ¡jsonlæ ¼å¼\n",
    "parsed_clusters = [json.loads(line) for line in output_cluster_agg.strip().splitlines() if line.strip()]\n",
    "\n",
    "#è®¡ç®—æ¯æ—¥è®¨è®ºçƒ­åº¦top5\n",
    "top5_results = extract_top5_heat_clusters(parsed_clusters, jsonl_lines01, top_k=5)\n",
    "\n",
    "#######æå–å­è¯æç°‡è®¨è®ºç‚¹###########################################################\n",
    "####### å»ºå­ç°‡ç´¢å¼•ï¼š_cluster_id -> å­ç°‡å¯¹è±¡ ########\n",
    "sub_map = {}\n",
    "for row in parsed_subclusters:\n",
    "    cid = row.get(\"_cluster_id\")\n",
    "    if isinstance(cid, str) and cid:\n",
    "        sub_map[cid] = row\n",
    "\n",
    "####### åœ¨ Top5 å¾ªç¯é‡Œï¼Œå¯¹æ¯ä¸ªã€å­è¯é¢˜ç°‡ã€‘å•ç‹¬è·‘æ¨¡å‹#4 ########\n",
    "for cluster in top5_results:\n",
    "    date = cluster[\"æ—¥æœŸ\"]\n",
    "    # èšåˆç°‡çš„æ•´ä½“æ—¶é—´è½´ï¼ˆåªç”¨æ¥å…œåº•ï¼‰\n",
    "    cluster_time_axis = cluster[\"æ—¶é—´è½´\"]\n",
    "    cid_list = cluster.get(\"å­è¯é¢˜ç°‡åˆ—è¡¨\", [])\n",
    "\n",
    "    for cid in cid_list:\n",
    "        sub = sub_map.get(cid)\n",
    "        if not sub:\n",
    "            print(f\"\\n[èšåˆè¯é¢˜ç°‡ï¼š{cluster.get('èšåˆè¯é¢˜ç°‡')}] å­è¯é¢˜ç°‡ IDï¼š{cid}\")\n",
    "            print(\"  âš  åœ¨ sub_map / parsed_subclusters ä¸­æœªæ‰¾åˆ°è¯¥ _cluster_idã€‚\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # ---- 1ï¼‰ä»å­è¯é¢˜ç°‡ã€æ ‡é¢˜ã€‘é‡Œæå–æ—¶é—´è½´ ----\n",
    "        sub_title = sub.get(\"è¯é¢˜ç°‡\", \"\") or \"\"\n",
    "        sub_time_axis = extract_time_axis_from_title(sub_title)\n",
    "\n",
    "        # ---- 2ï¼‰æŒ‰ã€å­è¯æç°‡æ—¶é—´è½´ã€‘ä»åŸå§‹å‘è¨€é‡ŒåŒ¹é… dialogs ----\n",
    "        dialogs = match_dialogs_by_time(parsed_msgs, date, sub_time_axis)\n",
    "\n",
    "        # ---- 3ï¼‰æ„é€  discussion_pointï¼ˆä¸æ‰“å°ï¼Œåªç”¨æ¥å†™ promptï¼‰----\n",
    "        discussion_point = (\n",
    "    \n",
    "            sub.get(\"æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶\")\n",
    "            or sub.get(\"è¯é¢˜ç°‡\")\n",
    "            or \"\"\n",
    "        )\n",
    "        topic_id = sub.get(\"_cluster_id\", cid)\n",
    "        \n",
    "        # ---- 4ï¼‰æ¨¡å‹4 ç©å®¶å‘è¨€æ„Ÿå—åˆ†ææ€»ç»“----\n",
    "        user_prompt4 = build_user_prompt_subcluster_opinion(\n",
    "            topic_id=topic_id,\n",
    "            discussion_point=discussion_point,\n",
    "            dialogs=dialogs,\n",
    "        )\n",
    "        if not isinstance(user_prompt4, str) or not user_prompt4.strip():\n",
    "            print(f\"  âš  user_prompt4 ä¸ºç©ºæˆ–éæ³•ï¼Œè·³è¿‡æœ¬å­è¯é¢˜ç°‡ï¼š{cid}\")\n",
    "            continue\n",
    "        opinion_output = call_ark_chat_completions(\n",
    "            api_url=API_URL,\n",
    "            api_key=API_KEY,\n",
    "            model=V3_1_MODEL_ID,             \n",
    "            system_prompt=system_prompt04,   \n",
    "            user_prompt=user_prompt4,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            timeout=TIMEOUT_SEC,\n",
    "            retries=RETRIES,\n",
    "        )\n",
    "\n",
    "\n",
    "       \n",
    "    opinion_obj = parse_and_normalize_opinion_output(\n",
    "        opinion_output=opinion_output,\n",
    "        topic_id=topic_id,\n",
    "        discussion_point=discussion_point,\n",
    "    )\n",
    "    sub_opinion_map[topic_id] = opinion_obj\n",
    "# ===================================\n",
    "# ç»„è£… Top5 + è§‚ç‚¹æ€»ç»“ï¼Œå¹¶å†™å…¥ jsonl\n",
    "# ===================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d0b52dd6-8cd5-46f1-88de-2691129b86b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_cluster' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#æ¯æ—¥æ€»å‘è¨€è½¬å•æ¡jonslæ ¼å¼\u001b[39;00m\n\u001b[0;32m      2\u001b[0m parsed_subclusters \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m----> 3\u001b[0m     json\u001b[38;5;241m.\u001b[39mloads(line\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m all_cluster\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplitlines() \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m      4\u001b[0m ]\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# è§£æåŸå§‹å‘è¨€ jsonlï¼Œä¸ºæŒ‰æ—¶é—´è½´ç­›é€‰å¯¹è¯ç”¨\u001b[39;00m\n\u001b[0;32m      7\u001b[0m parsed_msgs \u001b[38;5;241m=\u001b[39m [json\u001b[38;5;241m.\u001b[39mloads(line\u001b[38;5;241m.\u001b[39mstrip()) \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m jsonl_lines01 \u001b[38;5;28;01mif\u001b[39;00m line\u001b[38;5;241m.\u001b[39mstrip()]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'all_cluster' is not defined"
     ]
    }
   ],
   "source": [
    "#æ¯æ—¥æ€»å‘è¨€è½¬å•æ¡jonslæ ¼å¼\n",
    "parsed_subclusters = [\n",
    "    json.loads(line.strip()) for line in all_cluster.strip().splitlines() if line.strip()\n",
    "]\n",
    "\n",
    "# è§£æåŸå§‹å‘è¨€ jsonlï¼Œä¸ºæŒ‰æ—¶é—´è½´ç­›é€‰å¯¹è¯ç”¨\n",
    "parsed_msgs = [json.loads(line.strip()) for line in jsonl_lines01 if line.strip()]\n",
    "\n",
    "#æ¯æ—¥èšåˆåè¯æç°‡è½¬å•æ¡jsonlæ ¼å¼\n",
    "parsed_clusters = [json.loads(line) for line in output_cluster_agg.strip().splitlines() if line.strip()]\n",
    "\n",
    "#è®¡ç®—æ¯æ—¥è®¨è®ºçƒ­åº¦top5\n",
    "top5_results = extract_top5_heat_clusters(parsed_clusters, jsonl_lines01, top_k=5)\n",
    "\n",
    "#######æå–å­è¯æç°‡è®¨è®ºç‚¹###########################################################\n",
    "####### å»ºå­ç°‡ç´¢å¼•ï¼š_cluster_id -> å­ç°‡å¯¹è±¡ ########\n",
    "sub_map = {}\n",
    "for row in parsed_subclusters:\n",
    "    cid = row.get(\"_cluster_id\")\n",
    "    if isinstance(cid, str) and cid:\n",
    "        sub_map[cid] = row\n",
    "\n",
    "####### åœ¨ Top5 å¾ªç¯é‡Œï¼Œå¯¹æ¯ä¸ªã€å­è¯é¢˜ç°‡ã€‘å•ç‹¬è·‘æ¨¡å‹#4 ########\n",
    "for cluster in top5_results:\n",
    "    date = cluster[\"æ—¥æœŸ\"]\n",
    "    # èšåˆç°‡çš„æ•´ä½“æ—¶é—´è½´ï¼ˆåªç”¨æ¥å…œåº•ï¼‰\n",
    "    cluster_time_axis = cluster[\"æ—¶é—´è½´\"]\n",
    "    cid_list = cluster.get(\"å­è¯é¢˜ç°‡åˆ—è¡¨\", [])\n",
    "\n",
    "    for cid in cid_list:\n",
    "        sub = sub_map.get(cid)\n",
    "        if not sub:\n",
    "            print(f\"\\n[èšåˆè¯é¢˜ç°‡ï¼š{cluster.get('èšåˆè¯é¢˜ç°‡')}] å­è¯é¢˜ç°‡ IDï¼š{cid}\")\n",
    "            print(\"  âš  åœ¨ sub_map / parsed_subclusters ä¸­æœªæ‰¾åˆ°è¯¥ _cluster_idã€‚\")\n",
    "            continue\n",
    "\n",
    "        \n",
    "        # ---- 1ï¼‰ä»å­è¯é¢˜ç°‡ã€æ ‡é¢˜ã€‘é‡Œæå–æ—¶é—´è½´ ----\n",
    "        sub_title = sub.get(\"è¯é¢˜ç°‡\", \"\") or \"\"\n",
    "        sub_time_axis = extract_time_axis_from_title(sub_title)\n",
    "\n",
    "        # ---- 2ï¼‰æŒ‰ã€å­è¯æç°‡æ—¶é—´è½´ã€‘ä»åŸå§‹å‘è¨€é‡ŒåŒ¹é… dialogs ----\n",
    "        dialogs = match_dialogs_by_time(parsed_msgs, date, sub_time_axis)\n",
    "\n",
    "        # ---- 3ï¼‰æ„é€  discussion_pointï¼ˆä¸æ‰“å°ï¼Œåªç”¨æ¥å†™ promptï¼‰----\n",
    "        discussion_point = (\n",
    "    \n",
    "            sub.get(\"æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶\")\n",
    "            or sub.get(\"è¯é¢˜ç°‡\")\n",
    "            or \"\"\n",
    "        )\n",
    "        topic_id = sub.get(\"_cluster_id\", cid)\n",
    "        \n",
    "        # ---- 4ï¼‰æ¨¡å‹4 ç©å®¶å‘è¨€æ„Ÿå—åˆ†ææ€»ç»“----\n",
    "        user_prompt4 = build_user_prompt_subcluster_opinion(\n",
    "            topic_id=topic_id,\n",
    "            discussion_point=discussion_point,\n",
    "            dialogs=dialogs,\n",
    "        )\n",
    "        if not isinstance(user_prompt4, str) or not user_prompt4.strip():\n",
    "            print(f\"  âš  user_prompt4 ä¸ºç©ºæˆ–éæ³•ï¼Œè·³è¿‡æœ¬å­è¯é¢˜ç°‡ï¼š{cid}\")\n",
    "            continue\n",
    "        opinion_output = call_ark_chat_completions(\n",
    "            api_url=API_URL,\n",
    "            api_key=API_KEY,\n",
    "            model=V3_1_MODEL_ID,             \n",
    "            system_prompt=system_prompt04,   \n",
    "            user_prompt=user_prompt4,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            timeout=TIMEOUT_SEC,\n",
    "            retries=RETRIES,\n",
    "        )\n",
    "\n",
    "\n",
    "       \n",
    "    opinion_obj = parse_and_normalize_opinion_output(\n",
    "        opinion_output=opinion_output,\n",
    "        topic_id=topic_id,\n",
    "        discussion_point=discussion_point,\n",
    "    )\n",
    "    sub_opinion_map[topic_id] = opinion_obj\n",
    "# ===================================\n",
    "# ç»„è£… Top5 + è§‚ç‚¹æ€»ç»“ï¼Œå¹¶å†™å…¥ jsonl\n",
    "# ===================================\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ada78d16-fe72-4a92-94d2-8b626ef64d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš  å­è¯é¢˜ç°‡ 2025-12-04_B3_01 æœªåœ¨ sub_opinion_map ä¸­æ‰¾åˆ°æ¨¡å‹#4ç»“æœï¼Œè·³è¿‡ã€‚\n",
      "âœ… å·²å°†å½“æ—¥ Top5ï¼ˆå« _idx å’Œ _daily_top_idï¼‰è¿½åŠ å†™å…¥: version_daily_top5_with_opinion.jsonl\n"
     ]
    }
   ],
   "source": [
    "final_top5_with_opinion = build_daily_top5_opinion_records(\n",
    "    top5_results=top5_results,\n",
    "    sub_opinion_map=sub_opinion_map,\n",
    ")\n",
    "\n",
    "append_daily_top5_to_version_jsonl(\n",
    "    final_result=final_top5_with_opinion,\n",
    "    version_jsonl_path=\"version_daily_top5_with_opinion.jsonl\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dfa2bb40-0ca1-4558-a350-06dbc11c2e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2025-12-02_B1_03', '2025-12-02_B1_14', '2025-12-02_B2_07']\n",
      "{'è¯é¢˜ç°‡': 'æ¸¸æˆç‰©å“å¼‚å¸¸é—®é¢˜åé¦ˆï¼ˆ2025-12-02 22:57:27-22:57:31ï¼‰', 'æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶': 'æ¸¸æˆå†…ç‰©å“å¼‚å¸¸æ¶ˆå¤±é—®é¢˜çš„åé¦ˆä¸å¤„ç†', '_cluster_id': '2025-12-02_B2_07'}\n",
      "æ¸¸æˆç‰©å“å¼‚å¸¸é—®é¢˜åé¦ˆï¼ˆ2025-12-02 22:57:27-22:57:31ï¼‰\n",
      "22:57:27-22:57:31\n",
      "æ¸¸æˆå†…ç‰©å“å¼‚å¸¸æ¶ˆå¤±é—®é¢˜çš„åé¦ˆä¸å¤„ç†\n",
      "[{'å‘è¨€æ—¥æœŸ': '2025-12-02', 'å‘è¨€æ—¶é—´': '22:57:27', 'ç©å®¶ID': 'æ— ä¸ºæ— ç•(2514177080)', 'ç©å®¶æ¶ˆæ¯': '[å›¾ç‰‡]'}, {'å‘è¨€æ—¥æœŸ': '2025-12-02', 'å‘è¨€æ—¶é—´': '22:57:31', 'ç©å®¶ID': 'æ— ä¸ºæ— ç•(2514177080)', 'ç©å®¶æ¶ˆæ¯': 'ä¸œè¥¿æ²¡äº†'}]\n",
      "2025-12-02_B2_07\n"
     ]
    }
   ],
   "source": [
    "print(cid_list)\n",
    "print(sub)\n",
    "\n",
    "print(sub_title)\n",
    "print(sub_time_axis)\n",
    "print(discussion_point)\n",
    "print(dialogs)\n",
    "print(cid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "21cb4444-2eeb-41e2-9797-1d5eeb074f05",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'continue' not properly in loop (1099491447.py, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 10\u001b[1;36m\u001b[0m\n\u001b[1;33m    continue\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'continue' not properly in loop\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    obj = json.loads(line)\n",
    "except json.JSONDecodeError as e:\n",
    "    print(\n",
    "        f\"[æ‰¹æ¬¡ {b+1}] âš  è§£ææ¨¡å‹#2 è¾“å‡º JSON å¤±è´¥ï¼š{e}ï¼Œè¡Œå†…å®¹ï¼š{line!r}\"\n",
    "    )\n",
    "    print(\"=== è¯¥æ‰¹æ¬¡æ¨¡å‹#2 åŸå§‹å®Œæ•´è¾“å‡ºï¼ˆè°ƒè¯•ç”¨ï¼‰ ===\")\n",
    "    print(output_cluster)\n",
    "    print(\"=== â†‘â†‘â†‘ è¯·æŸ¥çœ‹è¿™ä¸€æ‰¹è¾“å‡ºä¸­å“ªé‡Œæœ‰å¤šä½™çš„å¤§æ‹¬å·/å¥‡æ€ªå­—ç¬¦ â†‘â†‘â†‘ ===\")\n",
    "    continue\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f29062e6-280e-4eb1-a0e3-b0aa462dc247",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import time\n",
    "import json\n",
    "from pathlib import Path\n",
    "TOP5_JSONL_PATH = Path(\"daily_top5-Copy1.jsonl\")\n",
    "\n",
    "# =========================\n",
    "# 1) å°å·¥å…·ï¼šè¯»/å†™ jsonl\n",
    "# =========================\n",
    "def read_jsonl(path: Path) -> list[dict]:\n",
    "    rows = []\n",
    "    if not path.exists():\n",
    "        return rows\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "def write_jsonl(path: Path, rows: list[dict], mode: str = \"w\"):\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, mode, encoding=\"utf-8\") as f:\n",
    "        for obj in rows:\n",
    "            f.write(json.dumps(obj, ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "# =========================\n",
    "# 2) ç¨³å¥è§£æï¼šæ¨¡å‹#2 è¾“å‡º -> list[dict]\n",
    "#    - å…¼å®¹ ```json åŒ…è£¹\n",
    "#    - å…¼å®¹å‡ºç°å•ç‹¬çš„ { / } / },\n",
    "#    - å…¼å®¹ä¸€è¡Œä¸€ä¸ª dictï¼ˆjsonlï¼‰\n",
    "# =========================\n",
    "def parse_model2_jsonl(output_text: str) -> list[dict]:\n",
    "    if not output_text or not isinstance(output_text, str):\n",
    "        return []\n",
    "    s = output_text.strip()\n",
    "\n",
    "    # å»æ‰ä»£ç å›´æ \n",
    "    if s.startswith(\"```\"):\n",
    "        s = s.split(\"```\", 1)[-1]\n",
    "        if \"```\" in s:\n",
    "            s = s.rsplit(\"```\", 1)[0].strip()\n",
    "\n",
    "    objs = []\n",
    "    for raw in s.splitlines():\n",
    "        line = raw.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        if line in (\"{\", \"}\", \"},\"):\n",
    "            continue\n",
    "        if not line.startswith(\"{\"):\n",
    "            continue\n",
    "\n",
    "        # å°¾éƒ¨é€—å·å®¹é”™\n",
    "        if line.endswith(\",\"):\n",
    "            line = line[:-1].strip()\n",
    "\n",
    "        try:\n",
    "            obj = json.loads(line)\n",
    "        except json.JSONDecodeError:\n",
    "            # å¦‚æœä¸æ˜¯â€œå•è¡Œå®Œæ•´jsonâ€ï¼Œè¿™é‡Œå°±è·³è¿‡ï¼ˆä¸åšå¤šè¡Œæ‹¼æ¥ï¼Œé¿å…å¼•å…¥æ›´å¤šä¸ç¡®å®šï¼‰\n",
    "            continue\n",
    "\n",
    "        if isinstance(obj, dict):\n",
    "            # ç»Ÿä¸€ keyï¼šè¯é¢˜ç°‡1/2/3 -> è¯é¢˜ç°‡\n",
    "            for k in list(obj.keys()):\n",
    "                if k.startswith(\"è¯é¢˜ç°‡\") and k != \"è¯é¢˜ç°‡\":\n",
    "                    obj[\"è¯é¢˜ç°‡\"] = obj.pop(k)\n",
    "            if obj.get(\"è¯é¢˜ç°‡\") or obj.get(\"æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶\"):\n",
    "                objs.append(obj)\n",
    "    return objs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "34e8df2c-b759-4281-9782-4a53a6602c27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å·²ä» daily_top5-Copy1.jsonl è¯»å– top5_resultsï¼š20 æ¡\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'parsed_subclusters' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 10\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# ===================================\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# 7) å­è¯é¢˜ç°‡ç´¢å¼•ï¼š_cluster_id -> å­ç°‡å¯¹è±¡\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# ===================================\u001b[39;00m\n\u001b[0;32m      9\u001b[0m sub_map \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m parsed_subclusters:\n\u001b[0;32m     11\u001b[0m     cid \u001b[38;5;241m=\u001b[39m row\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cluster_id\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cid, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m cid:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'parsed_subclusters' is not defined"
     ]
    }
   ],
   "source": [
    "top5_results = read_jsonl(TOP5_JSONL_PATH)\n",
    "if not top5_results:\n",
    "    raise RuntimeError(f\"top5_results.jsonl ä¸ºç©ºæˆ–ä¸å­˜åœ¨ï¼š{TOP5_JSONL_PATH}\")\n",
    "print(f\"âœ… å·²ä» {TOP5_JSONL_PATH} è¯»å– top5_resultsï¼š{len(top5_results)} æ¡\")\n",
    "\n",
    "# ===================================\n",
    "# 7) å­è¯é¢˜ç°‡ç´¢å¼•ï¼š_cluster_id -> å­ç°‡å¯¹è±¡\n",
    "# ===================================\n",
    "sub_map = {}\n",
    "for row in parsed_subclusters:\n",
    "    cid = row.get(\"_cluster_id\")\n",
    "    if isinstance(cid, str) and cid:\n",
    "        sub_map[cid] = row\n",
    "\n",
    "# ===================================\n",
    "# 8) è·‘æ¨¡å‹#4ï¼šå¯¹ Top5 çš„æ¯ä¸ªå­è¯é¢˜ç°‡æå–è§‚ç‚¹/æ„Ÿå—\n",
    "# ===================================\n",
    "for cluster in top5_results:\n",
    "    date = cluster.get(\"æ—¥æœŸ\")\n",
    "    cluster_time_axis = cluster.get(\"æ—¶é—´è½´\", \"\")\n",
    "    cid_list = cluster.get(\"å­è¯é¢˜ç°‡åˆ—è¡¨\", [])\n",
    "\n",
    "    if not date or not isinstance(cid_list, list):\n",
    "        continue\n",
    "\n",
    "    for cid in cid_list:\n",
    "        sub = sub_map.get(cid)\n",
    "        if not sub:\n",
    "            print(f\"\\n[èšåˆè¯é¢˜ç°‡ï¼š{cluster.get('èšåˆè¯é¢˜ç°‡')}] å­è¯é¢˜ç°‡ IDï¼š{cid}\")\n",
    "            print(\"  âš  åœ¨ sub_map / parsed_subclusters ä¸­æœªæ‰¾åˆ°è¯¥ _cluster_idã€‚\")\n",
    "            continue\n",
    "\n",
    "        # 1ï¼‰ä»å­è¯é¢˜ç°‡æ ‡é¢˜æå–æ—¶é—´è½´\n",
    "        sub_title = sub.get(\"è¯é¢˜ç°‡\", \"\") or \"\"\n",
    "        sub_time_axis = extract_time_axis_from_title(sub_title)\n",
    "\n",
    "        # 2ï¼‰å…œåº•ï¼šè‹¥å­ç°‡æ²¡æå–åˆ°æ—¶é—´è½´ï¼Œç”¨èšåˆç°‡æ—¶é—´è½´\n",
    "        time_axis_to_use = sub_time_axis or cluster_time_axis\n",
    "\n",
    "        # 3ï¼‰æŒ‰æ—¶é—´è½´ä»åŸå§‹å‘è¨€åŒ¹é… dialogs\n",
    "        dialogs = match_dialogs_by_time(parsed_msgs, date, time_axis_to_use)\n",
    "\n",
    "        # 4ï¼‰æ„é€  discussion_point\n",
    "        discussion_point = sub.get(\"æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶\") or sub.get(\"è¯é¢˜ç°‡\") or \"\"\n",
    "        topic_id = sub.get(\"_cluster_id\", cid)\n",
    "\n",
    "        # 5ï¼‰æ¨¡å‹4 prompt\n",
    "        user_prompt4 = build_user_prompt_subcluster_opinion(\n",
    "            topic_id=topic_id,\n",
    "            discussion_point=discussion_point,\n",
    "            dialogs=dialogs,\n",
    "        )\n",
    "        if not isinstance(user_prompt4, str) or not user_prompt4.strip():\n",
    "            print(f\"  âš  user_prompt4 ä¸ºç©ºæˆ–éæ³•ï¼Œè·³è¿‡æœ¬å­è¯é¢˜ç°‡ï¼š{cid}\")\n",
    "            continue\n",
    "\n",
    "        # 6ï¼‰è°ƒç”¨æ¨¡å‹#4\n",
    "        opinion_output = call_ark_chat_completions(\n",
    "            api_url=API_URL,\n",
    "            api_key=API_KEY,\n",
    "            model=V3_1_MODEL_ID,\n",
    "            system_prompt=system_prompt04,\n",
    "            user_prompt=user_prompt4,\n",
    "            temperature=TEMPERATURE,\n",
    "            max_tokens=MAX_TOKENS,\n",
    "            timeout=TIMEOUT_SEC,\n",
    "            retries=RETRIES,\n",
    "        )\n",
    "\n",
    "        # âœ… å¿…é¡»åœ¨ cid å¾ªç¯å†…è§£æå¹¶å†™å…¥ï¼ˆä¿®å¤ä½ åŸç¼©è¿›é—®é¢˜ï¼‰\n",
    "        opinion_obj = parse_and_normalize_opinion_output(\n",
    "            opinion_output=opinion_output,\n",
    "            topic_id=topic_id,\n",
    "            discussion_point=discussion_point,\n",
    "        )\n",
    "        sub_opinion_map[topic_id] = opinion_obj\n",
    "\n",
    "# ===================================\n",
    "# 9) ç»„è£… Top5 + è§‚ç‚¹æ€»ç»“ï¼Œå¹¶å†™å…¥ jsonl\n",
    "# ===================================\n",
    "OUT_TOP5_WITH_OPINION = Path(\"top5_with_opinionss.jsonl\")\n",
    "out_rows = []\n",
    "\n",
    "for cluster in top5_results:\n",
    "    cid_list = cluster.get(\"å­è¯é¢˜ç°‡åˆ—è¡¨\", [])\n",
    "    # è¿™é‡Œå‡è®¾ cid_list é‡Œå­˜çš„å°±æ˜¯ _cluster_idï¼ˆä½ çš„è®¾è®¡æ˜¯è¿™æ ·ï¼‰\n",
    "    cluster[\"å­è¯é¢˜ç°‡è§‚ç‚¹\"] = {cid: sub_opinion_map.get(cid) for cid in cid_list}\n",
    "    out_rows.append(cluster)\n",
    "\n",
    "write_jsonl(OUT_TOP5_WITH_OPINION, out_rows, mode=\"w\")\n",
    "print(f\"âœ… å·²è¾“å‡ºï¼š{OUT_TOP5_WITH_OPINION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385e4733-358c-46e0-91ef-5d86f7da4e12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
