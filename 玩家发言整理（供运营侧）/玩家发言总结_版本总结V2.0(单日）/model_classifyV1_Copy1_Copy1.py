
from __future__ import annotations
import json, time, typing as T
import pandas as pd
import requests
import math
from pathlib import Path
from typing import List, Dict, Any,Optional
import re, json, unicodedata
from datetime import datetime
import json

# --- openpyxl æ ·å¼/å·¥å…· ---
from openpyxl import Workbook, load_workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, NamedStyle

# --- docx æ ·å¼/å·¥å…· ---
from docx.oxml import OxmlElement
from docx import Document
from docx.shared import Pt
from docx.oxml.ns import qn


################æ¨¡å‹è°ƒç”¨ï¼Œå‡ºç»“æœ###################

def load_system_prompt(path: Path) -> str:
    if not path.exists():
        raise FileNotFoundError(f"Prompt file not found: {path}")
    return path.read_text(encoding="utf-8")

def build_user_prompt_filter(json_lines: T.List[str]) -> str:
    # æ¨¡å‹#1ï¼šç­›æ‰éæ¸¸æˆç›¸å…³ï¼Œåªè¾“å‡ºç›¸å…³ JSON è¡Œï¼ˆåŸæ ·ï¼‰
    return (
        "ä»¥ä¸‹æ˜¯è‹¥å¹²ç©å®¶/å®¢æœ/ç ”å‘çš„å‘è¨€è®°å½•ï¼Œè¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­è§„åˆ™ï¼Œ"
        "åˆ¤æ–­å“ªäº›æ˜¯ã€ä¸æ¸¸æˆå†…å®¹ç›¸å…³ã€‘çš„å‘è¨€ï¼Œä¿ç•™è¿™äº› JSON è¡Œï¼Œä¸ç›¸å…³çš„å¿½ç•¥ã€‚"
        "è¯·ä»…è¾“å‡ºã€ç›¸å…³å‘è¨€çš„åŸå§‹ JSON è¡Œã€‘ï¼Œä¸¥æ ¼ä¿æŒæ ¼å¼ä¸å˜ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + "\n".join(json_lines)
    )

def build_user_prompt_clsuter(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )

def build_user_prompt_cluster_agg(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )
from typing import List, Dict, Any
import json

def build_user_prompt_subcluster_opinion(
    topic_id: str,
    discussion_point: str,
    dialogs: List[Dict[str, Any]],
) -> str:
    lines = []

    meta = {
        "è¯é¢˜ç°‡ID": topic_id,
        "è®¨è®ºç‚¹": discussion_point,
    }
    lines.append(json.dumps(meta, ensure_ascii=False))

    for row in dialogs:
        lines.append(json.dumps(row, ensure_ascii=False))

    jsonl_block = "\n".join(lines)

    return (
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘æœ¬æ¬¡æ‰€æœ‰æ•°æ®ï¼ˆJSONLï¼Œç¬¬ä¸€è¡Œä¸ºè¯é¢˜ç°‡ä¿¡æ¯ï¼Œå…¶ä½™ä¸ºå‘è¨€ï¼‰ï¼š\n"
        + jsonl_block +
        "\n\nä»…æ ¹æ®ä»¥ä¸Šå†…å®¹ï¼Œå¹¶éµå¾ªç³»ç»Ÿæç¤ºè¯ä¸­çš„è§„åˆ™å’Œè¾“å‡ºæ ¼å¼ï¼Œ"
        "ç›´æ¥è¾“å‡º 1 ä¸ª JSON å¯¹è±¡ä½œä¸ºæœ€ç»ˆç»“æœã€‚"
    )

def call_ark_chat_completions(
    api_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    temperature: float = 0.3,
    max_tokens: int = 32700,
    timeout: int = 600,
    retries: int = 2,
) -> str:
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    last_err = None
    for attempt in range(retries + 1):
        try:
            resp = requests.post(api_url, headers=headers, json=payload, timeout=timeout)
            if resp.status_code != 200:
                raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            last_err = e
            time.sleep(1.2 * (attempt + 1))
    raise RuntimeError(f"Ark API è°ƒç”¨å¤±è´¥: {last_err}")

def extract_valid_json_lines(text: str) -> T.List[str]:
    """
    æŠŠæ¨¡å‹è¾“å‡ºé‡Œçš„çº¯ JSON è¡Œæå–å‡ºæ¥ï¼ˆé²æ£’å¤„ç†ï¼‰ï¼š
    - é€è¡Œåˆ¤æ–­ï¼šä»¥ { å¼€å¤´ ä¸” ä»¥ } ç»“å°¾ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ª JSON å¯¹è±¡è¡Œ
    - ä¹Ÿèƒ½å®¹å¿å‰åå¤šä½™ç©ºè¡Œæˆ–è§£é‡Šæ–‡å­—ï¼ˆä¼šè¢«å¿½ç•¥ï¼‰
    """
    lines = []
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith("{") and s.endswith("}"):
            lines.append(s)
    return lines

def add_index_to_jsonl_lines(jsonl_lines):
    """
    ç»™åŸå§‹ jsonl æ¯æ¡å‘è¨€åŠ ä¸Šä¸€ä¸ªå”¯ä¸€è¡Œå·å­—æ®µ `_idx`ï¼Œ
    è¿”å›æ–°çš„ List[str]ï¼Œæ¯ä¸ªå…ƒç´ ä»ç„¶æ˜¯ä¸€è¡Œ JSON å­—ç¬¦ä¸²ã€‚
    """
    new_lines = []
    for idx, line in enumerate(jsonl_lines, start=1):
        line = line.strip()
        if not line:
            continue
        obj = json.loads(line)
        obj["_idx"] = idx  # æ–°å¢è¡Œå·
        new_lines.append(json.dumps(obj, ensure_ascii=False))
    return new_lines


def count_output_filter_stats(output_filter: str):
    """
    ç»Ÿè®¡æ¨¡å‹#1 è¾“å‡ºä¸­çš„ï¼š
    - æ€»è¡Œæ•° total_linesï¼ˆç©å®¶+å®¢æœ+ç ”å‘ï¼‰
    - ç©å®¶å‘è¨€è¡Œæ•° player_linesï¼ˆç®€å•ç”¨ å‘è¨€äººID é‡Œçš„å…³é”®è¯åŒºåˆ†ï¼‰
    """
    total_lines = 0
    player_lines = 0

    for line in output_filter.splitlines():
        line = line.strip()
        if not line:
            continue
        total_lines += 1

        try:
            obj = json.loads(line)
        except Exception:
            # å¦‚æœè¿™ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±åªèƒ½ç®—è¿› total_linesï¼Œæ— æ³•ç»†åˆ†
            continue

        speaker = (
            obj.get("ç©å®¶ID")
            or obj.get("å‘è¨€äººID")
            or obj.get("è§’è‰²ID")
            or ""
        )

        # ç®€å•æ’é™¤å®¢æœ/GM/å®˜æ–¹/è¿è¥/ç ”å‘ï¼Œå…¶ä½™è§†ä¸ºç©å®¶
        if any(key in str(speaker) for key in ["å®¢æœ", "GM", "å®˜æ–¹", "è¿è¥", "ç ”å‘"]):
            continue

        player_lines += 1

    return total_lines, player_lines


def get_covered_indices_from_cluster_output(output_cluster: str):
    """
    ä»æ¨¡å‹#2 çš„è‡ªç„¶è¯­è¨€è¾“å‡ºä¸­ï¼Œè§£ææ‰€æœ‰ â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[...æ•°å­—...]â€ é‡Œçš„æ•°å­—ï¼Œ
    æ”¶é›†æˆä¸€ä¸ª set è¿”å›ã€‚

    é€‚é…ç±»ä¼¼æ ¼å¼ï¼ˆä½ ç°åœ¨çš„è¾“å‡ºå°±æ˜¯è¿™æ ·ï¼‰ï¼š
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[88, 90]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[124,125,126]

    æ”¯æŒï¼š
    - ä¸­æ–‡/è‹±æ–‡å†’å·ï¼ˆ: / ï¼šï¼‰
    - ä¸­æ–‡/è‹±æ–‡ä¸­æ‹¬å·ï¼ˆ[] / ã€ã€‘ï¼‰
    - é€—å·å¯ä¸º , æˆ– ï¼Œ
    """
    covered = set()

    # æ­£åˆ™åŒ¹é… â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]â€
    pattern = r"å‘è¨€è¡Œå·åˆ—è¡¨[:ï¼š]\s*[\[\ã€]([0-9,\sï¼Œ]+)[\]\ã€‘]"

    for m in re.finditer(pattern, output_cluster):
        nums_str = m.group(1)  # é‡Œé¢æ˜¯ç±»ä¼¼ "1, 2, 3" æˆ– "124,125,126"
        # æ›¿æ¢ä¸­æ–‡é€—å·ï¼ŒæŒ‰é€—å·åˆ‡åˆ†
        nums_str = nums_str.replace("ï¼Œ", ",")
        for part in nums_str.split(","):
            p = part.strip()
            if not p:
                continue
            try:
                covered.add(int(p))
            except ValueError:
                continue

    return covered


#################################è¯æç°‡å”¯ä¸€id#################################

###1. clusteridçš„æ—¥æœŸæ‰’å–##
def infer_date_for_batch(
    cluster_json_list: List[Dict[str, Any]],
    batch_lines: List[str]
) -> str:
    """
    ä¼˜å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜ä¸­æŠ½å–æ—¥æœŸï¼ˆå½¢å¦‚ï¼šxxxxï¼ˆ2025-12-02 16:30:01-17:42:41ï¼‰ï¼‰
    è‹¥å¤±è´¥ï¼Œåˆ™å›é€€åˆ°åŸå§‹å‘è¨€ä¸­çš„ `å‘è¨€æ—¥æœŸ` / `æ—¥æœŸ` å­—æ®µã€‚
    """
    # 1ï¼‰å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜é‡Œæ‰¾ YYYY-MM-DD
    for obj in cluster_json_list:
        title = str(
            obj.get("è¯é¢˜ç°‡")
            or obj.get("èšåˆè¯é¢˜ç°‡")
            or ""
        )
        m = re.search(r"(\d{4}-\d{2}-\d{2})", title)
        if m:
            return m.group(1)

    # 2ï¼‰å¦‚æœæ ‡é¢˜é‡Œæ²¡æœ‰ï¼Œå°±ä»åŸå§‹å‘è¨€é‡Œæ‹¿
    for line in batch_lines:
        line = line.strip()
        if not line:
            continue
        try:
            msg = json.loads(line)
        except Exception:
            continue

        date_str = msg.get("å‘è¨€æ—¥æœŸ") or msg.get("æ—¥æœŸ")
        if date_str:
            return date_str

    # 3ï¼‰å†ä¸è¡Œå°±æŠ¥é”™ï¼Œè®©ä½ æ„è¯†åˆ°æ•°æ®æ ¼å¼æœ‰é—®é¢˜
    raise ValueError("æ— æ³•ä»è¯é¢˜ç°‡æ ‡é¢˜æˆ–åŸå§‹å‘è¨€ä¸­æ¨æ–­å‡ºæ—¥æœŸï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼ã€‚")

##2.ç”Ÿæˆè¯æç°‡clusterid##
def assign_global_cluster_ids(cluster_list, date_str, batch_id):
    """
    ä¸ºæ¯ä¸ªè¯é¢˜ç°‡ç”Ÿæˆå…¨å±€å”¯ä¸€IDå­—æ®µ `_cluster_id`
    æ ¼å¼ï¼šYYYY-MM-DD_BX_XXï¼Œå¦‚ 2025-11-20_B2_03
    """
    for idx, cluster in enumerate(cluster_list, start=1):
        cluster["_cluster_id"] = f"{date_str}_{batch_id}_{idx:02d}"
    return cluster_list

#################################èšåˆæ¯å¤©çš„è¯æç°‡åˆ†æ‰¹è¾“å‡º#################################
def aggregate_cluster_outputs(batch_outputs: List[str]) -> str:
    all_lines = []

    for batch_id, text in enumerate(batch_outputs, start=1):
        if not text:
            continue

        for line in text.strip().splitlines():
            line = line.strip()
            if not line:
                continue

            try:
                obj = json.loads(line)
            except Exception:
                # å¦‚æœæœ‰ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±è·³è¿‡ï¼ˆä¹Ÿå¯ä»¥æ”¹æˆ raiseï¼Œçœ‹ä½ éœ€æ±‚ï¼‰
                continue

            clean_line = json.dumps(obj, ensure_ascii=False)
            all_lines.append(clean_line)

    # èšåˆä¸ºä¸€ä¸ªå¤§çš„ JSONL å­—ç¬¦ä¸²
    return "\n".join(all_lines)


################top5ç­›é€‰#################################


# -------------------------------
# ğŸ”¹ 1. åŒ¹é…åŸå§‹å‘è¨€
# -------------------------------
from datetime import datetime
import json
from typing import List, Dict

def parse_time_range(date_str, range_str):
    # å¦‚æœåªæœ‰ä¸€ä¸ªæ—¶é—´ç‚¹è€Œä¸æ˜¯èŒƒå›´ï¼Œè·³è¿‡
    if "-" not in range_str:
        return None, None
    start_str, end_str = range_str.split("-")
    start_dt = datetime.strptime(f"{date_str} {start_str.strip()}", "%Y-%m-%d %H:%M:%S")
    end_dt = datetime.strptime(f"{date_str} {end_str.strip()}", "%Y-%m-%d %H:%M:%S")
    return start_dt, end_dt


def match_dialogs_by_time(messages, date_str, time_axis_str):
    time_ranges = time_axis_str.split("ã€")
    matched = []
    for tr in time_ranges:
        start, end = parse_time_range(date_str, tr)
        if not start or not end:
            continue  # è·³è¿‡æ— æ³•è§£æçš„æ—¶é—´æ®µ
        for row in messages:
            ts = datetime.strptime(f"{row['å‘è¨€æ—¥æœŸ']} {row['å‘è¨€æ—¶é—´']}", "%Y-%m-%d %H:%M:%S")
            if start <= ts <= end:
                matched.append(row)
    return matched

def get_time_axis(cluster: dict) -> str | None:
    # ä¼˜å…ˆæ ‡å‡†å­—æ®µ
    for key in ("æ—¶é—´è½´", "timeè½´", "æ—¶é—´è½´ ", "æ—¶é—´æ®µ", "æ—¶é—´åŒºé—´"):
        val = cluster.get(key)
        if isinstance(val, str) and val.strip():
            return val.strip()
    return None

def extract_cluster_stats(èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict], åŸå§‹å‘è¨€: List[str]) -> List[Dict]:
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    results = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = get_time_axis(cluster)
        if not date or not time_axis:
            print(f"âš  èšåˆè¯é¢˜ç°‡ç¼ºå°‘æ—¥æœŸæˆ–æ—¶é—´è½´ï¼Œè·³è¿‡ï¼š{cluster}")
            continue
        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)
        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        result = {
            "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡"),
            "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
            "å‘è¨€ç©å®¶æ€»æ•°": len(players),
            "å‘è¨€æ€»æ•°": len(matched)
        }
        results.append(result)

    return results

# -------------------------------
# ğŸ”¹ 2. çƒ­åº¦è¯†åˆ«ä¸»å‡½æ•°
# -------------------------------
def compute_heat_score(U: int, M: int) -> float:
    if U == 0 or M == 0:
        return 0.0
    return round(U * math.sqrt(M), 2)

def extract_top5_heat_clusters(èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict], åŸå§‹å‘è¨€: List[str], top_k=5) -> List[Dict]:
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    enriched = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = cluster.get("æ—¶é—´è½´")
        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)

        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        U = len(players)
        M = len(matched)
        heat = compute_heat_score(U, M)

        enriched.append({
        "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥",
        "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
        "æ—¥æœŸ": date,
        "æ—¶é—´è½´": time_axis,
        "å‘è¨€ç©å®¶æ€»æ•°": U,
        "å‘è¨€æ€»æ•°": M,
        "çƒ­åº¦è¯„åˆ†": heat
         })


    # æŒ‰çƒ­åº¦è¯„åˆ†æ’åºï¼Œå– TopK
    enriched.sort(key=lambda x: x["çƒ­åº¦è¯„åˆ†"], reverse=True)
    return enriched[:top_k]

# -------------------------------
# ğŸ”¹ 3. æ·»åŠ è®¨è®ºç‚¹å­—æ®µ
# -------------------------------
def attach_discussion_points(top_clusters: List[Dict], subclusters: List[Dict]) -> List[Dict]:
    """
    å°† top_clusters ä¸­æ¯ä¸ªèšåˆç°‡çš„å­è¯é¢˜ç°‡åˆ—è¡¨ï¼Œä¸ subclusters ä¸­çš„ _cluster_id åŒ¹é…ï¼Œ
    æ‹¼å‡ºæ ¸å¿ƒæœºåˆ¶æè¿°ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºåˆ—ç‚¹ + ç©ºè¡Œã€‚ç§»é™¤å­è¯é¢˜ç°‡åˆ—è¡¨å­—æ®µã€‚
    """
    # æ„å»º _cluster_id â†’ æ ¸å¿ƒæœºåˆ¶ æ˜ å°„
    cluster_mechanism_map = {
        row["_cluster_id"]: row["æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶"]
        for row in subclusters
        if "_cluster_id" in row and "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶" in row
    }

    result = []
    for cluster in top_clusters:
        ids = cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨", [])
        mechanisms = [cluster_mechanism_map.get(cid) for cid in ids if cid in cluster_mechanism_map]

        # æ ¼å¼åŒ–ä¸ºåˆ—ç‚¹ + ç©ºè¡Œ
        discussion_point = [m for m in mechanisms if m]

        # ä¿®å¤èšåˆè¯é¢˜ç°‡åç§°å­—æ®µä¸ºç©ºçš„é—®é¢˜
        cluster_name = cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥"

        enriched_cluster = {
            "èšåˆè¯é¢˜ç°‡": cluster_name,
            "æ—¥æœŸ": cluster.get("æ—¥æœŸ"),
            "æ—¶é—´è½´": cluster.get("æ—¶é—´è½´"),
            "å‘è¨€ç©å®¶æ€»æ•°": cluster.get("å‘è¨€ç©å®¶æ€»æ•°"),
            "å‘è¨€æ€»æ•°": cluster.get("å‘è¨€æ€»æ•°"),
            "çƒ­åº¦è¯„åˆ†": cluster.get("çƒ­åº¦è¯„åˆ†"),
            "è®¨è®ºç‚¹": discussion_point
        }

        result.append(enriched_cluster)

    return result



####################### å­˜å…¥æ¯æ—¥å‘è¨€ top5 ########################
from pathlib import Path
from typing import List, Dict
import json

####################### å­˜å…¥æ¯æ—¥å‘è¨€ top5 ########################
from pathlib import Path
from typing import List, Dict
import json

def append_daily_top5_to_version_jsonl(
    final_result: List[Dict],
    version_jsonl_path: str = "version_daily_top5_with_opinion.jsonl",
):
    """
    å°†å½“æ—¥ Top5 è¿½åŠ å†™å…¥æŸä¸ªã€ç‰ˆæœ¬ç´¯è®¡ jsonl æ–‡ä»¶ã€‘ä¸­ã€‚
    
    åŒæ—¶ä¸ºæ¯æ¡è®°å½•è¡¥å……ä¸¤ä¸ªå­—æ®µï¼š
    1ï¼‰_idxï¼šåœ¨ version_jsonl_path ä¸­çš„å…¨å±€é€’å¢è¡Œå·ï¼ˆä»1å¼€å§‹ï¼‰
    2ï¼‰_daily_top_idï¼šå½“å¤©å†…çš„ Top è¯é¢˜ç°‡IDï¼Œå½¢å¼ï¼šYYYY-MM-DD_TXXï¼Œä¾‹å¦‚ 2025-12-03_T01
    
    è¯´æ˜ï¼š
    - å‡è®¾ final_result å†…æ‰€æœ‰è®°å½•çš„ "æ—¥æœŸ" ç›¸åŒï¼ˆå³åŒä¸€å¤©çš„ top5ï¼‰
    - å¦‚æœæ–‡ä»¶ä¸­å·²å­˜åœ¨åŒä¸€æ—¥æœŸçš„æ•°æ®ï¼Œä¼šåœ¨åŸæœ‰åŸºç¡€ä¸Šç»§ç»­ç´¯åŠ  _daily_top_id çš„ç¼–å·
    - ä¸ä¼šä¿®æ”¹ä»»ä½•å·²æœ‰çš„ `_cluster_id` å­—æ®µï¼ˆå­è¯é¢˜ç°‡ä»ç„¶ç”¨å®ƒï¼‰
    """
    if not final_result:
        print("âš  final_result ä¸ºç©ºï¼Œä»Šæ—¥æ—  Top5 å¯å†™å…¥ã€‚")
        return

    # å–å½“å¤©æ—¥æœŸï¼ˆå‡è®¾ final_result åŒä¸€æ‰¹éƒ½æ˜¯åŒä¸€å¤©ï¼‰
    date_str = final_result[0].get("æ—¥æœŸ")
    if not date_str:
        raise ValueError("final_result ä¸­ç¼ºå°‘ 'æ—¥æœŸ' å­—æ®µï¼Œæ— æ³•ç”Ÿæˆ _daily_top_idã€‚")

    path = Path(version_jsonl_path)

    # ---------- 1. è®¡ç®—å½“å‰æ–‡ä»¶ä¸­çš„æœ€å¤§ _idxï¼ˆå…¨å±€è¡Œå·ï¼‰ ----------
    global_max_idx = 0
    # åŒæ—¶ç»Ÿè®¡â€œä»Šå¤©â€å·²ç»æœ‰å¤šå°‘æ¡ï¼Œç”¨äº _daily_top_id ç»­ç¼–å·
    existing_count_for_day = 0

    if path.exists():
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError:
                    continue

                # ç»Ÿè®¡å…¨å±€ _idx
                if isinstance(obj.get("_idx"), int):
                    global_max_idx = max(global_max_idx, obj["_idx"])
                else:
                    # æ—§æ•°æ®æ²¡æœ‰ _idxï¼Œå°±ç®€å•å½“ä½œä¸€è¡Œï¼Œæ–°è¡Œå·å¾€åæ¨
                    global_max_idx += 1

                # ç»Ÿè®¡å½“å¤©çš„æ¡æ•°ï¼ˆç”¨äº _daily_top_idï¼‰
                if obj.get("æ—¥æœŸ") == date_str:
                    existing_count_for_day += 1

    # ---------- 2. è¿½åŠ å†™å…¥ï¼Œå¹¶ä¸ºæ–°è®°å½•ç”Ÿæˆ _idx + _daily_top_id ----------
    mode = "a" if path.exists() else "w"
    with path.open(mode, encoding="utf-8") as f:
        for offset, row in enumerate(final_result, start=1):
            row = dict(row)  # é¿å…ä¿®æ”¹åŸå¯¹è±¡å¼•ç”¨

            # 2.1 å…¨å±€é€’å¢ _idxï¼ˆç‰ˆæœ¬æ‰€æœ‰å¤©å…±ç”¨ä¸€ä¸ªåºå·ï¼‰
            global_max_idx += 1
            row["_idx"] = global_max_idx

            # 2.2 ç”Ÿæˆå½“å¤©çš„ top idï¼ˆä¸ç¢° `_cluster_id`ï¼‰
            # å½“å¤©å·²æœ‰ existing_count_for_day æ¡ï¼Œè¿™æ‰¹ä» +1 å¼€å§‹æ¥ç€æ’
            idx_for_day = existing_count_for_day + offset
            row["_daily_top_id"] = f"{date_str}_T{idx_for_day:02d}"

            f.write(json.dumps(row, ensure_ascii=False) + "\n")

    print(f"âœ… å·²å°†å½“æ—¥ Top5ï¼ˆå« _idx å’Œ _daily_top_idï¼‰è¿½åŠ å†™å…¥: {path}")


def extract_time_axis_from_title(title: str) -> str:
    """
    ä»è¯é¢˜ç°‡æ ‡é¢˜ä¸­æå–æ—¶é—´è½´éƒ¨åˆ†ï¼Œå¦‚ï¼š
    'ç»´æŠ¤è¡¥å¿è¯‰æ±‚è®¨è®ºï¼ˆ2025-11-19 14:21:57-14:34:08ï¼‰'
    æˆ– 'ç»´æŠ¤è¡¥å¿è¯‰æ±‚è®¨è®º(2025-11-19 14:21:57-14:34:08)'
    -> '14:21:57-14:34:08'
    """
    if not title:
        return ""
    title = str(title)

    # æ”¯æŒå…¨è§’/åŠè§’æ‹¬å·ï¼šï¼ˆï¼‰ã€()
    pattern = r"[ï¼ˆ(](\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2}:\d{2}-\d{2}:\d{2}:\d{2})[ï¼‰)]"
    m = re.search(pattern, title)
    if m:
        # ä½ çš„ match_dialogs_by_time æ˜¯æ—¥æœŸ+æ—¶é—´æ®µåˆ†å¼€ä¼ çš„ï¼Œæ‰€ä»¥åªè¦ HH:MM:SS-HH:MM:SS å³å¯
        return m.group(2)
    return ""
#-------è§£ææ¨¡å‹#4çš„è¾“å‡ºæ–‡æœ¬----------------
def parse_and_normalize_opinion_output(
    opinion_output: str,
    topic_id: str,
    discussion_point: str,
) -> Optional[Dict[str, Any]]:
    """
    è§£ææ¨¡å‹#4è¾“å‡ºï¼Œå¹¶åšè½»åº¦è§„èŒƒåŒ–ã€‚
    å‰æï¼šopinion_output æ˜¯ä¸€ä¸ªå•ç‹¬çš„ JSON å¯¹è±¡å­—ç¬¦ä¸²ã€‚

    è¿”å›ï¼šè§„èŒƒåŒ–åçš„ dictï¼›
         å¦‚æœè§£æå¤±è´¥åˆ™è¿”å› Noneã€‚
    """
    if not opinion_output or not opinion_output.strip():
        return None

    s = opinion_output.strip()

    # 1ï¼‰æœ€ç›´æ¥æƒ…å†µï¼šæ•´æ®µå°±æ˜¯ä¸€ä¸ª JSON å¯¹è±¡
    try:
        obj = json.loads(s)
    except Exception:
        # 2ï¼‰å…œåº•ï¼šå¦‚æœä¸Šæ¸¸å“ªå¤©åˆåœ¨å‰ååŠ äº†è§£é‡Šæ–‡å­—ï¼Œ
        #    å°è¯•æˆªå–ç¬¬ä¸€å¯¹ { ... } ä¸­é—´çš„å†…å®¹å†è§£æä¸€æ¬¡
        start = s.find("{")
        end = s.rfind("}")
        if start == -1 or end == -1 or start >= end:
            return None
        try:
            obj = json.loads(s[start : end + 1])
        except Exception:
            return None

    # â€”â€” ç”¨æˆ‘ä»¬è‡ªå·±ä¼ è¿›æ¥çš„ä¿¡æ¯è¦†ç›–ï¼Œç¡®ä¿å¯¹å¾—ä¸Š top5_results â€”â€” 
    obj["è¯é¢˜ç°‡ID"] = topic_id          # é€šå¸¸æ˜¯ _cluster_idï¼Œæ¯”å¦‚ "2025-12-02_B2_01"
    obj["è®¨è®ºç‚¹"] = discussion_point    # ä¸Šæ¸¸â€œæ ¸å¿ƒå¯¹è±¡/æœºåˆ¶â€å­—æ®µ

    # â€”â€” ç»Ÿä¸€â€œä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹â€ä¸º list[str] â€”â€” 
    ex = obj.get("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹")
    if isinstance(ex, str):
        obj["ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"] = [ex.strip()] if ex.strip() else []
    elif isinstance(ex, list):
        obj["ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"] = [str(x).strip() for x in ex if str(x).strip()]
    else:
        obj["ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"] = []

    # â€”â€” åˆ†æ­§ç‚¹åšä¸ª stripï¼Œæ–¹ä¾¿åé¢åˆ¤æ–­â€œæ— æ˜æ˜¾åˆ†æ­§â€ â€”â€” 
    diff = obj.get("ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹")
    if isinstance(diff, str):
        obj["ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹"] = diff.strip()

    return obj

#--------------final_top5_result---------------------

def build_daily_top5_opinion_records(
    top5_results: List[Dict[str, Any]],
    sub_opinion_map: Dict[str, Dict[str, Any]],
) -> List[Dict[str, Any]]:
    final_records: List[Dict[str, Any]] = []

    for cluster in top5_results:
        date = cluster.get("æ—¥æœŸ")
        cluster_name = cluster.get("èšåˆè¯é¢˜ç°‡") or cluster.get("è¯é¢˜ç°‡") or "æœªçŸ¥"

        base = {
            "æ—¥æœŸ": date,
            "èšåˆè¯é¢˜ç°‡": cluster_name,
            "çƒ­åº¦è¯„åˆ†": cluster.get("çƒ­åº¦è¯„åˆ†"),
            "å‘è¨€ç©å®¶æ•°": cluster.get("å‘è¨€ç©å®¶æ€»æ•°"),
            "å‘è¨€æ€»æ•°": cluster.get("å‘è¨€æ€»æ•°"),
            "æ—¶é—´è½´": cluster.get("æ—¶é—´è½´"),
        }

        cid_list = cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨", []) or []

        discussion_items: List[Dict[str, Any]] = []
        all_examples: List[str] = []
        seen_examples = set()  # é˜²æ­¢åŒä¸€å¥åŸè¯é‡å¤

        for idx, cid in enumerate(cid_list, start=1):
            op = sub_opinion_map.get(cid)
            if not op:
                print(f"âš  å­è¯é¢˜ç°‡ {cid} æœªåœ¨ sub_opinion_map ä¸­æ‰¾åˆ°æ¨¡å‹#4ç»“æœï¼Œè·³è¿‡ã€‚")
                continue

            # åŠ¨æ€å­—æ®µåï¼šè®¨è®ºç‚¹1 / è®¨è®ºç‚¹2 / ...
            key_discussion = f"è®¨è®ºç‚¹{idx}"

            item: Dict[str, Any] = {
                key_discussion: op.get("è®¨è®ºç‚¹", ""),
                "ç©å®¶å…±è¯†": op.get("ç©å®¶å…±è¯†", ""),
            }

            # åªåœ¨ã€æœ‰çœŸå®åˆ†æ­§ã€‘æ—¶å†™å…¥â€œç©å®¶ä¸»è¦åˆ†æ­§ç‚¹â€
            diff = op.get("ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹", "")
            if isinstance(diff, str):
                diff_clean = diff.strip()
                if diff_clean and diff_clean != "æ— æ˜æ˜¾åˆ†æ­§":
                    item["ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹"] = diff_clean

            discussion_items.append(item)

            # æ”¶é›†å…¸å‹å‘è¨€
            examples = op.get("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹", [])
            if isinstance(examples, list):
                for ex in examples:
                    if not ex:
                        continue
                    s = str(ex)
                    if s in seen_examples:
                        continue
                    seen_examples.add(s)
                    all_examples.append(s)

        record = {
            **base,
            "è®¨è®ºç‚¹åˆ—è¡¨": discussion_items,
            "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": all_examples,
        }
        final_records.append(record)

    return final_records
def normalize_agg_cluster_key_name(raw_key: str) -> str:
    """
    å°†æ¨¡å‹#3è¾“å‡ºä¸­çš„å„ç§ä¹±ä¸ƒå…«ç³Ÿkeyï¼Œç»Ÿä¸€æ˜ å°„æˆè§„èŒƒå­—æ®µåã€‚

    ç›®æ ‡è§„èŒƒå­—æ®µï¼š
    - è¯é¢˜ç°‡
    - å­è¯é¢˜ç°‡åˆ—è¡¨
    - æ—¥æœŸ
    - æ—¶é—´è½´

    å…¶ä»–ä¹±é€ å­—æ®µä¿ç•™åŸåï¼ˆä½ ä¹‹åå¦‚æœæƒ³ç»§ç»­æ‰©å±•ï¼Œå¯ä»¥åœ¨è¿™é‡ŒåŠ è§„åˆ™ï¼‰
    """
    if not isinstance(raw_key, str):
        raw_key = str(raw_key)
    k = raw_key.strip()
    lower = k.lower()

    # 1) è¯é¢˜ç°‡æœ¬ä½“ï¼ˆèšåˆåçš„åå­—ï¼‰
    if k in ("è¯é¢˜ç°‡", "èšåˆè¯é¢˜ç°‡", "èšåˆç°‡", "èšåˆè¯æç°‡"):
        return "è¯é¢˜ç°‡"
    # è¯¯æ‹¼ï¼šè¯æç°‡ / è¯ææ—
    if ("è¯é¢˜" in k or "è¯æ" in k) and ("ç°‡" in k or "æ—" in k) and "å­" not in k:
        return "è¯é¢˜ç°‡"

    # 2) å­è¯é¢˜ç°‡åˆ—è¡¨
    if "å­" in k and ("è¯é¢˜" in k or "è¯æ" in k) and ("ç°‡" in k or "æ—" in k):
        return "å­è¯é¢˜ç°‡åˆ—è¡¨"
    if "å­ç°‡" in k or "å­è¯é¢˜ç°‡id" in lower:
        return "å­è¯é¢˜ç°‡åˆ—è¡¨"

    # 3) æ—¥æœŸ
    if "æ—¥æœŸ" in k or lower == "date" or "å‘è¨€æ—¥æœŸ" in k:
        return "æ—¥æœŸ"

    # 4) æ—¶é—´è½´ï¼ˆå„ç§å¥‡æ€ªå†™æ³•ï¼štimeè½´ / æ—¶é—´æ / æè½´ / æ—¶é—´è½´ å¸¦ç©ºæ ¼ç­‰ï¼‰
    if "æ—¶é—´è½´" == k:
        return "æ—¶é—´è½´"
    if "æ—¶é—´" in k or "æ—¶" in k or "è½´" in k or "æè½´" in k or "time" in lower:
        # è¿™é‡Œç›´æ¥æ”¶æ•›æˆ æ—¶é—´è½´ï¼Œä½ çš„æ•°æ®ç»“æ„é‡Œæœ¬æ¥ä¹Ÿåªåº”è¯¥æœ‰ä¸€ä¸ªâ€œæ—¶é—´æ®µå­—æ®µâ€
        return "æ—¶é—´è½´"

    # 5) å…¶ä»–å­—æ®µï¼ˆå¯èƒ½æ˜¯ä½ ä¹‹åç®—çš„ å‘è¨€ç©å®¶æ€»æ•° / å‘è¨€æ€»æ•° / çƒ­åº¦è¯„åˆ† ç­‰ï¼‰åŸæ ·ä¿ç•™
    return k
def normalize_agg_cluster_object(obj: dict) -> dict:
    """
    å¯¹ã€ä¸€æ¡ã€‘èšåˆè¯é¢˜ç°‡è®°å½•åš key å½’ä¸€åŒ–ï¼š
    - æŠŠå„ç§é”™åˆ«å­—ã€å˜ä½“key æ”¶æ•›æˆæ ‡å‡†key
    - å¦‚æœå¤šä¸ª key æ˜ å°„åˆ°åŒä¸€ä¸ªæ–°keyï¼ŒæŒ‰ç®€å•ç­–ç•¥åˆå¹¶ï¼š
        - å¦‚æœéƒ½æ˜¯ listï¼Œå°±ç›´æ¥æ‹¼æ¥
        - å¦åˆ™åå‡ºç°çš„è¦†ç›–å‰é¢çš„ï¼ˆä½ ä¹Ÿå¯ä»¥æ¢æˆä¿ç•™ç¬¬ä¸€ä¸ªï¼‰
    """
    fixed = {}
    for raw_k, v in obj.items():
        new_k = normalize_agg_cluster_key_name(raw_k)

        # åˆå¹¶ç­–ç•¥
        if new_k in fixed:
            old_v = fixed[new_k]
            if isinstance(old_v, list) and isinstance(v, list):
                fixed[new_k] = old_v + v
            else:
                # é»˜è®¤ç”¨åè€…è¦†ç›–å‰è€…ï¼›å¦‚æœä½ æ›´æƒ³ä¿ç•™ç¬¬ä¸€æ¬¡ç»“æœï¼Œå¯ä»¥æ”¹æˆ `continue`
                fixed[new_k] = v
        else:
            fixed[new_k] = v

    return fixed
import json

def fix_output_cluster_agg_keys(output_cluster_agg: str) -> str:
    """
    è¯»å–æ¨¡å‹#3è¿”å›çš„æ–‡æœ¬ï¼ˆoutput_cluster_aggï¼‰ï¼Œ
    é€è¡Œè§£æ JSONï¼Œå¯¹æ¯æ¡è®°å½•çš„ key åšè§„èŒƒåŒ–å¤„ç†ï¼Œ
    å†é‡æ–° dump å› jsonl å­—ç¬¦ä¸²ã€‚

    - è‡ªåŠ¨è·³è¿‡é JSON è¡Œï¼ˆæ¯”å¦‚æ¨¡å‹åœ¨å‰åä¹±åŠ çš„è¯´æ˜ï¼‰
    - è‡ªåŠ¨è·³è¿‡æ— æ³•è§£æçš„è„è¡Œï¼Œå¹¶æ‰“å°å‘Šè­¦
    """
    fixed_lines = []

    for line in output_cluster_agg.strip().splitlines():
        line = line.strip()
        if not line:
            continue
        if not line.startswith("{"):
            # é JSON è¡Œç»Ÿä¸€è·³è¿‡ï¼Œé¿å… json.loads æŠ¥é”™
            print("âš  æ¨¡å‹#3 è¾“å‡ºä¸­å‘ç°é JSON è¡Œï¼Œè¢«è·³è¿‡ï¼š", line[:80])
            continue

        try:
            obj = json.loads(line)
        except json.JSONDecodeError as e:
            print("âš  èšåˆç°‡ JSON è§£æå¤±è´¥ï¼Œå·²è·³è¿‡ï¼š", e, "åŸå§‹è¡Œ:", line[:120])
            continue

        fixed_obj = normalize_agg_cluster_object(obj)
        fixed_lines.append(json.dumps(fixed_obj, ensure_ascii=False))

    return "\n".join(fixed_lines)
