
from __future__ import annotations
import json, time, typing as T
import pandas as pd
import requests
import math
from pathlib import Path
from typing import List, Dict, Any,Optional,Union,Tuple
import re, json, unicodedata
from datetime import datetime
import json
from collections import defaultdict
from json import JSONDecodeError
# --- openpyxl æ ·å¼/å·¥å…· ---
from openpyxl import Workbook, load_workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, NamedStyle

# --- docx æ ·å¼/å·¥å…· ---
from docx.oxml import OxmlElement
from docx import Document
from docx.shared import Pt
from docx.oxml.ns import qn


################æ¨¡å‹è°ƒç”¨ï¼Œå‡ºç»“æœ###################

def load_system_prompt(path: Path) -> str:
    if not path.exists():
        raise FileNotFoundError(f"Prompt file not found: {path}")
    return path.read_text(encoding="utf-8")

def build_user_prompt_filter(json_lines: T.List[str]) -> str:
    # æ¨¡å‹#1ï¼šç­›æ‰éæ¸¸æˆç›¸å…³ï¼Œåªè¾“å‡ºç›¸å…³ JSON è¡Œï¼ˆåŸæ ·ï¼‰
    return (
        "ä»¥ä¸‹æ˜¯è‹¥å¹²ç©å®¶/å®¢æœ/ç ”å‘çš„å‘è¨€è®°å½•ï¼Œè¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­è§„åˆ™ï¼Œ"
        "åˆ¤æ–­å“ªäº›æ˜¯ã€ä¸æ¸¸æˆå†…å®¹ç›¸å…³ã€‘çš„å‘è¨€ï¼Œä¿ç•™è¿™äº› JSON è¡Œï¼Œä¸ç›¸å…³çš„å¿½ç•¥ã€‚"
        "è¯·ä»…è¾“å‡ºã€ç›¸å…³å‘è¨€çš„åŸå§‹ JSON è¡Œã€‘ï¼Œä¸¥æ ¼ä¿æŒæ ¼å¼ä¸å˜ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + "\n".join(json_lines)
    )

def build_user_prompt_clsuter(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )

def build_user_prompt_cluster_agg(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
        
    )



from typing import List, Any

def build_user_prompt_subcluster_opinion(
    discussion_point: str,
    json_lines: List[Any],   # å®é™…ä¼ çš„æ˜¯ list[str]
) -> str:
    dp = (discussion_point or "").strip()
    jsonl_block = "\n".join(
        (line or "").strip() for line in json_lines if line
    )
    return (
        "ä½ å°†æ”¶åˆ°ä¸€æ®µç©å®¶èŠå¤©åŸæ–‡ï¼ˆJSONLï¼Œæ¯è¡Œä¸€æ¡ï¼‰ã€‚æœ¬æ¬¡åªåˆ†ææŒ‡å®šã€è®¨è®ºç‚¹ã€‘ï¼Œå…¶ä»–å†…å®¹å¿½ç•¥ã€‚\n\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡º1ä¸ªJSONå¯¹è±¡ï¼Œç¦æ­¢è§£é‡Šæ–‡å­—ï¼Œç¦æ­¢Markdownä»£ç å—ã€‚\n\n"
        "ã€è¾“å…¥è¯é¢˜ç‚¹ã€‘ï¼š\n" + dp + "\n\n"
        "ã€è¾“å…¥åŸæ–‡JSONLã€‘\n" + jsonl_block
    )

def call_ark_chat_completions(
    api_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    temperature: float = 0.3,
    max_tokens: int = 32700,
    timeout: int = 600,
    retries: int = 2,
) -> str:
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    last_err = None
    for attempt in range(retries + 1):
        try:
            resp = requests.post(api_url, headers=headers, json=payload, timeout=timeout)
            if resp.status_code != 200:
                raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            last_err = e
            time.sleep(1.2 * (attempt + 1))
    raise RuntimeError(f"Ark API è°ƒç”¨å¤±è´¥: {last_err}")

def extract_valid_json_lines(text: str) -> T.List[str]:
    """
    æŠŠæ¨¡å‹è¾“å‡ºé‡Œçš„çº¯ JSON è¡Œæå–å‡ºæ¥ï¼ˆé²æ£’å¤„ç†ï¼‰ï¼š
    - é€è¡Œåˆ¤æ–­ï¼šä»¥ { å¼€å¤´ ä¸” ä»¥ } ç»“å°¾ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ª JSON å¯¹è±¡è¡Œ
    - ä¹Ÿèƒ½å®¹å¿å‰åå¤šä½™ç©ºè¡Œæˆ–è§£é‡Šæ–‡å­—ï¼ˆä¼šè¢«å¿½ç•¥ï¼‰
    """
    lines = []
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith("{") and s.endswith("}"):
            lines.append(s)
    return lines

def add_index_to_jsonl_lines(jsonl_lines):
    """
    ç»™åŸå§‹ jsonl æ¯æ¡å‘è¨€åŠ ä¸Šä¸€ä¸ªå”¯ä¸€è¡Œå·å­—æ®µ `_idx`ï¼Œ
    è¿”å›æ–°çš„ List[str]ï¼Œæ¯ä¸ªå…ƒç´ ä»ç„¶æ˜¯ä¸€è¡Œ JSON å­—ç¬¦ä¸²ã€‚
    """
    new_lines = []
    for idx, line in enumerate(jsonl_lines, start=1):
        line = line.strip()
        if not line:
            continue
        obj = json.loads(line)
        obj["_idx"] = idx  # æ–°å¢è¡Œå·
        new_lines.append(json.dumps(obj, ensure_ascii=False))
    return new_lines


def count_output_filter_stats(output_filter: str):
    """
    ç»Ÿè®¡æ¨¡å‹#1 è¾“å‡ºä¸­çš„ï¼š
    - æ€»è¡Œæ•° total_linesï¼ˆç©å®¶+å®¢æœ+ç ”å‘ï¼‰
    - ç©å®¶å‘è¨€è¡Œæ•° player_linesï¼ˆç®€å•ç”¨ å‘è¨€äººID é‡Œçš„å…³é”®è¯åŒºåˆ†ï¼‰
    """
    total_lines = 0
    player_lines = 0

    for line in output_filter.splitlines():
        line = line.strip()
        if not line:
            continue
        total_lines += 1

        try:
            obj = json.loads(line)
        except Exception:
            # å¦‚æœè¿™ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±åªèƒ½ç®—è¿› total_linesï¼Œæ— æ³•ç»†åˆ†
            continue

        speaker = (
            obj.get("ç©å®¶ID")
            or obj.get("å‘è¨€äººID")
            or obj.get("è§’è‰²ID")
            or ""
        )

        # ç®€å•æ’é™¤å®¢æœ/GM/å®˜æ–¹/è¿è¥/ç ”å‘ï¼Œå…¶ä½™è§†ä¸ºç©å®¶
        if any(key in str(speaker) for key in ["å®¢æœ", "GM", "å®˜æ–¹", "è¿è¥", "ç ”å‘"]):
            continue

        player_lines += 1

    return total_lines, player_lines


def get_covered_indices_from_cluster_output(output_cluster: str):
    """
    ä»æ¨¡å‹#2 çš„è‡ªç„¶è¯­è¨€è¾“å‡ºä¸­ï¼Œè§£ææ‰€æœ‰ â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[...æ•°å­—...]â€ é‡Œçš„æ•°å­—ï¼Œ
    æ”¶é›†æˆä¸€ä¸ª set è¿”å›ã€‚

    é€‚é…ç±»ä¼¼æ ¼å¼ï¼ˆä½ ç°åœ¨çš„è¾“å‡ºå°±æ˜¯è¿™æ ·ï¼‰ï¼š
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[88, 90]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[124,125,126]

    æ”¯æŒï¼š
    - ä¸­æ–‡/è‹±æ–‡å†’å·ï¼ˆ: / ï¼šï¼‰
    - ä¸­æ–‡/è‹±æ–‡ä¸­æ‹¬å·ï¼ˆ[] / ã€ã€‘ï¼‰
    - é€—å·å¯ä¸º , æˆ– ï¼Œ
    """
    covered = set()

    # æ­£åˆ™åŒ¹é… â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]â€
    pattern = r"å‘è¨€è¡Œå·åˆ—è¡¨[:ï¼š]\s*[\[\ã€]([0-9,\sï¼Œ]+)[\]\ã€‘]"

    for m in re.finditer(pattern, output_cluster):
        nums_str = m.group(1)  # é‡Œé¢æ˜¯ç±»ä¼¼ "1, 2, 3" æˆ– "124,125,126"
        # æ›¿æ¢ä¸­æ–‡é€—å·ï¼ŒæŒ‰é€—å·åˆ‡åˆ†
        nums_str = nums_str.replace("ï¼Œ", ",")
        for part in nums_str.split(","):
            p = part.strip()
            if not p:
                continue
            try:
                covered.add(int(p))
            except ValueError:
                continue

    return covered



################################ä¿®è¡¥bugè¯æç°‡åˆ’åˆ†è§£æ#########################

def _normalize_json_text(text: str) -> str:
    """
    å¯¹æ¨¡å‹è¾“å‡ºåšä¸€äº›å°ä¿®å¤ï¼Œä»¥ä¾¿ json.loads æ­£å¸¸è§£æï¼š
    1ï¼‰å»æ‰æ•´æ®µæœ«å°¾å¤šä½™çš„é€—å·ï¼š{"a":1},
    2ï¼‰å»æ‰ å±æ€§/å…ƒç´  åç´§è·Ÿ } æˆ– ] çš„éæ³•é€—å·ï¼š
        ä¾‹å¦‚ï¼š
            "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "xxx",
        }
        â†’  "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "xxx"
           }
    """
    text = text.rstrip()

    # 1) æ•´ä¸ªå¯¹è±¡æœ«å°¾å¤šæ‰“ä¸€é¢—é€—å·ï¼š{"a":1},
    if text.endswith(","):
        text = text[:-1].rstrip()

    # 2) å±æ€§/å…ƒç´ åç›´æ¥è·Ÿ } æˆ– ] çš„é€—å·ï¼š
    #    æŠŠ ",\n  }" æˆ– ",\n]" å˜æˆ "\n  }" æˆ– "\n]"
    text = re.sub(r",\s*(\n\s*[}\]])", r"\1", text)

    return text

def parse_model2_output_to_json_list(output_cluster: str, batch_idx: int = 0) -> List[Dict[str, Any]]:
    """
    æŠŠæ¨¡å‹#2çš„åŸå§‹è¾“å‡ºè§£ææˆ list[dict]ï¼š
    - æ”¯æŒä¸€è¡Œä¸€ä¸ª JSONï¼š
        {"è¯é¢˜ç°‡1": "...", "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "..."}
    - ä¹Ÿæ”¯æŒå¤šè¡Œæ¼‚äº® JSONï¼š
        {
          "è¯é¢˜ç°‡1": "...",
          "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "..."
        }
    - è‡ªåŠ¨å¿½ç•¥è§£é‡Šæ€§æ–‡å­—ã€markdown åˆ—è¡¨ "- {...}"
    - è°ƒç”¨ _normalize_json_text ä¿®å¤å°¾é€—å·ç­‰æ ¼å¼é—®é¢˜
    - è§£æå¤±è´¥æ—¶æ‰“å°å®Œæ•´å¯¹è±¡åŸæ–‡ï¼Œä½†ä¸ä¸­æ–­æ•´æ‰¹
    """
    objs: List[Dict[str, Any]] = []
    cur_lines: List[str] = []
    depth = 0  # å½“å‰å¤§æ‹¬å·åµŒå¥—å±‚æ•°

    lines = output_cluster.strip().splitlines()

    for idx, raw in enumerate(lines, start=1):
        line = (raw or "").rstrip()
        if not line.strip():
            continue

        # å»æ‰ markdown åˆ—è¡¨å‰ç¼€ï¼ˆå¸¸è§æ ¼å¼ï¼š"- {...}"ï¼‰
        if line.lstrip().startswith("- "):
            line = line.lstrip()[2:].lstrip()

        # å½“å‰è¿˜æ²¡è¿›å…¥ä»»ä½• JSON å¯¹è±¡ï¼Œå¹¶ä¸”è¿™ä¸€è¡Œä¹Ÿçœ‹ä¸åˆ° "{"
        if depth == 0 and "{" not in line:
            # å¤§æ¦‚ç‡æ˜¯è§£é‡Šæ€§æ–‡å­—ï¼Œæ¯”å¦‚ "ä»¥ä¸‹æ˜¯è¯é¢˜ç°‡..."
            # å¦‚æœæƒ³çœ‹ï¼Œå¯æ‰“å¼€ä¸‹é¢è¿™è¡Œï¼š
            # tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] â© è·³è¿‡éJSONè¡Œ #{idx}: {line[:80]}")
            continue

        # ç»Ÿè®¡è¿™ä¸€è¡Œçš„å¤§æ‹¬å·æ•°é‡
        open_cnt = line.count("{")
        close_cnt = line.count("}")

        if depth == 0:
            # åˆšåˆšè¿›å…¥ä¸€ä¸ªæ–°çš„å¯¹è±¡
            cur_lines = [line]
            depth = open_cnt - close_cnt

            # depth <= 0 è¯´æ˜è¿™æ˜¯â€œå•è¡Œå¯¹è±¡â€ï¼š{"a":1} æˆ– {"a":1},
            if depth <= 0:
                text = "\n".join(cur_lines)
                text = _normalize_json_text(text)
                try:
                    obj = json.loads(text)
                except JSONDecodeError as e:
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] âŒ JSONè§£æå¤±è´¥ï¼ˆè¡Œ#{idx}é™„è¿‘ï¼‰ï¼š{e}")
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] è¯¥å¯¹è±¡åŸæ–‡ï¼š{text}")
                else:
                    # ç»Ÿä¸€ "è¯é¢˜ç°‡1"/"è¯é¢˜ç°‡2"... -> "è¯é¢˜ç°‡"
                    for key in list(obj.keys()):
                        if key.startswith("è¯é¢˜ç°‡") and key != "è¯é¢˜ç°‡":
                            obj["è¯é¢˜ç°‡"] = obj.pop(key)
                    objs.append(obj)
                cur_lines = []
                depth = 0
        else:
            # å·²ç»åœ¨ä¸€ä¸ª JSON å¯¹è±¡å†…éƒ¨ï¼ˆå¤šè¡Œåœºæ™¯ï¼‰
            cur_lines.append(line)
            depth += open_cnt - close_cnt

            # depth å›åˆ° 0ï¼Œè¡¨ç¤ºä¸€ä¸ªå¯¹è±¡ç»“æŸ
            if depth <= 0:
                text = "\n".join(cur_lines)
                text = _normalize_json_text(text)

                try:
                    obj = json.loads(text)
                except JSONDecodeError as e:
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] âŒ JSONè§£æå¤±è´¥ï¼ˆè¡Œ#{idx}é™„è¿‘ï¼‰ï¼š{e}")
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] è¯¥å¯¹è±¡åŸæ–‡ï¼š{text}")
                else:
                    for key in list(obj.keys()):
                        if key.startswith("è¯é¢˜ç°‡") and key != "è¯é¢˜ç°‡":
                            obj["è¯é¢˜ç°‡"] = obj.pop(key)
                    objs.append(obj)

                cur_lines = []
                depth = 0

    return objs
    
def parse_jsonl_text_safe(text: str, label: str = "æ¨¡å‹#3èšåˆè¾“å‡º") -> List[Dict[str, Any]]:
    """
    å°è¯•æŠŠ text æŒ‰è¡Œè§£æä¸º JSON å¯¹è±¡ï¼š
    - æ¯è¡Œç‹¬ç«‹å°è¯• json.loads
    - è§£æå‰å…ˆå¯¹â€œæè½´â€ç­‰å¸¸è§é”™è¯¯åšä¸€æ¬¡æ­£åˆ™ä¿®å¤
    - è§£æå¤±è´¥æ—¶ä¸ä¼šæŠ›å¼‚å¸¸ï¼Œè€Œæ˜¯æ‰“å°å‡ºå…·ä½“è¡Œå·å’ŒåŸæ–‡ï¼Œæ–¹ä¾¿æ’æŸ¥
    """
    objs: List[Dict[str, Any]] = []
    lines = text.strip().splitlines()

    for idx, raw in enumerate(lines, start=1):
        s = (raw or "").strip()
        if not s:
            continue

        # è·³è¿‡ ```json / ``` ç­‰ä»£ç å—æ ‡è®°
        if s.startswith("```"):
            continue

        # â­ å…ˆä¿®å¤â€œæè½´â€ç›¸å…³æ ¼å¼é”™è¯¯
        s_fixed = fix_model3_line_extreme_axis(s)

        try:
            obj = json.loads(s_fixed)
        except JSONDecodeError as e:
            print(f"[{label}] âŒ JSONè§£æå¤±è´¥ï¼šè¡Œ#{idx} -> {e}")
            print(f"[{label}] è¯¥è¡ŒåŸæ–‡(ä¿®å¤å)ï¼š{s_fixed}")
            continue

        objs.append(obj)

    return objs

def fix_model3_line_extreme_axis(s: str) -> str:
    """
    ä¸“é—¨ä¿®å¤æ¨¡å‹#3è¾“å‡ºä¸­â€œæè½´â€ç›¸å…³çš„åæ ¼å¼ï¼š
    é’ˆå¯¹ä»¥ä¸‹å‡ ç§æƒ…å†µï¼š
    1) "æ—¥æœŸ": "æè½´": "2025-12-06"  =>  "æ—¥æœŸ": "2025-12-06"
    2) "æ—¶é—´è½´": "æè½´": "22:30:57-22:33:57"  =>  "æ—¶é—´è½´": "22:30:57-22:33:57"
    3) "æ—¶é—´è½´": "22:34:24-æè½´": "22:38:33" => "æ—¶é—´è½´": "22:34:24-22:38:33"
    """

    # 1) ä¿®å¤æ—¥æœŸå­—æ®µè¢«â€œæè½´â€æ±¡æŸ“ï¼š
    #    "æ—¥æœŸ": "æè½´": "2025-12-06"
    s = re.sub(
        r'"æ—¥æœŸ"\s*:\s*"æè½´"\s*:\s*"([^"]+)"',
        r'"æ—¥æœŸ": "\1"',
        s
    )

    # 2) ä¿®å¤æ—¶é—´è½´å­—æ®µå®Œå…¨æ˜¯ "æè½´": "xxx" çš„æƒ…å†µï¼š
    #    "æ—¶é—´è½´": "æè½´": "22:30:57-22:33:57"
    s = re.sub(
        r'"æ—¶é—´è½´"\s*:\s*"æè½´"\s*:\s*"([^"]+)"',
        r'"æ—¶é—´è½´": "\1"',
        s
    )

    # 3) ä¿®å¤æ—¶é—´è½´å€¼ä¸­é—´å¸¦ "-æè½´": " çš„æƒ…å†µï¼š
    #    "æ—¶é—´è½´": "22:34:24-æè½´": "22:38:33"
    #    -> "æ—¶é—´è½´": "22:34:24-22:38:33"
    s = re.sub(
        r'"æ—¶é—´è½´"\s*:\s*"([^"]*?)-æè½´"\s*:\s*"([^"]+)"',
        r'"æ—¶é—´è½´": "\1-\2"',
        s
    )

    return s


def _strip_fences(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
    s = re.sub(r"\s*```$", "", s)
    return s.strip()

def _as_list(v):
    if v is None:
        return []
    if isinstance(v, list):
        return v
    return [v]

def _merge_time_axes(time_axes: list[str]) -> str:
    seen, out = set(), []
    for t in time_axes:
        t = (t or "").strip()
        if not t or t in seen:
            continue
        seen.add(t)
        out.append(t)
    return "ã€".join(out)


def clean_time_axis(raw: str) -> str:
    """
    ä¿ç•™ï¼šæ•°å­—ã€å†’å·ã€æ¨ªæ ã€é¡¿å·ã€ç©ºç™½ï¼›å…¶ä»–å…¨éƒ¨åˆ æ‰
    """
    if not raw:
        return ""
    return re.sub(r"[^\d:ã€\-\s]", "", str(raw))


def normalize_model3_clusters(output_text: str, parsed_subclusters: list[dict]):
     # 0) å…ˆè®©å­ç°‡å…·å¤‡ æ—¥æœŸ/æ—¶é—´è½´
    parsed_subclusters = enrich_subclusters_with_datetime(parsed_subclusters)
   
    

    # 1) å»ºç´¢å¼•ï¼š_cluster_id -> å­ç°‡(å«æ—¥æœŸ/æ—¶é—´è½´)
    sub_by_id = {}
    for sc in parsed_subclusters:
        cid = (sc.get("_cluster_id") or "").strip()
        if cid:
            sub_by_id[cid] = sc

    # 2) é€è¡Œä¿®å¤â€œæè½´â€æ±¡æŸ“å† parse
    raw = _strip_fences(output_text)
    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
    lines = [fix_model3_line_extreme_axis(ln) for ln in lines]

    objs = []
    for ln in lines:
        try:
            objs.append(json.loads(ln))
        except Exception:
            continue

    normalized = []
    for obj in objs:
        topic = (obj.get("è¯é¢˜ç°‡") or obj.get("èšåˆè¯é¢˜ç°‡") or obj.get("è¯é¢˜ç°‡3") or "").strip()

        sub_list = (
            obj.get("å­è¯é¢˜ç°‡åˆ—è¡¨")
            or obj.get("å­è¯é¢˜ç°‡")
            or obj.get("å­è¯é¢˜ç°‡idåˆ—è¡¨")
            or obj.get("å­è¯é¢˜ç°‡IDåˆ—è¡¨")
            or []
        )
        sub_list = [str(x).strip() for x in _as_list(sub_list) if str(x).strip()]

        date = (obj.get("æ—¥æœŸ") or "").strip()
        time_axis = (obj.get("æ—¶é—´è½´") or "").strip()

        # 3) ç¼ºå¤±å›å¡«ï¼šä»å­ç°‡å–æ—¥æœŸ/æ—¶é—´è½´
        if (not date) or (not time_axis):
            sub_dates, sub_axes = [], []
            for cid in sub_list:
                sc = sub_by_id.get(cid)
                if not sc:
                    continue
                d = (sc.get("æ—¥æœŸ") or "").strip()
                ta = (sc.get("æ—¶é—´è½´") or "").strip()
                if d:
                    sub_dates.append(d)
                if ta:
                    sub_axes.append(ta)
            if not date and sub_dates:
                date = sub_dates[0]
            if not time_axis and sub_axes:
                time_axis = _merge_time_axes(sub_axes)

        # 4) å¼ºåˆ¶ schemaï¼šç¼ºå…³é”®å­—æ®µå°±ä¸¢å¼ƒï¼Œé¿å…åç»­ split(None) å´©
        if not topic or not sub_list or not date or not time_axis:
            continue

        normalized.append({
            "è¯é¢˜ç°‡": topic,
            "å­è¯é¢˜ç°‡åˆ—è¡¨": sub_list,
            "æ—¥æœŸ": date,
            "æ—¶é—´è½´": clean_time_axis(time_axis)
        })

    return normalized, parsed_subclusters

######åŒ¹é…all_CLUSTERæ—¶é—´

# åŒ¹é…ï¼šâ€¦â€¦ï¼ˆ2025-12-07 22:40:36-22:41:27ï¼‰ æˆ– â€¦â€¦(2025-12-07 22:40:36-22:41:27)

SUB_TIME_RE = re.compile(
    r"[ï¼ˆ(]?\s*(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2}:\d{2}-\d{2}:\d{2}:\d{2})\s*[ï¼‰)]?"
)



def enrich_subclusters_with_datetime(parsed_subclusters: list[dict]) -> list[dict]:
    out = []
    for sc in parsed_subclusters:
        sc2 = dict(sc)
        title = sc2.get("è¯é¢˜ç°‡") or ""
        m = SUB_TIME_RE.search(title)

        if m:
            sc2["æ—¥æœŸ"] = m.group(1)
            sc2["æ—¶é—´è½´"] = m.group(2)
        else:
            # âœ… æ–°å¢ï¼šä»â€œå‘è¨€æ—¶é—´â€é‡Œè¡¥é½
            ft = (sc2.get("å‘è¨€æ—¶é—´") or "").strip()
            m2 = SUB_TIME_RE.search(ft)
            if m2:
                sc2["æ—¥æœŸ"] = m2.group(1)
                sc2["æ—¶é—´è½´"] = m2.group(2)
            else:
                # å…œåº•ï¼šæ—¥æœŸä» _cluster_id å–
                cid = (sc2.get("_cluster_id") or "").strip()
                if cid and re.match(r"^\d{4}-\d{2}-\d{2}_", cid):
                    sc2["æ—¥æœŸ"] = cid[:10]
                sc2.setdefault("æ—¶é—´è½´", "")

        out.append(sc2)
    return out

#################################è¯æç°‡å”¯ä¸€id#################################

###1. clusteridçš„æ—¥æœŸæ‰’å–##
def infer_date_for_batch(
    cluster_json_list: List[Dict[str, Any]],
    batch_lines: List[str]
) -> str:
    """
    ä¼˜å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜ä¸­æŠ½å–æ—¥æœŸï¼ˆå½¢å¦‚ï¼šxxxxï¼ˆ2025-12-02 16:30:01-17:42:41ï¼‰ï¼‰
    è‹¥å¤±è´¥ï¼Œåˆ™å›é€€åˆ°åŸå§‹å‘è¨€ä¸­çš„ `å‘è¨€æ—¥æœŸ` / `æ—¥æœŸ` å­—æ®µã€‚
    """
    # 1ï¼‰å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜é‡Œæ‰¾ YYYY-MM-DD
    for obj in cluster_json_list:
        title = str(
            obj.get("è¯é¢˜ç°‡")
            or obj.get("èšåˆè¯é¢˜ç°‡")
            or ""
        )
        m = re.search(r"(\d{4}-\d{2}-\d{2})", title)
        if m:
            return m.group(1)

    # 2ï¼‰å¦‚æœæ ‡é¢˜é‡Œæ²¡æœ‰ï¼Œå°±ä»åŸå§‹å‘è¨€é‡Œæ‹¿
    for line in batch_lines:
        line = line.strip()
        if not line:
            continue
        try:
            msg = json.loads(line)
        except Exception:
            continue

        date_str = msg.get("å‘è¨€æ—¥æœŸ") or msg.get("æ—¥æœŸ")
        if date_str:
            return date_str

    # 3ï¼‰å†ä¸è¡Œå°±æŠ¥é”™ï¼Œè®©ä½ æ„è¯†åˆ°æ•°æ®æ ¼å¼æœ‰é—®é¢˜
    raise ValueError("æ— æ³•ä»è¯é¢˜ç°‡æ ‡é¢˜æˆ–åŸå§‹å‘è¨€ä¸­æ¨æ–­å‡ºæ—¥æœŸï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼ã€‚")

##2.ç”Ÿæˆè¯æç°‡clusterid##
def assign_global_cluster_ids(cluster_list, date_str, batch_id):
    """
    ä¸ºæ¯ä¸ªè¯é¢˜ç°‡ç”Ÿæˆå…¨å±€å”¯ä¸€IDå­—æ®µ `_cluster_id`
    æ ¼å¼ï¼šYYYY-MM-DD_BX_XXï¼Œå¦‚ 2025-11-20_B2_03
    """
    for idx, cluster in enumerate(cluster_list, start=1):
        cluster["_cluster_id"] = f"{date_str}_{batch_id}_{idx:02d}"
    return cluster_list

#################################èšåˆæ¯å¤©çš„è¯æç°‡åˆ†æ‰¹è¾“å‡º#################################
def aggregate_cluster_outputs(batch_outputs: List[str]) -> str:
    all_lines = []

    for batch_id, text in enumerate(batch_outputs, start=1):
        if not text:
            continue

        for line in text.strip().splitlines():
            line = line.strip()
            if not line:
                continue

            try:
                obj = json.loads(line)
            except Exception:
                # å¦‚æœæœ‰ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±è·³è¿‡ï¼ˆä¹Ÿå¯ä»¥æ”¹æˆ raiseï¼Œçœ‹ä½ éœ€æ±‚ï¼‰
                continue

            clean_line = json.dumps(obj, ensure_ascii=False)
            all_lines.append(clean_line)

    # èšåˆä¸ºä¸€ä¸ªå¤§çš„ JSONL å­—ç¬¦ä¸²
    return "\n".join(all_lines)


################top5ç­›é€‰#################################


# -------------------------------
# ğŸ”¹ 1. åŒ¹é…åŸå§‹å‘è¨€
# -------------------------------
from datetime import datetime
import json
from typing import List, Dict

import json
from datetime import datetime
from typing import List, Tuple

from datetime import datetime
from typing import List, Tuple

import re
from datetime import datetime

def parse_time_range(date_str: str, range_str: str):
    """
    ä» range_str ä¸­é²æ£’åœ°è§£æå‡ºä¸€ä¸ªæ—¶é—´æ®µï¼š
    - æ”¯æŒå‡ºç°â€œæè½´â€â€œæâ€ç­‰è„å­—ç¬¦
    - åªè®¤é‡Œé¢çš„ HH:MM:SS æ¨¡å¼
    - è§£æå¤±è´¥æ—¶è¿”å› (None, None)ï¼Œé¿å…ç›´æ¥æŠ›å¼‚å¸¸
    """
    if not range_str:
        return None, None

    # æå–æ‰€æœ‰å½¢å¦‚ 16:10:56 çš„æ—¶é—´ç‰‡æ®µ
    times = re.findall(r"\d{1,2}:\d{2}:\d{2}", str(range_str))
    if len(times) < 2:
        print(f"[parse_time_range] æ— æ³•ä» {range_str!r} æå–åˆ° 2 ä¸ªæ—¶é—´ï¼Œè·³è¿‡")
        return None, None

    start_str, end_str = times[0], times[1]

    try:
        start_dt = datetime.strptime(f"{date_str} {start_str}", "%Y-%m-%d %H:%M:%S")
        end_dt   = datetime.strptime(f"{date_str} {end_str}", "%Y-%m-%d %H:%M:%S")
        return start_dt, end_dt
    except ValueError as e:
        print(f"[parse_time_range] è§£æå¤±è´¥ï¼šdate_str={date_str!r}, range_str={range_str!r}, err={e}")
        return None, None


from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple


def match_dialogs_by_time(
    messages: List[Dict[str, Any]],
    date_str: str,
    time_axis_str: Optional[str],
) -> List[Dict[str, Any]]:
    """
    æ ¹æ® æ—¥æœŸ + æ—¶é—´è½´ï¼Œä» messages ä¸­ç­›é€‰å‡ºå¯¹åº”çš„åŸå§‹å‘è¨€ï¼š
    - messages é‡Œçš„æ—¶é—´å­—æ®µä¸ºï¼šå‘è¨€æ—¥æœŸ(YYYY-MM-DD) + å‘è¨€æ—¶é—´(HH:MM:SS)
    - time_axis_str æ”¯æŒå¤šæ®µï¼š"16:10:56-16:23:00ã€21:00:00-21:10:00"
    - å†…éƒ¨ç”¨ parse_time_range åšâ€œæè½´â€é²æ£’è§£æ
    """
    if not messages or not date_str:
        return []
    if not time_axis_str or not isinstance(time_axis_str, str) or not time_axis_str.strip():
        return []

    # è§£ææ‰€æœ‰æ—¶é—´æ®µ -> List[(start_dt, end_dt)]
    ranges: List[Tuple[datetime, datetime]] = []
    for part in str(time_axis_str).split("ã€"):
        part = part.strip()
        if not part:
            continue
        start_dt, end_dt = parse_time_range(date_str, part)
        if start_dt and end_dt:
            ranges.append((start_dt, end_dt))

    if not ranges:
        return []

    matched: List[Dict[str, Any]] = []
    seen = set()

    for msg in messages:
        if (msg.get("å‘è¨€æ—¥æœŸ") or "") != date_str:
            continue

        ts = msg.get("å‘è¨€æ—¶é—´") or ""
        try:
            msg_dt = datetime.strptime(f"{date_str} {ts}", "%Y-%m-%d %H:%M:%S")
        except Exception:
            continue

        hit = any(start <= msg_dt <= end for (start, end) in ranges)
        if not hit:
            continue

        key = (
            msg.get("_idx"),
            msg.get("å‘è¨€æ—¥æœŸ"),
            msg.get("å‘è¨€æ—¶é—´"),
            msg.get("ç©å®¶ID"),
            msg.get("ç©å®¶æ¶ˆæ¯"),
        )
        if key in seen:
            continue
        seen.add(key)
        matched.append(msg)

    return matched

from datetime import datetime
from typing import List, Tuple

def _parse_time_ranges(fayan_time: str) -> List[Tuple[datetime.time, datetime.time]]:
    """
    æŠŠ '14:00:01-14:09:52ã€21:00:00-21:10:00' è§£ææˆ List[(start_time, end_time)]
    åªè¿”å› timeï¼Œä¸è¿”å› datetimeï¼Œé¿å…å’Œä¸‹æ¸¸æ¯”è¾ƒç±»å‹ä¸ä¸€è‡´ã€‚
    """
    if not fayan_time:
        return []

    parts = [p.strip() for p in str(fayan_time).split("ã€") if p.strip()]
    ranges: List[Tuple[datetime.time, datetime.time]] = []

    for part in parts:
        # æå–æ‰€æœ‰å½¢å¦‚ 16:10:56 çš„æ—¶é—´ç‰‡æ®µï¼ˆå…¼å®¹â€œæè½´â€ç­‰è„å­—ç¬¦ï¼‰
        times = re.findall(r"\d{1,2}:\d{2}:\d{2}", part)
        if len(times) < 2:
            continue
        start_str, end_str = times[0], times[1]
        try:
            start_t = datetime.strptime(start_str, "%H:%M:%S").time()
            end_t   = datetime.strptime(end_str,   "%H:%M:%S").time()
            ranges.append((start_t, end_t))
        except Exception:
            continue

    return ranges



def get_dialogs_lines_by_fayan_time_debug(
    jsonl_lines01: list[str],
    date_str: str,
    fayan_time: str,
    debug: bool = True,
) -> list[str]:
    """
    debug=True ä¸”ç»“æœä¸ºç©ºæ—¶ï¼Œä¼šæ‰“å°ï¼š
    - ä¼ å…¥çš„ date/time_axis
    - è§£æåçš„æ—¶é—´æ®µ
    - å½“å¤©åŸæ–‡æœ€æ—©/æœ€æ™šæ—¶é—´
    - å½“å¤©åŸæ–‡æ€»æ¡æ•°
    - å‰å‡ æ¡åŸæ–‡æ—¶é—´æ ·ä¾‹
    """
    if not date_str:
        if debug:
            print("ğŸ§¯[DEBUG] date_str ä¸ºç©ºï¼Œæ— æ³•å›æº¯")
        return []

    time_ranges = _parse_time_ranges(fayan_time)
    if not time_ranges:
        if debug:
            print("ğŸ§¯[DEBUG] æ—¶é—´è½´è§£æå¤±è´¥")
            print("  date_str =", date_str)
            print("  fayan_time =", repr(fayan_time))
        return []

    # å…ˆæ‰«æå½“å¤©çš„æ‰€æœ‰æ—¶é—´ï¼Œå¾—åˆ° min/maxï¼ˆç”¨äºåˆ¤æ–­æ˜¯ä¸æ˜¯æ—¥æœŸé”™äº† or åŸæ–‡æ—¶é—´æ ¼å¼ä¸å¯¹ï¼‰
    day_times = []
    day_samples = []
    for line in jsonl_lines01:
        s = (line or "").strip()
        if not s:
            continue
        try:
            obj = json.loads(s)
        except json.JSONDecodeError:
            continue
        if (obj.get("å‘è¨€æ—¥æœŸ") or "") != date_str:
            continue
        ts = obj.get("å‘è¨€æ—¶é—´") or ""
        try:
            t = datetime.strptime(ts, "%H:%M:%S").time()
        except Exception:
            continue
        day_times.append(t)
        if len(day_samples) < 8:
            day_samples.append(ts)

    # æ­£å¼è¿‡æ»¤
    out = []
    for line in jsonl_lines01:
        s = (line or "").strip()
        if not s:
            continue
        try:
            obj = json.loads(s)
        except json.JSONDecodeError:
            continue

        if (obj.get("å‘è¨€æ—¥æœŸ") or "") != date_str:
            continue

        ts = obj.get("å‘è¨€æ—¶é—´") or ""
        try:
            t = datetime.strptime(ts, "%H:%M:%S").time()
        except Exception:
            continue

        for start_t, end_t in time_ranges:
            if start_t <= t <= end_t:
                out.append(s)
                break

    # ç»“æœä¸ºç©º -> æ‰“å°é”å®šä¿¡æ¯
    if debug and not out:
        print("\n" + "ğŸ§¯"*20)
        print("ğŸ§¯[DEBUG] æ—¶é—´æ®µå†…æ²¡æœ‰åŸæ–‡ï¼Œé”å®šä¿¡æ¯å¦‚ä¸‹ï¼š")
        print("  date_str =", date_str)
        print("  fayan_time =", fayan_time)
        print("  parsed_ranges =", [(a.strftime('%H:%M:%S'), b.strftime('%H:%M:%S')) for a,b in time_ranges])

        if day_times:
            print("  day_count =", len(day_times))
            print("  day_min =", min(day_times).strftime("%H:%M:%S"))
            print("  day_max =", max(day_times).strftime("%H:%M:%S"))
            print("  day_time_samples =", day_samples)
        else:
            print("  day_count = 0  ğŸ‘‰ è¯´æ˜ï¼šè¿™ä¸ª date_str åœ¨åŸæ–‡é‡Œæ ¹æœ¬ä¸å­˜åœ¨ï¼ˆæ—¥æœŸä¸ä¸€è‡´ï¼‰")

        # é¢å¤–ï¼šæŠŠç›®æ ‡æ—¶é—´æ®µæ‰“å°å‡ºæ¥ï¼Œæ£€æŸ¥æ˜¯å¦è¶…å‡ºå½“å¤©èŒƒå›´
        for a, b in time_ranges:
            if day_times and (b < min(day_times) or a > max(day_times)):
                print("  âš  ç›®æ ‡æ—¶é—´æ®µå®Œå…¨è½åœ¨å½“å¤©åŸæ–‡èŒƒå›´ä¹‹å¤–ï¼ˆå¾ˆå¯èƒ½ï¼šæ—¥æœŸé”™ or æ—¶é—´è½´é”™ï¼‰")
                break

        print("ğŸ§¯"*20 + "\n")

    return out


def get_time_axis(cluster: dict) -> str | None:
    # ä¼˜å…ˆæ ‡å‡†å­—æ®µ
    for key in ("æ—¶é—´è½´", "timeè½´", "æ—¶é—´è½´ ", "æ—¶é—´æ®µ", "æ—¶é—´åŒºé—´"):
        val = cluster.get(key)
        if isinstance(val, str) and val.strip():
            return val.strip()
    return None

def extract_cluster_stats(èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict], åŸå§‹å‘è¨€: List[str]) -> List[Dict]:
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    results = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = get_time_axis(cluster)
        if not date or not time_axis:
            print(f"âš  èšåˆè¯é¢˜ç°‡ç¼ºå°‘æ—¥æœŸæˆ–æ—¶é—´è½´ï¼Œè·³è¿‡ï¼š{cluster}")
            continue
        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)
        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        result = {
            "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡"),
            "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
            "å‘è¨€ç©å®¶æ€»æ•°": len(players),
            "å‘è¨€æ€»æ•°": len(matched)
        }
        results.append(result)

    return results

# -------------------------------
# ğŸ”¹ 2. çƒ­åº¦è¯†åˆ«ä¸»å‡½æ•°
# -------------------------------
def compute_heat_score(U: int, M: int) -> float:
    if U == 0 or M == 0:
        return 0.0
    return round(U * math.sqrt(M), 2)

def extract_top5_heat_clusters(èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict], åŸå§‹å‘è¨€: List[str], top_k=5) -> List[Dict]:
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    enriched = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = cluster.get("æ—¶é—´è½´")
        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)

        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        U = len(players)
        M = len(matched)
        heat = compute_heat_score(U, M)

        enriched.append({
        "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥",
        "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
        "æ—¥æœŸ": date,
        "æ—¶é—´è½´": time_axis,
        "å‘è¨€ç©å®¶æ€»æ•°": U,
        "å‘è¨€æ€»æ•°": M,
        "çƒ­åº¦è¯„åˆ†": heat
         })


    # æŒ‰çƒ­åº¦è¯„åˆ†æ’åºï¼Œå– TopK
    enriched.sort(key=lambda x: x["çƒ­åº¦è¯„åˆ†"], reverse=True)
    return enriched[:top_k]

# -------------------------------
# ğŸ”¹ 3. æ·»åŠ è®¨è®ºç‚¹å­—æ®µ
# -------------------------------
def attach_discussion_points_day(top_clusters: List[Dict], subclusters: List[Dict]) -> List[Dict]:
    """
    å°† top_clusters ä¸­æ¯ä¸ªèšåˆç°‡çš„å­è¯é¢˜ç°‡åˆ—è¡¨ï¼Œä¸ subclusters ä¸­çš„ _cluster_id åŒ¹é…ï¼Œ
    æ‹¼å‡ºæ ¸å¿ƒæœºåˆ¶æè¿°ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºåˆ—ç‚¹ + ç©ºè¡Œã€‚ç§»é™¤å­è¯é¢˜ç°‡åˆ—è¡¨å­—æ®µã€‚
    """
    # æ„å»º _cluster_id â†’ æ ¸å¿ƒæœºåˆ¶ æ˜ å°„
    cluster_mechanism_map = {
        row["_cluster_id"]: row["æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶"]
        for row in subclusters
        if "_cluster_id" in row and "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶" in row
    }

    result = []
    for cluster in top_clusters:
        ids = cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨", [])
        mechanisms = [cluster_mechanism_map.get(cid) for cid in ids if cid in cluster_mechanism_map]

        # æ ¼å¼åŒ–ä¸ºåˆ—ç‚¹ + ç©ºè¡Œ
        discussion_point = [m for m in mechanisms if m]

        # ä¿®å¤èšåˆè¯é¢˜ç°‡åç§°å­—æ®µä¸ºç©ºçš„é—®é¢˜
        cluster_name = cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥"

        enriched_cluster = {
            "èšåˆè¯é¢˜ç°‡": cluster_name,
            "æ—¥æœŸ": cluster.get("æ—¥æœŸ"),
            "æ—¶é—´è½´": cluster.get("æ—¶é—´è½´"),
            "å‘è¨€ç©å®¶æ€»æ•°": cluster.get("å‘è¨€ç©å®¶æ€»æ•°"),
            "å‘è¨€æ€»æ•°": cluster.get("å‘è¨€æ€»æ•°"),
            "çƒ­åº¦è¯„åˆ†": cluster.get("çƒ­åº¦è¯„åˆ†"),
            "è®¨è®ºç‚¹": discussion_point
        }

        result.append(enriched_cluster)

    return result


def attach_discussion_points_all(
    top_clusters: List[Dict[str, Any]],
    subclusters: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    å¯¹æ¯ä¸ªèšåˆç°‡ï¼š
    - æ ¹æ® å­è¯é¢˜ç°‡åˆ—è¡¨(_cluster_id) æ‰¾å›å­è¯é¢˜ç°‡
    - ç”¨ å­è¯é¢˜ç°‡["æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶"]ï¼ˆæˆ–["è®¨è®ºç‚¹"]ï¼‰ ä½œä¸ºè®¨è®ºç‚¹æ–‡æœ¬
    - åªè¾“å‡ºï¼šè®¨è®ºç‚¹ + æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨ + å­è¯é¢˜ç°‡åˆ—è¡¨
    ä¸åš TopKï¼Œä¸åšçƒ­åº¦ï¼Œä¸æ’åº
    """

    # _cluster_id -> å­è¯é¢˜ç°‡row
    sub_by_id: Dict[str, Dict[str, Any]] = {}
    for row in subclusters:
        cid = row.get("_cluster_id")
        if cid:
            sub_by_id[str(cid)] = row

    result: List[Dict[str, Any]] = []

    for cluster in top_clusters:
        ids = cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨", []) or []

        # ç”¨ point_text èšåˆï¼šåŒä¸€ä¸ªè®¨è®ºç‚¹å¯èƒ½å¯¹åº”å¤šä¸ªå­ç°‡
        point_bucket: Dict[str, Dict[str, Any]] = {}

        for cid in ids:
            sc = sub_by_id.get(str(cid))
            if not sc:
                continue

            point_text = (sc.get("æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶") or sc.get("è®¨è®ºç‚¹") or "").strip()
            if not point_text:
                continue

            d = sc.get("æ—¥æœŸ") or cluster.get("æ—¥æœŸ")
            t_axis = sc.get("æ—¶é—´è½´") or sc.get("æ—¶é—´èŒƒå›´") or sc.get("æ—¥æœŸæ—¶é—´è½´") or ""

            if point_text not in point_bucket:
                point_bucket[point_text] = {
                    "è®¨è®ºç‚¹": point_text,
                    "æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨": [],
                    "å­è¯é¢˜ç°‡åˆ—è¡¨": [],
                }

            point_bucket[point_text]["å­è¯é¢˜ç°‡åˆ—è¡¨"].append(str(cid))
            point_bucket[point_text]["æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨"].append({
                "æ—¥æœŸ": d,
                "æ—¶é—´è½´": t_axis
            })

        # ä¸æ’åºã€ä¸æˆªæ–­ï¼šå…¨éƒ¨è¿”å›
        points = list(point_bucket.values())

        cluster_name = cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥"

        enriched_cluster = {
            "èšåˆè¯é¢˜ç°‡": cluster_name,
            "æ—¥æœŸ": cluster.get("æ—¥æœŸ"),
            "æ—¶é—´è½´": cluster.get("æ—¶é—´è½´"),
            "å‘è¨€ç©å®¶æ€»æ•°": cluster.get("å‘è¨€ç©å®¶æ€»æ•°"),
            "å‘è¨€æ€»æ•°": cluster.get("å‘è¨€æ€»æ•°"),
            "çƒ­åº¦è¯„åˆ†": cluster.get("çƒ­åº¦è¯„åˆ†"),
            "è®¨è®ºç‚¹": points,
        }

        result.append(enriched_cluster)

    return result

####################### å­˜å…¥æ¯æ—¥å‘è¨€ top5 ########################


def append_daily_top5_to_version_jsonl(
    final_result: List[Dict],
    version_jsonl_path: str = "version_daily_top5_with_opinion.jsonl",
):
    """
    å°†å½“æ—¥ Top5 è¿½åŠ å†™å…¥æŸä¸ªã€ç‰ˆæœ¬ç´¯è®¡ jsonl æ–‡ä»¶ã€‘ä¸­ã€‚
    
    åŒæ—¶ä¸ºæ¯æ¡è®°å½•è¡¥å……ä¸¤ä¸ªå­—æ®µï¼š
    1ï¼‰_idxï¼šåœ¨ version_jsonl_path ä¸­çš„å…¨å±€é€’å¢è¡Œå·ï¼ˆä»1å¼€å§‹ï¼‰
    2ï¼‰_daily_top_idï¼šå½“å¤©å†…çš„ Top è¯é¢˜ç°‡IDï¼Œå½¢å¼ï¼šYYYY-MM-DD_TXXï¼Œä¾‹å¦‚ 2025-12-03_T01
    
    è¯´æ˜ï¼š
    - å‡è®¾ final_result å†…æ‰€æœ‰è®°å½•çš„ "æ—¥æœŸ" ç›¸åŒï¼ˆå³åŒä¸€å¤©çš„ top5ï¼‰
    - å¦‚æœæ–‡ä»¶ä¸­å·²å­˜åœ¨åŒä¸€æ—¥æœŸçš„æ•°æ®ï¼Œä¼šåœ¨åŸæœ‰åŸºç¡€ä¸Šç»§ç»­ç´¯åŠ  _daily_top_id çš„ç¼–å·
    - ä¸ä¼šä¿®æ”¹ä»»ä½•å·²æœ‰çš„ `_cluster_id` å­—æ®µï¼ˆå­è¯é¢˜ç°‡ä»ç„¶ç”¨å®ƒï¼‰
    """
    if not final_result:
        print("âš  final_result ä¸ºç©ºï¼Œä»Šæ—¥æ—  Top5 å¯å†™å…¥ã€‚")
        return

    # å–å½“å¤©æ—¥æœŸï¼ˆå‡è®¾ final_result åŒä¸€æ‰¹éƒ½æ˜¯åŒä¸€å¤©ï¼‰
    date_str = final_result[0].get("æ—¥æœŸ")
    if not date_str:
        raise ValueError("final_result ä¸­ç¼ºå°‘ 'æ—¥æœŸ' å­—æ®µï¼Œæ— æ³•ç”Ÿæˆ _daily_top_idã€‚")

    path = Path(version_jsonl_path)

    # ---------- 1. è®¡ç®—å½“å‰æ–‡ä»¶ä¸­çš„æœ€å¤§ _idxï¼ˆå…¨å±€è¡Œå·ï¼‰ ----------
    global_max_idx = 0
    # åŒæ—¶ç»Ÿè®¡â€œä»Šå¤©â€å·²ç»æœ‰å¤šå°‘æ¡ï¼Œç”¨äº _daily_top_id ç»­ç¼–å·
    existing_count_for_day = 0

    if path.exists():
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError:
                    continue

                # ç»Ÿè®¡å…¨å±€ _idx
                if isinstance(obj.get("_idx"), int):
                    global_max_idx = max(global_max_idx, obj["_idx"])
                else:
                    # æ—§æ•°æ®æ²¡æœ‰ _idxï¼Œå°±ç®€å•å½“ä½œä¸€è¡Œï¼Œæ–°è¡Œå·å¾€åæ¨
                    global_max_idx += 1

                # ç»Ÿè®¡å½“å¤©çš„æ¡æ•°ï¼ˆç”¨äº _daily_top_idï¼‰
                if obj.get("æ—¥æœŸ") == date_str:
                    existing_count_for_day += 1

    # ---------- 2. è¿½åŠ å†™å…¥ï¼Œå¹¶ä¸ºæ–°è®°å½•ç”Ÿæˆ _idx + _daily_top_id ----------
    mode = "a" if path.exists() else "w"
    with path.open(mode, encoding="utf-8") as f:
        for offset, row in enumerate(final_result, start=1):
            row = dict(row)  # é¿å…ä¿®æ”¹åŸå¯¹è±¡å¼•ç”¨

            # 2.1 å…¨å±€é€’å¢ _idxï¼ˆç‰ˆæœ¬æ‰€æœ‰å¤©å…±ç”¨ä¸€ä¸ªåºå·ï¼‰
            global_max_idx += 1
            row["_idx"] = global_max_idx

            # 2.2 ç”Ÿæˆå½“å¤©çš„ top idï¼ˆä¸ç¢° `_cluster_id`ï¼‰
            # å½“å¤©å·²æœ‰ existing_count_for_day æ¡ï¼Œè¿™æ‰¹ä» +1 å¼€å§‹æ¥ç€æ’
            idx_for_day = existing_count_for_day + offset
            row["_daily_top_id"] = f"{date_str}_T{idx_for_day:02d}"

            f.write(json.dumps(row, ensure_ascii=False) + "\n")

    print(f"âœ… å·²å°†å½“æ—¥ Top5ï¼ˆå« _idx å’Œ _daily_top_idï¼‰è¿½åŠ å†™å…¥: {path}")


#########åŒ¹é…final+output ==>è®¨è®ºç‚¹+æ—¶é—´è½´################
import json
import re
from typing import List, Dict, Any

def parse_jsonl_text(text: str) -> List[Dict[str, Any]]:
    """
    è§£ææ¨¡å‹è¾“å‡ºçš„â€œä¼ª jsonlâ€ï¼Œå°½é‡ä»ä¸­æå–å‡ºè‹¥å¹²åˆæ³•çš„ dictã€‚
    - æ”¯æŒå¼€å¤´/ç»“å°¾æœ‰ ```json ä»£ç å—åŒ…è£¹
    - è·³è¿‡é { å¼€å¤´çš„è¡Œ
    - è‡ªåŠ¨å»æ‰è¡Œå°¾çš„é€—å·
    - è§£æå¤±è´¥æ—¶æ‰“å° warningï¼Œä¸ä¸­æ–­
    """
    if not text:
        return []

    s = text.strip()

    # å»æ‰ ```json / ``` åŒ…è£¹
    if s.startswith("```"):
        s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
        s = re.sub(r"\s*```$", "", s)

    results: List[Dict[str, Any]] = []

    for raw in s.splitlines():
        line = raw.strip()
        if not line:
            continue

        # æœ‰äº›æ¨¡å‹ä¼šè¾“å‡º "- {...}" ä¹‹ç±»çš„ markdown åˆ—è¡¨
        if line.startswith("- "):
            line = line[2:].lstrip()

        # è·³è¿‡æ˜æ˜¾ä¸æ˜¯ JSON å¯¹è±¡çš„è¡Œ
        if not line.startswith("{"):
            continue

        # å»æ‰æœ«å°¾å¤šä½™çš„é€—å·
        if line.endswith(","):
            line = line[:-1].rstrip()

        try:
            obj = json.loads(line)
        except json.JSONDecodeError as e:
            print(f"âš  è§£æ JSON è¡Œå¤±è´¥ï¼š{e} | è¡Œå†…å®¹å‰ 120 å­—ç¬¦ï¼š{line[:120]}")
            continue

        if isinstance(obj, dict):
            results.append(obj)
        else:
            print(f"âš  è§£æç»“æœä¸æ˜¯ dictï¼Œå·²è·³è¿‡ï¼š{type(obj)}")

    return results



def print_mech_time_from_top5(top5_results: list[dict], output_cluster_jsonl: str):
    output_clusters = parse_jsonl_text(output_cluster_jsonl)

    # æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶ -> å‘è¨€æ—¶é—´
    mech_time = {}
    for r in output_clusters:
        mech = (r.get("æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶") or "").strip()
        t = (r.get("å‘è¨€æ—¶é—´") or "").strip()
        if mech and mech not in mech_time:
            mech_time[mech] = t

    rows = []
    miss = 0

    for top in top5_results:
        for dp in (top.get("è®¨è®ºç‚¹") or []):
            mech = (dp or "").strip()
            if not mech:
                continue

            row = {
                "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": mech,
                "å‘è¨€æ—¶é—´": mech_time.get(mech, "")
            }
            rows.append(row)

            # æ‰“å° jsonl è¡Œ
            #print(json.dumps(row, ensure_ascii=False))

            if not row["å‘è¨€æ—¶é—´"]:
                miss += 1

    # é¢å¤–ç»™ä½ ä¸€ä¸ªå°æç¤ºï¼Œæ–¹ä¾¿ç¡®è®¤æœ‰æ²¡æœ‰æ¼åŒ¹é…
    if miss:
        print(f"[WARN] æœ‰ {miss} æ¡è®¨è®ºç‚¹æœªåŒ¹é…åˆ°å‘è¨€æ—¶é—´ï¼ˆè¯·æ£€æŸ¥æ˜¯å¦å®Œå…¨ä¸€è‡´ï¼‰")

    return rows

######################é”å®šæ—¶é—´è¿˜åŸåŸæ–‡#############

def parse_fayan_time_range_str(fayan_time: str):
    """
    è¾“å…¥:  '2025-12-04 14:00:01-14:09:52'
    è¾“å‡º:  ('2025-12-04', '14:00:01', '14:09:52')
    """
    if not fayan_time:
        return None
    s = str(fayan_time).strip()
    m = re.search(r"(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2}:\d{2})\s*-\s*(\d{2}:\d{2}:\d{2})", s)
    if not m:
        return None
    return m.group(1), m.group(2), m.group(3)

def get_dialogs_lines_by_fayan_time(jsonl_lines01: list[str], fayan_time: str) -> list[str]:
    """
    ä»åŸå§‹ jsonl_lines01 (list[str]) é‡Œç­›å‡ºè½åœ¨å‘è¨€æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰åŸæ–‡è¡Œï¼ˆä»ç„¶è¿”å› list[str]ï¼‰ã€‚
    """
    parsed = parse_fayan_time_range_str(fayan_time)
    if not parsed:
        return []
    date_str, start_str, end_str = parsed

    start_t = datetime.strptime(start_str, "%H:%M:%S").time()
    end_t   = datetime.strptime(end_str, "%H:%M:%S").time()

    out = []
    for line in jsonl_lines01:
        s = (line or "").strip()
        if not s:
            continue
        try:
            obj = json.loads(s)
        except json.JSONDecodeError:
            continue

        if (obj.get("å‘è¨€æ—¥æœŸ") or "") != date_str:
            continue

        ts = obj.get("å‘è¨€æ—¶é—´") or ""
        try:
            t = datetime.strptime(ts, "%H:%M:%S").time()
        except Exception:
            continue

        if start_t <= t <= end_t:
            out.append(s)  # âœ… ä¿æŒåŸæ ·ï¼šä¸€æ¡jsonå­—ç¬¦ä¸²
    return out

#------------------------æ¨¡å‹4è¾“å‡ºæ‹†åˆ†---------------------------------

def parse_opinion_output_to_list(opinion_output: str) -> List[Dict[str, Any]]:
    """
    æŠŠæ¨¡å‹4è¿”å›çš„å­—ç¬¦ä¸²è§£ææˆ List[dict]ï¼š
    - æ”¯æŒï¼š
      * å•ä¸ª JSON å¯¹è±¡
      * JSON æ•°ç»„
      * jsonlï¼ˆå¤šè¡Œï¼Œæ¯è¡Œä¸€ä¸ª JSONï¼‰
      * ```json ä»£ç å— + å¤šè¡Œæ¼‚äº® JSON
    """
    if not opinion_output:
        return []

    s = opinion_output.strip()

    # 0) å»æ‰ ```json / ``` å¤–å£³
    s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
    s = re.sub(r"\s*```$", "", s)
    s = s.strip()

    # 1) å…ˆå°è¯•æ•´ä½“è§£æï¼ˆèƒ½åƒæ‰å•å¯¹è±¡ / æ•°ç»„ï¼‰
    try:
        obj = json.loads(s)
        if isinstance(obj, dict):
            return [obj]
        if isinstance(obj, list):
            return [x for x in obj if isinstance(x, dict)]
    except json.JSONDecodeError:
        pass

    # 2) ç”¨â€œæ‹¬å·æ·±åº¦â€æ–¹å¼ï¼Œä»å¤šè¡Œé‡Œæ‹¼å‡ºä¸€ä¸ªæˆ–å¤šä¸ª JSON å¯¹è±¡
    objs: List[Dict[str, Any]] = []
    buf: List[str] = []
    depth = 0

    for raw in s.splitlines():
        line = (raw or "").strip()
        if not line:
            continue

        # å»æ‰ markdown åˆ—è¡¨å‰ç¼€ï¼Œå¦‚ "- { ... }"
        if line.startswith("- "):
            line = line[2:].lstrip()

        # å¦‚æœè¿™ä¸€è¡Œå®Œå…¨ä¸å« { æˆ– }ï¼Œä¸”å½“å‰æ·±åº¦ä¸º 0ï¼ŒåŸºæœ¬æ˜¯è§£é‡Šæ–‡å­—ï¼Œè·³è¿‡
        if depth == 0 and "{" not in line:
            continue

        # è¿›å…¥/ç»§ç»­ä¸€ä¸ªå¯¹è±¡
        open_cnt = line.count("{")
        close_cnt = line.count("}")

        if depth == 0 and "{" in line:
            # æ–°å¯¹è±¡çš„å¼€å§‹
            buf = [line]
            depth = open_cnt - close_cnt
            if depth <= 0:
                # å•è¡Œå¯¹è±¡ï¼š{...}
                text = "\n".join(buf)
                try:
                    obj = json.loads(text)
                    if isinstance(obj, dict):
                        objs.append(obj)
                except json.JSONDecodeError:
                    pass
                buf = []
                depth = 0
        else:
            # å·²ç»åœ¨å¯¹è±¡å†…éƒ¨
            buf.append(line)
            depth += open_cnt - close_cnt
            if depth <= 0:
                # å¯¹è±¡ç»“æŸ
                text = "\n".join(buf)
                try:
                    obj = json.loads(text)
                    if isinstance(obj, dict):
                        objs.append(obj)
                except json.JSONDecodeError:
                    pass
                buf = []
                depth = 0

    return objs

######################è§‚ç‚¹å›æº¯è‡³top5åšæœ€åè¾“å‡º##############
def _norm_text(s: str) -> str:
    """æ–‡æœ¬å½’ä¸€åŒ–ï¼šå»ç©ºæ ¼å’Œå¸¸è§æ ‡ç‚¹ï¼Œç”¨äºå®½æ¾åŒ¹é…è®¨è®ºç‚¹."""
    if not s:
        return ""
    s = str(s)
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[ï¼Œ,ã€‚\.ï¼ï¼Ÿ!ï¼š:ã€ï¼›;ï¼ˆï¼‰()\[\]ã€ã€‘\"'â€œâ€â€˜â€™]", "", s)
    return s


def merge_top5_with_opinions_numbered(
    top5_results: List[Dict[str, Any]],
    opinions: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:

    # 1) opinions -> multimapï¼šå½’ä¸€åŒ–è®¨è®ºç‚¹ => [op1, op2, ...]
    op_map: Dict[str, List[Dict[str, Any]]] = defaultdict(list)
    for op in opinions:
        dp = (op.get("è®¨è®ºç‚¹") or "").strip()
        if not dp:
            continue
        op_map[_norm_text(dp)].append(op)

    merged: List[Dict[str, Any]] = []
    missing = 0
    hit = 0

    # 2) åˆå¹¶å› top5ï¼ˆé€ä¸ªæ¶ˆè´¹ listï¼Œé¿å…è¦†ç›–ä¸¢å¤±ï¼‰
    for row in top5_results:
        dps = row.get("è®¨è®ºç‚¹") or []
        if isinstance(dps, str):
            dps = [dps]

        discussion_list: List[Dict[str, Any]] = []

        for idx, dp in enumerate(dps, start=1):
            raw_dp = (dp or "").strip()
            if not raw_dp:
                continue

            k = _norm_text(raw_dp)
            numbered_key = f"è®¨è®ºç‚¹{idx}"

            if not op_map.get(k):
                discussion_list.append({
                    numbered_key: raw_dp,
                    "ç©å®¶è§‚ç‚¹": [],
                    "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": [],
                    "_missing_opinion": True,   # è°ƒè¯•ç”¨
                })
                missing += 1
                continue

            # âœ… å…³é”®ï¼šæŒ‰é¡ºåºå–å‡ºå¹¶æ¶ˆè´¹ï¼Œé˜²æ­¢åŒåè¦†ç›–
            op = op_map[k].pop(0)
            hit += 1

            examples = op.get("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹") or []
            if not isinstance(examples, list):
                examples = [str(examples)]

            viewpoints = op.get("ç©å®¶è§‚ç‚¹") or []
            if not isinstance(viewpoints, list):
                viewpoints = [str(viewpoints)]

            discussion_list.append({
                numbered_key: raw_dp,
                "ç©å®¶è§‚ç‚¹": viewpoints,
                "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": examples,
            })

        new_row = dict(row)
        new_row.pop("è®¨è®ºç‚¹", None)
        new_row.pop("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹", None)
        new_row["è®¨è®ºç‚¹åˆ—è¡¨"] = discussion_list
        merged.append(new_row)

    # 3) å¯é€‰ï¼šçœ‹æœ‰æ²¡æœ‰ opinion æ²¡è¢«ç”¨æ‰ï¼ˆä¸€èˆ¬ä¹Ÿèƒ½å¸®åŠ©ç¡®è®¤æ˜¯å¦ key å†²çªï¼‰
    leftover = sum(len(v) for v in op_map.values())
    print(f"âœ… merge å‘½ä¸­={hit}, missing={missing}, leftover_unused_opinions={leftover}")

    return merged


#####æ—¶é—´è½´æ ¡æ­£###
import re
from typing import Any, Dict, List, Optional, Tuple

def _pick_time_axis_value(c: Dict[str, Any]) -> Tuple[Optional[str], Optional[str]]:
    """
    åœ¨ dict c ä¸­æŒ‘ä¸€ä¸ªæœ€åƒâ€œæ—¶é—´è½´â€çš„å­—æ®µå€¼ã€‚
    è¿”å› (value, source_key)
    """
    # å€™é€‰ key çš„ä¼˜å…ˆçº§ï¼šè¶Šé å‰è¶Šåƒæ—¶é—´è½´
    priority_patterns = [
        r"^æ—¶é—´è½´$", r"æ—¶é—´è½´",
        r"æ—¶é—´æ®µ", r"æ—¶é—´èŒƒå›´", r"æ—¶æ®µ", r"èŒƒå›´",
        r"time[_\s-]*axis", r"axis",
        r"æè½´", r"æ—¶é—´æ"
    ]

    # 1) å…ˆæŒ‰ä¼˜å…ˆçº§æ‰¾ key
    keys = list(c.keys())
    for pat in priority_patterns:
        for k in keys:
            if re.search(pat, str(k), flags=re.IGNORECASE):
                v = c.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip(), str(k)
                # æœ‰äº›æ¨¡å‹ä¼šç»™ list[str]ï¼Œæ‹¼ä¸€ä¸‹
                if isinstance(v, list) and v:
                    s = "ã€".join([str(x).strip() for x in v if str(x).strip()])
                    if s:
                        return s, str(k)

    # 2) å†æ‰«æ‰€æœ‰ keyï¼šåªè¦ key å«â€œæ—¶é—´/æ—¶æ®µ/èŒƒå›´/time/axis/æè½´â€
    for k in keys:
        kk = str(k)
        if any(x in kk.lower() for x in ["æ—¶é—´", "æ—¶æ®µ", "èŒƒå›´", "time", "axis", "æè½´"]):
            v = c.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip(), str(k)
            if isinstance(v, list) and v:
                s = "ã€".join([str(x).strip() for x in v if str(x).strip()])
                if s:
                    return s, str(k)

    return None, None


def ensure_time_axis_key(c: Dict[str, Any], verbose: bool = True) -> bool:
    """
    å¦‚æœ c ä¸­æ²¡æœ‰ key == 'æ—¶é—´è½´' æˆ–å…¶å€¼ä¸ºç©ºï¼Œå°±ç”¨åˆ«åå­—æ®µè¦†ç›–ç”Ÿæˆ c['æ—¶é—´è½´']ã€‚
    è¿”å›ï¼šæ˜¯å¦å‘ç”Ÿäº†ä¿®å¤ï¼ˆTrue/Falseï¼‰
    """
    # å·²ç»æœ‰æ—¶é—´è½´ä¸”éç©º -> ä¸åŠ¨
    ta = c.get("æ—¶é—´è½´")
    if isinstance(ta, str) and ta.strip():
        return False

    # æ²¡æœ‰â€œæ—¶é—´è½´â€ä¸‰ä¸ªå­—çš„ keyï¼ˆä½ è¦æ±‚ï¼šåªè¦æ²¡å‡ºç°æ—¶é—´è½´ä¸‰ä¸ªå­—å°±è¦†ç›–ï¼‰
    has_time_axis_key = any("æ—¶é—´è½´" in str(k) for k in c.keys())
    if not has_time_axis_key:
        v, src = _pick_time_axis_value(c)
        if v:
            c["æ—¶é—´è½´"] = v
            c["_time_axis_from"] = src  # è®°å½•æ¥æºï¼Œæ–¹ä¾¿æ’æŸ¥
            if verbose:
                print(f"âœ… å·²è¡¥é½ æ—¶é—´è½´: {v}  (from: {src}) | èšåˆè¯é¢˜ç°‡={c.get('èšåˆè¯é¢˜ç°‡')}")
            return True
        else:
            if verbose:
                print(f"âš  æ‰¾ä¸åˆ°å¯ç”¨æ—¶é—´å­—æ®µæ¥è¡¥é½ æ—¶é—´è½´ | èšåˆè¯é¢˜ç°‡={c.get('èšåˆè¯é¢˜ç°‡')} keys={list(c.keys())}")
            return False

    # æœ‰ key å«â€œæ—¶é—´è½´â€ä½†å€¼ä¸ºç©ºï¼šä¹Ÿå°è¯•ç”¨åˆ«åè¡¥é½
    v, src = _pick_time_axis_value(c)
    if v:
        c["æ—¶é—´è½´"] = v
        c["_time_axis_from"] = src
        if verbose:
            print(f"âœ… å·²ä¿®å¤ ç©ºæ—¶é—´è½´: {v} (from: {src}) | èšåˆè¯é¢˜ç°‡={c.get('èšåˆè¯é¢˜ç°‡')}")
        return True

    if verbose:
        print(f"âš  æœ‰æ—¶é—´è½´keyä½†æ— æ³•ä¿®å¤ï¼ˆæ— å¯ç”¨å€¼ï¼‰| èšåˆè¯é¢˜ç°‡={c.get('èšåˆè¯é¢˜ç°‡')}")
    return False

def ensure_subcluster_list_key(c: Dict[str, Any]) -> bool:
    if isinstance(c.get("å­è¯é¢˜ç°‡åˆ—è¡¨"), list) and c["å­è¯é¢˜ç°‡åˆ—è¡¨"]:
        return False

    candidates = ["å­è¯é¢˜ç°‡åˆ—è¡¨", "å­è¯æç°‡åˆ—è¡¨", "å­è¯é¢˜ç°‡IDåˆ—è¡¨", "å­ç°‡åˆ—è¡¨", "å­è¯é¢˜ç°‡"]
    for k in candidates:
        v = c.get(k)
        if v is None:
            continue
        if isinstance(v, list):
            c["å­è¯é¢˜ç°‡åˆ—è¡¨"] = v
            c["_sub_list_from"] = k
            return True
        if isinstance(v, str) and v.strip():
            c["å­è¯é¢˜ç°‡åˆ—è¡¨"] = [x.strip() for x in v.split("ã€") if x.strip()]
            c["_sub_list_from"] = k
            return True
    return False
#######æŒ‰ _idx å›åŸæ–‡ç®—çœŸå®æ—¶é—´è½´ / å–åŸæ–‡è¡Œ#######
from datetime import datetime
import json
import re

def extract_idx_list_from_cluster_obj(c: dict) -> list[int]:
    v = c.get("å‘è¨€è¡Œå·åˆ—è¡¨")
    if v is None:
        return []
    if isinstance(v, list):
        out = []
        for x in v:
            try:
                out.append(int(x))
            except:
                pass
        return out
    if isinstance(v, str):
        nums = re.findall(r"\d+", v)
        return [int(n) for n in nums]
    return []

def calc_fayan_time_by_idx(jsonl_lines01_idx: list[str], idx_list: list[int]) -> str:
    """
    ç”¨ idx_list å›åŸæ–‡ç®—çœŸå® min/max -> 'YYYY-MM-DD HH:MM:SS-HH:MM:SS'
    """
    if not idx_list:
        return ""
    idx_set = set(idx_list)

    dts = []
    for line in jsonl_lines01_idx:
        try:
            obj = json.loads(line)
        except:
            continue
        if obj.get("_idx") not in idx_set:
            continue

        d = (obj.get("å‘è¨€æ—¥æœŸ") or "").strip()
        t = (obj.get("å‘è¨€æ—¶é—´") or "").strip()
        if not d or not t:
            continue
        try:
            dt = datetime.strptime(f"{d} {t}", "%Y-%m-%d %H:%M:%S")
        except:
            continue
        dts.append(dt)

    if not dts:
        return ""

    dts.sort()
    date_str = dts[0].strftime("%Y-%m-%d")
    return f"{date_str} {dts[0].strftime('%H:%M:%S')}-{dts[-1].strftime('%H:%M:%S')}"


def refill_cluster_fayan_time(cluster_json_list: list[dict], jsonl_lines01_idx: list[str]) -> int:
    ok = 0
    for c in cluster_json_list:
        idxs = extract_idx_list_from_cluster_obj(c)
        axis = calc_fayan_time_by_idx(jsonl_lines01_idx, idxs)
        c["å‘è¨€æ—¶é—´"] = axis  # âœ… å†™å›ï¼šåé¢é“¾è·¯ä»ç„¶æŒ‰â€œå‘è¨€æ—¶é—´â€è·‘
        c["_å‘è¨€æ—¶é—´æ¥æº"] = "idx_minmax" if axis else "idx_empty"
        ok += 1 if axis else 0
    return ok
