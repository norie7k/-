
from __future__ import annotations
import json, time, typing as T
import pandas as pd
import requests
import math
from pathlib import Path
from typing import List, Dict, Any,Optional,Union,Tuple
import re, json, unicodedata
from datetime import datetime
import json
from json import JSONDecodeError
# --- openpyxl æ ·å¼/å·¥å…· ---
from openpyxl import Workbook, load_workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, NamedStyle

# --- docx æ ·å¼/å·¥å…· ---
from docx.oxml import OxmlElement
from docx import Document
from docx.shared import Pt
from docx.oxml.ns import qn


################æ¨¡å‹è°ƒç”¨ï¼Œå‡ºç»“æœ###################

def load_system_prompt(path: Path) -> str:
    if not path.exists():
        raise FileNotFoundError(f"Prompt file not found: {path}")
    return path.read_text(encoding="utf-8")

def build_user_prompt_filter(json_lines: T.List[str]) -> str:
    # æ¨¡å‹#1ï¼šç­›æ‰éæ¸¸æˆç›¸å…³ï¼Œåªè¾“å‡ºç›¸å…³ JSON è¡Œï¼ˆåŸæ ·ï¼‰
    return (
        "ä»¥ä¸‹æ˜¯è‹¥å¹²ç©å®¶/å®¢æœ/ç ”å‘çš„å‘è¨€è®°å½•ï¼Œè¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­è§„åˆ™ï¼Œ"
        "åˆ¤æ–­å“ªäº›æ˜¯ã€ä¸æ¸¸æˆå†…å®¹ç›¸å…³ã€‘çš„å‘è¨€ï¼Œä¿ç•™è¿™äº› JSON è¡Œï¼Œä¸ç›¸å…³çš„å¿½ç•¥ã€‚"
        "è¯·ä»…è¾“å‡ºã€ç›¸å…³å‘è¨€çš„åŸå§‹ JSON è¡Œã€‘ï¼Œä¸¥æ ¼ä¿æŒæ ¼å¼ä¸å˜ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + "\n".join(json_lines)
    )

def build_user_prompt_clsuter(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )

def build_user_prompt_cluster_agg(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
        
    )



from typing import List, Any

def build_user_prompt_subcluster_opinion(
    discussion_point: str,
    json_lines: List[Any],   # å®é™…ä¼ çš„æ˜¯ list[str]
) -> str:
    dp = (discussion_point or "").strip()
    jsonl_block = "\n".join(
        (line or "").strip() for line in json_lines if line
    )
    return (
        "ä½ å°†æ”¶åˆ°ä¸€æ®µç©å®¶èŠå¤©åŸæ–‡ï¼ˆJSONLï¼Œæ¯è¡Œä¸€æ¡ï¼‰ã€‚æœ¬æ¬¡åªåˆ†ææŒ‡å®šã€è®¨è®ºç‚¹ã€‘ï¼Œå…¶ä»–å†…å®¹å¿½ç•¥ã€‚\n\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡º1ä¸ªJSONå¯¹è±¡ï¼Œç¦æ­¢è§£é‡Šæ–‡å­—ï¼Œç¦æ­¢Markdownä»£ç å—ã€‚\n\n"
        "ã€è¾“å…¥è¯é¢˜ç‚¹ã€‘ï¼š\n" + dp + "\n\n"
        "ã€è¾“å…¥åŸæ–‡JSONLã€‘\n" + jsonl_block
    )

def call_ark_chat_completions(
    api_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    temperature: float = 0.3,
    max_tokens: int = 32700,
    timeout: int = 600,
    retries: int = 2,
) -> str:
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    last_err = None
    for attempt in range(retries + 1):
        try:
            resp = requests.post(api_url, headers=headers, json=payload, timeout=timeout)
            if resp.status_code != 200:
                raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            last_err = e
            time.sleep(1.2 * (attempt + 1))
    raise RuntimeError(f"Ark API è°ƒç”¨å¤±è´¥: {last_err}")

def extract_valid_json_lines(text: str) -> T.List[str]:
    """
    æŠŠæ¨¡å‹è¾“å‡ºé‡Œçš„çº¯ JSON è¡Œæå–å‡ºæ¥ï¼ˆé²æ£’å¤„ç†ï¼‰ï¼š
    - é€è¡Œåˆ¤æ–­ï¼šä»¥ { å¼€å¤´ ä¸” ä»¥ } ç»“å°¾ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ª JSON å¯¹è±¡è¡Œ
    - ä¹Ÿèƒ½å®¹å¿å‰åå¤šä½™ç©ºè¡Œæˆ–è§£é‡Šæ–‡å­—ï¼ˆä¼šè¢«å¿½ç•¥ï¼‰
    """
    lines = []
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith("{") and s.endswith("}"):
            lines.append(s)
    return lines

def add_index_to_jsonl_lines(jsonl_lines):
    """
    ç»™åŸå§‹ jsonl æ¯æ¡å‘è¨€åŠ ä¸Šä¸€ä¸ªå”¯ä¸€è¡Œå·å­—æ®µ `_idx`ï¼Œ
    è¿”å›æ–°çš„ List[str]ï¼Œæ¯ä¸ªå…ƒç´ ä»ç„¶æ˜¯ä¸€è¡Œ JSON å­—ç¬¦ä¸²ã€‚
    """
    new_lines = []
    for idx, line in enumerate(jsonl_lines, start=1):
        line = line.strip()
        if not line:
            continue
        obj = json.loads(line)
        obj["_idx"] = idx  # æ–°å¢è¡Œå·
        new_lines.append(json.dumps(obj, ensure_ascii=False))
    return new_lines


def count_output_filter_stats(output_filter: str):
    """
    ç»Ÿè®¡æ¨¡å‹#1 è¾“å‡ºä¸­çš„ï¼š
    - æ€»è¡Œæ•° total_linesï¼ˆç©å®¶+å®¢æœ+ç ”å‘ï¼‰
    - ç©å®¶å‘è¨€è¡Œæ•° player_linesï¼ˆç®€å•ç”¨ å‘è¨€äººID é‡Œçš„å…³é”®è¯åŒºåˆ†ï¼‰
    """
    total_lines = 0
    player_lines = 0

    for line in output_filter.splitlines():
        line = line.strip()
        if not line:
            continue
        total_lines += 1

        try:
            obj = json.loads(line)
        except Exception:
            # å¦‚æœè¿™ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±åªèƒ½ç®—è¿› total_linesï¼Œæ— æ³•ç»†åˆ†
            continue

        speaker = (
            obj.get("ç©å®¶ID")
            or obj.get("å‘è¨€äººID")
            or obj.get("è§’è‰²ID")
            or ""
        )

        # ç®€å•æ’é™¤å®¢æœ/GM/å®˜æ–¹/è¿è¥/ç ”å‘ï¼Œå…¶ä½™è§†ä¸ºç©å®¶
        if any(key in str(speaker) for key in ["å®¢æœ", "GM", "å®˜æ–¹", "è¿è¥", "ç ”å‘"]):
            continue

        player_lines += 1

    return total_lines, player_lines


def get_covered_indices_from_cluster_output(output_cluster: str):
    """
    ä»æ¨¡å‹#2 çš„è‡ªç„¶è¯­è¨€è¾“å‡ºä¸­ï¼Œè§£ææ‰€æœ‰ â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[...æ•°å­—...]â€ é‡Œçš„æ•°å­—ï¼Œ
    æ”¶é›†æˆä¸€ä¸ª set è¿”å›ã€‚

    é€‚é…ç±»ä¼¼æ ¼å¼ï¼ˆä½ ç°åœ¨çš„è¾“å‡ºå°±æ˜¯è¿™æ ·ï¼‰ï¼š
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[88, 90]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[124,125,126]

    æ”¯æŒï¼š
    - ä¸­æ–‡/è‹±æ–‡å†’å·ï¼ˆ: / ï¼šï¼‰
    - ä¸­æ–‡/è‹±æ–‡ä¸­æ‹¬å·ï¼ˆ[] / ã€ã€‘ï¼‰
    - é€—å·å¯ä¸º , æˆ– ï¼Œ
    """
    covered = set()

    # æ­£åˆ™åŒ¹é… â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]â€
    pattern = r"å‘è¨€è¡Œå·åˆ—è¡¨[:ï¼š]\s*[\[\ã€]([0-9,\sï¼Œ]+)[\]\ã€‘]"

    for m in re.finditer(pattern, output_cluster):
        nums_str = m.group(1)  # é‡Œé¢æ˜¯ç±»ä¼¼ "1, 2, 3" æˆ– "124,125,126"
        # æ›¿æ¢ä¸­æ–‡é€—å·ï¼ŒæŒ‰é€—å·åˆ‡åˆ†
        nums_str = nums_str.replace("ï¼Œ", ",")
        for part in nums_str.split(","):
            p = part.strip()
            if not p:
                continue
            try:
                covered.add(int(p))
            except ValueError:
                continue

    return covered



################################ä¿®è¡¥bugè¯æç°‡åˆ’åˆ†è§£æ#########################

def _normalize_json_text(text: str) -> str:
    """
    å¯¹æ¨¡å‹è¾“å‡ºåšä¸€äº›å°ä¿®å¤ï¼Œä»¥ä¾¿ json.loads æ­£å¸¸è§£æï¼š
    1ï¼‰å»æ‰æ•´æ®µæœ«å°¾å¤šä½™çš„é€—å·ï¼š{"a":1},
    2ï¼‰å»æ‰ å±æ€§/å…ƒç´  åç´§è·Ÿ } æˆ– ] çš„éæ³•é€—å·ï¼š
        ä¾‹å¦‚ï¼š
            "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "xxx",
        }
        â†’  "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "xxx"
           }
    """
    text = text.rstrip()

    # 1) æ•´ä¸ªå¯¹è±¡æœ«å°¾å¤šæ‰“ä¸€é¢—é€—å·ï¼š{"a":1},
    if text.endswith(","):
        text = text[:-1].rstrip()

    # 2) å±æ€§/å…ƒç´ åç›´æ¥è·Ÿ } æˆ– ] çš„é€—å·ï¼š
    #    æŠŠ ",\n  }" æˆ– ",\n]" å˜æˆ "\n  }" æˆ– "\n]"
    text = re.sub(r",\s*(\n\s*[}\]])", r"\1", text)

    return text

def parse_model2_output_to_json_list(output_cluster: str, batch_idx: int = 0) -> List[Dict[str, Any]]:
    """
    æŠŠæ¨¡å‹#2çš„åŸå§‹è¾“å‡ºè§£ææˆ list[dict]ï¼š
    - æ”¯æŒä¸€è¡Œä¸€ä¸ª JSONï¼š
        {"è¯é¢˜ç°‡1": "...", "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "..."}
    - ä¹Ÿæ”¯æŒå¤šè¡Œæ¼‚äº® JSONï¼š
        {
          "è¯é¢˜ç°‡1": "...",
          "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "..."
        }
    - è‡ªåŠ¨å¿½ç•¥è§£é‡Šæ€§æ–‡å­—ã€markdown åˆ—è¡¨ "- {...}"
    - è°ƒç”¨ _normalize_json_text ä¿®å¤å°¾é€—å·ç­‰æ ¼å¼é—®é¢˜
    - è§£æå¤±è´¥æ—¶æ‰“å°å®Œæ•´å¯¹è±¡åŸæ–‡ï¼Œä½†ä¸ä¸­æ–­æ•´æ‰¹
    """
    objs: List[Dict[str, Any]] = []
    cur_lines: List[str] = []
    depth = 0  # å½“å‰å¤§æ‹¬å·åµŒå¥—å±‚æ•°

    lines = output_cluster.strip().splitlines()

    for idx, raw in enumerate(lines, start=1):
        line = (raw or "").rstrip()
        if not line.strip():
            continue

        # å»æ‰ markdown åˆ—è¡¨å‰ç¼€ï¼ˆå¸¸è§æ ¼å¼ï¼š"- {...}"ï¼‰
        if line.lstrip().startswith("- "):
            line = line.lstrip()[2:].lstrip()

        # å½“å‰è¿˜æ²¡è¿›å…¥ä»»ä½• JSON å¯¹è±¡ï¼Œå¹¶ä¸”è¿™ä¸€è¡Œä¹Ÿçœ‹ä¸åˆ° "{"
        if depth == 0 and "{" not in line:
            # å¤§æ¦‚ç‡æ˜¯è§£é‡Šæ€§æ–‡å­—ï¼Œæ¯”å¦‚ "ä»¥ä¸‹æ˜¯è¯é¢˜ç°‡..."
            # å¦‚æœæƒ³çœ‹ï¼Œå¯æ‰“å¼€ä¸‹é¢è¿™è¡Œï¼š
            # tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] â© è·³è¿‡éJSONè¡Œ #{idx}: {line[:80]}")
            continue

        # ç»Ÿè®¡è¿™ä¸€è¡Œçš„å¤§æ‹¬å·æ•°é‡
        open_cnt = line.count("{")
        close_cnt = line.count("}")

        if depth == 0:
            # åˆšåˆšè¿›å…¥ä¸€ä¸ªæ–°çš„å¯¹è±¡
            cur_lines = [line]
            depth = open_cnt - close_cnt

            # depth <= 0 è¯´æ˜è¿™æ˜¯â€œå•è¡Œå¯¹è±¡â€ï¼š{"a":1} æˆ– {"a":1},
            if depth <= 0:
                text = "\n".join(cur_lines)
                text = _normalize_json_text(text)
                try:
                    obj = json.loads(text)
                except JSONDecodeError as e:
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] âŒ JSONè§£æå¤±è´¥ï¼ˆè¡Œ#{idx}é™„è¿‘ï¼‰ï¼š{e}")
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] è¯¥å¯¹è±¡åŸæ–‡ï¼š{text}")
                else:
                    # ç»Ÿä¸€ "è¯é¢˜ç°‡1"/"è¯é¢˜ç°‡2"... -> "è¯é¢˜ç°‡"
                    for key in list(obj.keys()):
                        if key.startswith("è¯é¢˜ç°‡") and key != "è¯é¢˜ç°‡":
                            obj["è¯é¢˜ç°‡"] = obj.pop(key)
                    objs.append(obj)
                cur_lines = []
                depth = 0
        else:
            # å·²ç»åœ¨ä¸€ä¸ª JSON å¯¹è±¡å†…éƒ¨ï¼ˆå¤šè¡Œåœºæ™¯ï¼‰
            cur_lines.append(line)
            depth += open_cnt - close_cnt

            # depth å›åˆ° 0ï¼Œè¡¨ç¤ºä¸€ä¸ªå¯¹è±¡ç»“æŸ
            if depth <= 0:
                text = "\n".join(cur_lines)
                text = _normalize_json_text(text)

                try:
                    obj = json.loads(text)
                except JSONDecodeError as e:
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] âŒ JSONè§£æå¤±è´¥ï¼ˆè¡Œ#{idx}é™„è¿‘ï¼‰ï¼š{e}")
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] è¯¥å¯¹è±¡åŸæ–‡ï¼š{text}")
                else:
                    for key in list(obj.keys()):
                        if key.startswith("è¯é¢˜ç°‡") and key != "è¯é¢˜ç°‡":
                            obj["è¯é¢˜ç°‡"] = obj.pop(key)
                    objs.append(obj)

                cur_lines = []
                depth = 0

    return objs
    
def parse_jsonl_text_safe(text: str, label: str = "æ¨¡å‹#3èšåˆè¾“å‡º") -> List[Dict[str, Any]]:
    """
    å°è¯•æŠŠ text æŒ‰è¡Œè§£æä¸º JSON å¯¹è±¡ï¼š
    - æ¯è¡Œç‹¬ç«‹å°è¯• json.loads
    - è§£æå‰å…ˆå¯¹â€œæè½´â€ç­‰å¸¸è§é”™è¯¯åšä¸€æ¬¡æ­£åˆ™ä¿®å¤
    - è§£æå¤±è´¥æ—¶ä¸ä¼šæŠ›å¼‚å¸¸ï¼Œè€Œæ˜¯æ‰“å°å‡ºå…·ä½“è¡Œå·å’ŒåŸæ–‡ï¼Œæ–¹ä¾¿æ’æŸ¥
    """
    objs: List[Dict[str, Any]] = []
    lines = text.strip().splitlines()

    for idx, raw in enumerate(lines, start=1):
        s = (raw or "").strip()
        if not s:
            continue

        # è·³è¿‡ ```json / ``` ç­‰ä»£ç å—æ ‡è®°
        if s.startswith("```"):
            continue

        # â­ å…ˆä¿®å¤â€œæè½´â€ç›¸å…³æ ¼å¼é”™è¯¯
        s_fixed = fix_model3_line_extreme_axis(s)

        try:
            obj = json.loads(s_fixed)
        except JSONDecodeError as e:
            print(f"[{label}] âŒ JSONè§£æå¤±è´¥ï¼šè¡Œ#{idx} -> {e}")
            print(f"[{label}] è¯¥è¡ŒåŸæ–‡(ä¿®å¤å)ï¼š{s_fixed}")
            continue

        objs.append(obj)

    return objs

def fix_model3_line_extreme_axis(s: str) -> str:
    """
    ä¸“é—¨ä¿®å¤æ¨¡å‹#3è¾“å‡ºä¸­â€œæè½´â€ç›¸å…³çš„åæ ¼å¼ï¼š
    é’ˆå¯¹ä»¥ä¸‹å‡ ç§æƒ…å†µï¼š
    1) "æ—¥æœŸ": "æè½´": "2025-12-06"  =>  "æ—¥æœŸ": "2025-12-06"
    2) "æ—¶é—´è½´": "æè½´": "22:30:57-22:33:57"  =>  "æ—¶é—´è½´": "22:30:57-22:33:57"
    3) "æ—¶é—´è½´": "22:34:24-æè½´": "22:38:33" => "æ—¶é—´è½´": "22:34:24-22:38:33"
    """

    # 1) ä¿®å¤æ—¥æœŸå­—æ®µè¢«â€œæè½´â€æ±¡æŸ“ï¼š
    #    "æ—¥æœŸ": "æè½´": "2025-12-06"
    s = re.sub(
        r'"æ—¥æœŸ"\s*:\s*"æè½´"\s*:\s*"([^"]+)"',
        r'"æ—¥æœŸ": "\1"',
        s
    )

    # 2) ä¿®å¤æ—¶é—´è½´å­—æ®µå®Œå…¨æ˜¯ "æè½´": "xxx" çš„æƒ…å†µï¼š
    #    "æ—¶é—´è½´": "æè½´": "22:30:57-22:33:57"
    s = re.sub(
        r'"æ—¶é—´è½´"\s*:\s*"æè½´"\s*:\s*"([^"]+)"',
        r'"æ—¶é—´è½´": "\1"',
        s
    )

    # 3) ä¿®å¤æ—¶é—´è½´å€¼ä¸­é—´å¸¦ "-æè½´": " çš„æƒ…å†µï¼š
    #    "æ—¶é—´è½´": "22:34:24-æè½´": "22:38:33"
    #    -> "æ—¶é—´è½´": "22:34:24-22:38:33"
    s = re.sub(
        r'"æ—¶é—´è½´"\s*:\s*"([^"]*?)-æè½´"\s*:\s*"([^"]+)"',
        r'"æ—¶é—´è½´": "\1-\2"',
        s
    )

    return s

#################################è¯æç°‡å”¯ä¸€id#################################

###1. clusteridçš„æ—¥æœŸæ‰’å–##
def infer_date_for_batch(
    cluster_json_list: List[Dict[str, Any]],
    batch_lines: List[str]
) -> str:
    """
    ä¼˜å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜ä¸­æŠ½å–æ—¥æœŸï¼ˆå½¢å¦‚ï¼šxxxxï¼ˆ2025-12-02 16:30:01-17:42:41ï¼‰ï¼‰
    è‹¥å¤±è´¥ï¼Œåˆ™å›é€€åˆ°åŸå§‹å‘è¨€ä¸­çš„ `å‘è¨€æ—¥æœŸ` / `æ—¥æœŸ` å­—æ®µã€‚
    """
    # 1ï¼‰å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜é‡Œæ‰¾ YYYY-MM-DD
    for obj in cluster_json_list:
        title = str(
            obj.get("è¯é¢˜ç°‡")
            or obj.get("èšåˆè¯é¢˜ç°‡")
            or ""
        )
        m = re.search(r"(\d{4}-\d{2}-\d{2})", title)
        if m:
            return m.group(1)

    # 2ï¼‰å¦‚æœæ ‡é¢˜é‡Œæ²¡æœ‰ï¼Œå°±ä»åŸå§‹å‘è¨€é‡Œæ‹¿
    for line in batch_lines:
        line = line.strip()
        if not line:
            continue
        try:
            msg = json.loads(line)
        except Exception:
            continue

        date_str = msg.get("å‘è¨€æ—¥æœŸ") or msg.get("æ—¥æœŸ")
        if date_str:
            return date_str

    # 3ï¼‰å†ä¸è¡Œå°±æŠ¥é”™ï¼Œè®©ä½ æ„è¯†åˆ°æ•°æ®æ ¼å¼æœ‰é—®é¢˜
    raise ValueError("æ— æ³•ä»è¯é¢˜ç°‡æ ‡é¢˜æˆ–åŸå§‹å‘è¨€ä¸­æ¨æ–­å‡ºæ—¥æœŸï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼ã€‚")

##2.ç”Ÿæˆè¯æç°‡clusterid##
def assign_global_cluster_ids(cluster_list, date_str, batch_id):
    """
    ä¸ºæ¯ä¸ªè¯é¢˜ç°‡ç”Ÿæˆå…¨å±€å”¯ä¸€IDå­—æ®µ `_cluster_id`
    æ ¼å¼ï¼šYYYY-MM-DD_BX_XXï¼Œå¦‚ 2025-11-20_B2_03
    """
    for idx, cluster in enumerate(cluster_list, start=1):
        cluster["_cluster_id"] = f"{date_str}_{batch_id}_{idx:02d}"
    return cluster_list

#################################èšåˆæ¯å¤©çš„è¯æç°‡åˆ†æ‰¹è¾“å‡º#################################
def aggregate_cluster_outputs(batch_outputs: List[str]) -> str:
    all_lines = []

    for batch_id, text in enumerate(batch_outputs, start=1):
        if not text:
            continue

        for line in text.strip().splitlines():
            line = line.strip()
            if not line:
                continue

            try:
                obj = json.loads(line)
            except Exception:
                # å¦‚æœæœ‰ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±è·³è¿‡ï¼ˆä¹Ÿå¯ä»¥æ”¹æˆ raiseï¼Œçœ‹ä½ éœ€æ±‚ï¼‰
                continue

            clean_line = json.dumps(obj, ensure_ascii=False)
            all_lines.append(clean_line)

    # èšåˆä¸ºä¸€ä¸ªå¤§çš„ JSONL å­—ç¬¦ä¸²
    return "\n".join(all_lines)


################top5ç­›é€‰#################################


# -------------------------------
# ğŸ”¹ 1. åŒ¹é…åŸå§‹å‘è¨€
# -------------------------------
from datetime import datetime
import json
from typing import List, Dict

import json
from datetime import datetime
from typing import List, Tuple

from datetime import datetime
from typing import List, Tuple

def _parse_time_ranges(time_axis_str: str) -> List[Tuple[datetime.time, datetime.time]]:
    """
    æ”¯æŒæ—¶é—´è½´æ ¼å¼ï¼š
    - "16:31:26-16:32:57"
    - "16:31:26 è‡³ 16:32:57"
    - "16:31:26åˆ°16:32:57"
    - "16:31:26~16:32:57" / "16:31:26ï½16:32:57"
    - å¤šæ®µç”¨ "ã€" åˆ†éš”
    """
    if not time_axis_str or not isinstance(time_axis_str, str):
        return []

    # ç»Ÿä¸€å»æ‰ä¸¤è¾¹ç©ºç™½
    s = time_axis_str.strip()

    # ç»Ÿä¸€å„ç§è¿æ¥ç¬¦ä¸º "-"
    for sep in ["è‡³", "åˆ°", "~", "ï½", "â€”", "â€“"]:
        s = s.replace(sep, "-")

    # å»æ‰ä¸­é—´å¤šä½™ç©ºæ ¼ï¼ˆé˜²æ­¢ "16:31:26 - 16:32:57"ï¼‰
    s = s.replace(" ", "")

    parts = [p.strip() for p in s.split("ã€") if p.strip()]
    ranges: List[Tuple[datetime.time, datetime.time]] = []

    for p in parts:
        if "-" not in p:
            continue
        a, b = [x.strip() for x in p.split("-", 1)]
        try:
            start_t = datetime.strptime(a, "%H:%M:%S").time()
            end_t   = datetime.strptime(b, "%H:%M:%S").time()
        except Exception:
            continue
        ranges.append((start_t, end_t))

    return ranges



from datetime import datetime
from typing import Any, Dict, List, Optional, Tuple

def match_dialogs_by_time(
    messages: List[Dict[str, Any]],
    date_str: str,
    time_axis_str: Optional[str],
    dedup: bool = True,
) -> List[Dict[str, Any]]:
    """
    ä¸¥æ ¼æ¨¡å¼ï¼ˆæ— å…œåº•ï¼‰ï¼š
    - date_str / time_axis_str ä¸ºç©ºæˆ–è§£æå¤±è´¥ -> è¿”å› []
    - ä»…åŒ¹é… row['å‘è¨€æ—¥æœŸ'] == date_str ä¸” row['å‘è¨€æ—¶é—´'] è½åœ¨æ—¶é—´è½´èŒƒå›´å†…
    - æ”¯æŒå¤šæ®µï¼š "a-bã€c-d"
    - æ”¯æŒè·¨å¤©æ®µï¼ˆend < startï¼‰ï¼šè®¤ä¸ºè·¨åˆ°æ¬¡æ—¥ï¼›åœ¨åŒä¸€å¤©æ•°æ®é‡ŒæŒ‰ t>=start æˆ– t<=end å‘½ä¸­
    - dedup=Trueï¼šé¿å…å¤šæ®µé‡å å¯¼è‡´åŒä¸€æ¡æ¶ˆæ¯é‡å¤è®¡æ•°
    """
    if not messages or not date_str:
        return []
    if not time_axis_str or not isinstance(time_axis_str, str) or not time_axis_str.strip():
        return []

    ranges = _parse_time_ranges(time_axis_str)  # âœ… è¿™é‡Œä¸€æ¬¡æ€§è§£ææ•´ä¸ªæ—¶é—´è½´å­—ç¬¦ä¸²
    if not ranges:
        return []

    matched: List[Dict[str, Any]] = []
    seen = set()

    for row in messages:
        if (row.get("å‘è¨€æ—¥æœŸ") or "") != date_str:
            continue

        ts = row.get("å‘è¨€æ—¶é—´") or ""
        try:
            t = datetime.strptime(ts, "%H:%M:%S").time()
        except Exception:
            continue

        hit = False
        for start_t, end_t in ranges:
            if start_t <= end_t:
                # æ™®é€šåŒºé—´
                if start_t <= t <= end_t:
                    hit = True
                    break
            else:
                # è·¨å¤©åŒºé—´ï¼ˆå¦‚ 23:59:50-00:02:10ï¼‰
                if t >= start_t or t <= end_t:
                    hit = True
                    break

        if not hit:
            continue

        if dedup:
            key = (
                row.get("_idx"),
                row.get("å‘è¨€æ—¥æœŸ"),
                row.get("å‘è¨€æ—¶é—´"),
                row.get("ç©å®¶ID"),
                row.get("ç©å®¶æ¶ˆæ¯"),
            )
            if key in seen:
                continue
            seen.add(key)

        matched.append(row)

    return matched


def get_dialogs_lines_by_fayan_time_debug(
    jsonl_lines01: list[str],
    date_str: str,
    fayan_time: str,
    debug: bool = True,
) -> list[str]:
    """
    debug=True ä¸”ç»“æœä¸ºç©ºæ—¶ï¼Œä¼šæ‰“å°ï¼š
    - ä¼ å…¥çš„ date/time_axis
    - è§£æåçš„æ—¶é—´æ®µ
    - å½“å¤©åŸæ–‡æœ€æ—©/æœ€æ™šæ—¶é—´
    - å½“å¤©åŸæ–‡æ€»æ¡æ•°
    - å‰å‡ æ¡åŸæ–‡æ—¶é—´æ ·ä¾‹
    """
    if not date_str:
        if debug:
            print("ğŸ§¯[DEBUG] date_str ä¸ºç©ºï¼Œæ— æ³•å›æº¯")
        return []

    time_ranges = _parse_time_ranges(fayan_time)
    if not time_ranges:
        if debug:
            print("ğŸ§¯[DEBUG] æ—¶é—´è½´è§£æå¤±è´¥")
            print("  date_str =", date_str)
            print("  fayan_time =", repr(fayan_time))
        return []

    # å…ˆæ‰«æå½“å¤©çš„æ‰€æœ‰æ—¶é—´ï¼Œå¾—åˆ° min/maxï¼ˆç”¨äºåˆ¤æ–­æ˜¯ä¸æ˜¯æ—¥æœŸé”™äº† or åŸæ–‡æ—¶é—´æ ¼å¼ä¸å¯¹ï¼‰
    day_times = []
    day_samples = []
    for line in jsonl_lines01:
        s = (line or "").strip()
        if not s:
            continue
        try:
            obj = json.loads(s)
        except json.JSONDecodeError:
            continue
        if (obj.get("å‘è¨€æ—¥æœŸ") or "") != date_str:
            continue
        ts = obj.get("å‘è¨€æ—¶é—´") or ""
        try:
            t = datetime.strptime(ts, "%H:%M:%S").time()
        except Exception:
            continue
        day_times.append(t)
        if len(day_samples) < 8:
            day_samples.append(ts)

    # æ­£å¼è¿‡æ»¤
    out = []
    for line in jsonl_lines01:
        s = (line or "").strip()
        if not s:
            continue
        try:
            obj = json.loads(s)
        except json.JSONDecodeError:
            continue

        if (obj.get("å‘è¨€æ—¥æœŸ") or "") != date_str:
            continue

        ts = obj.get("å‘è¨€æ—¶é—´") or ""
        try:
            t = datetime.strptime(ts, "%H:%M:%S").time()
        except Exception:
            continue

        for start_t, end_t in time_ranges:
            if start_t <= t <= end_t:
                out.append(s)
                break

    # ç»“æœä¸ºç©º -> æ‰“å°é”å®šä¿¡æ¯
    if debug and not out:
        print("\n" + "ğŸ§¯"*20)
        print("ğŸ§¯[DEBUG] æ—¶é—´æ®µå†…æ²¡æœ‰åŸæ–‡ï¼Œé”å®šä¿¡æ¯å¦‚ä¸‹ï¼š")
        print("  date_str =", date_str)
        print("  fayan_time =", fayan_time)
        print("  parsed_ranges =", [(a.strftime('%H:%M:%S'), b.strftime('%H:%M:%S')) for a,b in time_ranges])

        if day_times:
            print("  day_count =", len(day_times))
            print("  day_min =", min(day_times).strftime("%H:%M:%S"))
            print("  day_max =", max(day_times).strftime("%H:%M:%S"))
            print("  day_time_samples =", day_samples)
        else:
            print("  day_count = 0  ğŸ‘‰ è¯´æ˜ï¼šè¿™ä¸ª date_str åœ¨åŸæ–‡é‡Œæ ¹æœ¬ä¸å­˜åœ¨ï¼ˆæ—¥æœŸä¸ä¸€è‡´ï¼‰")

        # é¢å¤–ï¼šæŠŠç›®æ ‡æ—¶é—´æ®µæ‰“å°å‡ºæ¥ï¼Œæ£€æŸ¥æ˜¯å¦è¶…å‡ºå½“å¤©èŒƒå›´
        for a, b in time_ranges:
            if day_times and (b < min(day_times) or a > max(day_times)):
                print("  âš  ç›®æ ‡æ—¶é—´æ®µå®Œå…¨è½åœ¨å½“å¤©åŸæ–‡èŒƒå›´ä¹‹å¤–ï¼ˆå¾ˆå¯èƒ½ï¼šæ—¥æœŸé”™ or æ—¶é—´è½´é”™ï¼‰")
                break

        print("ğŸ§¯"*20 + "\n")

    return out


def get_time_axis(cluster: dict) -> str | None:
    # ä¼˜å…ˆæ ‡å‡†å­—æ®µ
    for key in ("æ—¶é—´è½´", "timeè½´", "æ—¶é—´è½´ ", "æ—¶é—´æ®µ", "æ—¶é—´åŒºé—´"):
        val = cluster.get(key)
        if isinstance(val, str) and val.strip():
            return val.strip()
    return None

def extract_cluster_stats(èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict], åŸå§‹å‘è¨€: List[str]) -> List[Dict]:
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    results = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = get_time_axis(cluster)
        if not date or not time_axis:
            print(f"âš  èšåˆè¯é¢˜ç°‡ç¼ºå°‘æ—¥æœŸæˆ–æ—¶é—´è½´ï¼Œè·³è¿‡ï¼š{cluster}")
            continue
        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)
        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        result = {
            "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡"),
            "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
            "å‘è¨€ç©å®¶æ€»æ•°": len(players),
            "å‘è¨€æ€»æ•°": len(matched)
        }
        results.append(result)

    return results

# -------------------------------
# ğŸ”¹ 2. çƒ­åº¦è¯†åˆ«ä¸»å‡½æ•°
# -------------------------------
def compute_heat_score(U: int, M: int) -> float:
    if U == 0 or M == 0:
        return 0.0
    return round(U * math.sqrt(M), 2)

def extract_top5_heat_clusters(èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict], åŸå§‹å‘è¨€: List[str], top_k=5) -> List[Dict]:
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    enriched = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = cluster.get("æ—¶é—´è½´")
        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)

        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        U = len(players)
        M = len(matched)
        heat = compute_heat_score(U, M)

        enriched.append({
        "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥",
        "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
        "æ—¥æœŸ": date,
        "æ—¶é—´è½´": time_axis,
        "å‘è¨€ç©å®¶æ€»æ•°": U,
        "å‘è¨€æ€»æ•°": M,
        "çƒ­åº¦è¯„åˆ†": heat
         })


    # æŒ‰çƒ­åº¦è¯„åˆ†æ’åºï¼Œå– TopK
    enriched.sort(key=lambda x: x["çƒ­åº¦è¯„åˆ†"], reverse=True)
    return enriched[:top_k]

# -------------------------------
# ğŸ”¹ 3. æ·»åŠ è®¨è®ºç‚¹å­—æ®µ
# -------------------------------
def attach_discussion_points(top_clusters: List[Dict], subclusters: List[Dict]) -> List[Dict]:
    """
    å°† top_clusters ä¸­æ¯ä¸ªèšåˆç°‡çš„å­è¯é¢˜ç°‡åˆ—è¡¨ï¼Œä¸ subclusters ä¸­çš„ _cluster_id åŒ¹é…ï¼Œ
    æ‹¼å‡ºæ ¸å¿ƒæœºåˆ¶æè¿°ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºåˆ—ç‚¹ + ç©ºè¡Œã€‚ç§»é™¤å­è¯é¢˜ç°‡åˆ—è¡¨å­—æ®µã€‚
    """
    # æ„å»º _cluster_id â†’ æ ¸å¿ƒæœºåˆ¶ æ˜ å°„
    cluster_mechanism_map = {
        row["_cluster_id"]: row["æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶"]
        for row in subclusters
        if "_cluster_id" in row and "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶" in row
    }

    result = []
    for cluster in top_clusters:
        ids = cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨", [])
        mechanisms = [cluster_mechanism_map.get(cid) for cid in ids if cid in cluster_mechanism_map]

        # æ ¼å¼åŒ–ä¸ºåˆ—ç‚¹ + ç©ºè¡Œ
        discussion_point = [m for m in mechanisms if m]

        # ä¿®å¤èšåˆè¯é¢˜ç°‡åç§°å­—æ®µä¸ºç©ºçš„é—®é¢˜
        cluster_name = cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥"

        enriched_cluster = {
            "èšåˆè¯é¢˜ç°‡": cluster_name,
            "æ—¥æœŸ": cluster.get("æ—¥æœŸ"),
            "æ—¶é—´è½´": cluster.get("æ—¶é—´è½´"),
            "å‘è¨€ç©å®¶æ€»æ•°": cluster.get("å‘è¨€ç©å®¶æ€»æ•°"),
            "å‘è¨€æ€»æ•°": cluster.get("å‘è¨€æ€»æ•°"),
            "çƒ­åº¦è¯„åˆ†": cluster.get("çƒ­åº¦è¯„åˆ†"),
            "è®¨è®ºç‚¹": discussion_point
        }

        result.append(enriched_cluster)

    return result



####################### å­˜å…¥æ¯æ—¥å‘è¨€ top5 ########################


def append_daily_top5_to_version_jsonl(
    final_result: List[Dict],
    version_jsonl_path: str = "version_daily_top5_with_opinion.jsonl",
):
    """
    å°†å½“æ—¥ Top5 è¿½åŠ å†™å…¥æŸä¸ªã€ç‰ˆæœ¬ç´¯è®¡ jsonl æ–‡ä»¶ã€‘ä¸­ã€‚
    
    åŒæ—¶ä¸ºæ¯æ¡è®°å½•è¡¥å……ä¸¤ä¸ªå­—æ®µï¼š
    1ï¼‰_idxï¼šåœ¨ version_jsonl_path ä¸­çš„å…¨å±€é€’å¢è¡Œå·ï¼ˆä»1å¼€å§‹ï¼‰
    2ï¼‰_daily_top_idï¼šå½“å¤©å†…çš„ Top è¯é¢˜ç°‡IDï¼Œå½¢å¼ï¼šYYYY-MM-DD_TXXï¼Œä¾‹å¦‚ 2025-12-03_T01
    
    è¯´æ˜ï¼š
    - å‡è®¾ final_result å†…æ‰€æœ‰è®°å½•çš„ "æ—¥æœŸ" ç›¸åŒï¼ˆå³åŒä¸€å¤©çš„ top5ï¼‰
    - å¦‚æœæ–‡ä»¶ä¸­å·²å­˜åœ¨åŒä¸€æ—¥æœŸçš„æ•°æ®ï¼Œä¼šåœ¨åŸæœ‰åŸºç¡€ä¸Šç»§ç»­ç´¯åŠ  _daily_top_id çš„ç¼–å·
    - ä¸ä¼šä¿®æ”¹ä»»ä½•å·²æœ‰çš„ `_cluster_id` å­—æ®µï¼ˆå­è¯é¢˜ç°‡ä»ç„¶ç”¨å®ƒï¼‰
    """
    if not final_result:
        print("âš  final_result ä¸ºç©ºï¼Œä»Šæ—¥æ—  Top5 å¯å†™å…¥ã€‚")
        return

    # å–å½“å¤©æ—¥æœŸï¼ˆå‡è®¾ final_result åŒä¸€æ‰¹éƒ½æ˜¯åŒä¸€å¤©ï¼‰
    date_str = final_result[0].get("æ—¥æœŸ")
    if not date_str:
        raise ValueError("final_result ä¸­ç¼ºå°‘ 'æ—¥æœŸ' å­—æ®µï¼Œæ— æ³•ç”Ÿæˆ _daily_top_idã€‚")

    path = Path(version_jsonl_path)

    # ---------- 1. è®¡ç®—å½“å‰æ–‡ä»¶ä¸­çš„æœ€å¤§ _idxï¼ˆå…¨å±€è¡Œå·ï¼‰ ----------
    global_max_idx = 0
    # åŒæ—¶ç»Ÿè®¡â€œä»Šå¤©â€å·²ç»æœ‰å¤šå°‘æ¡ï¼Œç”¨äº _daily_top_id ç»­ç¼–å·
    existing_count_for_day = 0

    if path.exists():
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError:
                    continue

                # ç»Ÿè®¡å…¨å±€ _idx
                if isinstance(obj.get("_idx"), int):
                    global_max_idx = max(global_max_idx, obj["_idx"])
                else:
                    # æ—§æ•°æ®æ²¡æœ‰ _idxï¼Œå°±ç®€å•å½“ä½œä¸€è¡Œï¼Œæ–°è¡Œå·å¾€åæ¨
                    global_max_idx += 1

                # ç»Ÿè®¡å½“å¤©çš„æ¡æ•°ï¼ˆç”¨äº _daily_top_idï¼‰
                if obj.get("æ—¥æœŸ") == date_str:
                    existing_count_for_day += 1

    # ---------- 2. è¿½åŠ å†™å…¥ï¼Œå¹¶ä¸ºæ–°è®°å½•ç”Ÿæˆ _idx + _daily_top_id ----------
    mode = "a" if path.exists() else "w"
    with path.open(mode, encoding="utf-8") as f:
        for offset, row in enumerate(final_result, start=1):
            row = dict(row)  # é¿å…ä¿®æ”¹åŸå¯¹è±¡å¼•ç”¨

            # 2.1 å…¨å±€é€’å¢ _idxï¼ˆç‰ˆæœ¬æ‰€æœ‰å¤©å…±ç”¨ä¸€ä¸ªåºå·ï¼‰
            global_max_idx += 1
            row["_idx"] = global_max_idx

            # 2.2 ç”Ÿæˆå½“å¤©çš„ top idï¼ˆä¸ç¢° `_cluster_id`ï¼‰
            # å½“å¤©å·²æœ‰ existing_count_for_day æ¡ï¼Œè¿™æ‰¹ä» +1 å¼€å§‹æ¥ç€æ’
            idx_for_day = existing_count_for_day + offset
            row["_daily_top_id"] = f"{date_str}_T{idx_for_day:02d}"

            f.write(json.dumps(row, ensure_ascii=False) + "\n")

    print(f"âœ… å·²å°†å½“æ—¥ Top5ï¼ˆå« _idx å’Œ _daily_top_idï¼‰è¿½åŠ å†™å…¥: {path}")


#########åŒ¹é…final+output ==>è®¨è®ºç‚¹+æ—¶é—´è½´################
import json
import re
from typing import List, Dict, Any

def parse_jsonl_text(text: str) -> List[Dict[str, Any]]:
    """
    è§£ææ¨¡å‹è¾“å‡ºçš„â€œä¼ª jsonlâ€ï¼Œå°½é‡ä»ä¸­æå–å‡ºè‹¥å¹²åˆæ³•çš„ dictã€‚
    - æ”¯æŒå¼€å¤´/ç»“å°¾æœ‰ ```json ä»£ç å—åŒ…è£¹
    - è·³è¿‡é { å¼€å¤´çš„è¡Œ
    - è‡ªåŠ¨å»æ‰è¡Œå°¾çš„é€—å·
    - è§£æå¤±è´¥æ—¶æ‰“å° warningï¼Œä¸ä¸­æ–­
    """
    if not text:
        return []

    s = text.strip()

    # å»æ‰ ```json / ``` åŒ…è£¹
    if s.startswith("```"):
        s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
        s = re.sub(r"\s*```$", "", s)

    results: List[Dict[str, Any]] = []

    for raw in s.splitlines():
        line = raw.strip()
        if not line:
            continue

        # æœ‰äº›æ¨¡å‹ä¼šè¾“å‡º "- {...}" ä¹‹ç±»çš„ markdown åˆ—è¡¨
        if line.startswith("- "):
            line = line[2:].lstrip()

        # è·³è¿‡æ˜æ˜¾ä¸æ˜¯ JSON å¯¹è±¡çš„è¡Œ
        if not line.startswith("{"):
            continue

        # å»æ‰æœ«å°¾å¤šä½™çš„é€—å·
        if line.endswith(","):
            line = line[:-1].rstrip()

        try:
            obj = json.loads(line)
        except json.JSONDecodeError as e:
            print(f"âš  è§£æ JSON è¡Œå¤±è´¥ï¼š{e} | è¡Œå†…å®¹å‰ 120 å­—ç¬¦ï¼š{line[:120]}")
            continue

        if isinstance(obj, dict):
            results.append(obj)
        else:
            print(f"âš  è§£æç»“æœä¸æ˜¯ dictï¼Œå·²è·³è¿‡ï¼š{type(obj)}")

    return results



def print_mech_time_from_top5(top5_results: list[dict], output_cluster_jsonl: str):
    output_clusters = parse_jsonl_text(output_cluster_jsonl)

    # æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶ -> å‘è¨€æ—¶é—´
    mech_time = {}
    for r in output_clusters:
        mech = (r.get("æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶") or "").strip()
        t = (r.get("å‘è¨€æ—¶é—´") or "").strip()
        if mech and mech not in mech_time:
            mech_time[mech] = t

    rows = []
    miss = 0

    for top in top5_results:
        for dp in (top.get("è®¨è®ºç‚¹") or []):
            mech = (dp or "").strip()
            if not mech:
                continue

            row = {
                "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": mech,
                "å‘è¨€æ—¶é—´": mech_time.get(mech, "")
            }
            rows.append(row)

            # æ‰“å° jsonl è¡Œ
            #print(json.dumps(row, ensure_ascii=False))

            if not row["å‘è¨€æ—¶é—´"]:
                miss += 1

    # é¢å¤–ç»™ä½ ä¸€ä¸ªå°æç¤ºï¼Œæ–¹ä¾¿ç¡®è®¤æœ‰æ²¡æœ‰æ¼åŒ¹é…
    if miss:
        print(f"[WARN] æœ‰ {miss} æ¡è®¨è®ºç‚¹æœªåŒ¹é…åˆ°å‘è¨€æ—¶é—´ï¼ˆè¯·æ£€æŸ¥æ˜¯å¦å®Œå…¨ä¸€è‡´ï¼‰")

    return rows

######################é”å®šæ—¶é—´è¿˜åŸåŸæ–‡#############

def parse_fayan_time_range_str(fayan_time: str):
    """
    è¾“å…¥:  '2025-12-04 14:00:01-14:09:52'
    è¾“å‡º:  ('2025-12-04', '14:00:01', '14:09:52')
    """
    if not fayan_time:
        return None
    s = str(fayan_time).strip()
    m = re.search(r"(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2}:\d{2})\s*-\s*(\d{2}:\d{2}:\d{2})", s)
    if not m:
        return None
    return m.group(1), m.group(2), m.group(3)

def get_dialogs_lines_by_fayan_time(jsonl_lines01: list[str], fayan_time: str) -> list[str]:
    """
    ä»åŸå§‹ jsonl_lines01 (list[str]) é‡Œç­›å‡ºè½åœ¨å‘è¨€æ—¶é—´èŒƒå›´å†…çš„æ‰€æœ‰åŸæ–‡è¡Œï¼ˆä»ç„¶è¿”å› list[str]ï¼‰ã€‚
    """
    parsed = parse_fayan_time_range_str(fayan_time)
    if not parsed:
        return []
    date_str, start_str, end_str = parsed

    start_t = datetime.strptime(start_str, "%H:%M:%S").time()
    end_t   = datetime.strptime(end_str, "%H:%M:%S").time()

    out = []
    for line in jsonl_lines01:
        s = (line or "").strip()
        if not s:
            continue
        try:
            obj = json.loads(s)
        except json.JSONDecodeError:
            continue

        if (obj.get("å‘è¨€æ—¥æœŸ") or "") != date_str:
            continue

        ts = obj.get("å‘è¨€æ—¶é—´") or ""
        try:
            t = datetime.strptime(ts, "%H:%M:%S").time()
        except Exception:
            continue

        if start_t <= t <= end_t:
            out.append(s)  # âœ… ä¿æŒåŸæ ·ï¼šä¸€æ¡jsonå­—ç¬¦ä¸²
    return out

#------------------------æ¨¡å‹4è¾“å‡ºæ‹†åˆ†---------------------------------

def parse_opinion_output_to_list(opinion_output: str) -> List[Dict[str, Any]]:
    """
    æŠŠæ¨¡å‹4è¿”å›çš„å­—ç¬¦ä¸²è§£ææˆ List[dict]ï¼š
    - å¯èƒ½æ˜¯ä¸€ä¸ª JSON å¯¹è±¡
    - ä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ª JSON æ•°ç»„
    - ä¹Ÿå¯èƒ½æ˜¯ jsonlï¼ˆå¤šè¡Œï¼Œæ¯è¡Œä¸€ä¸ª JSONï¼‰
    """
    if not opinion_output:
        return []

    s = opinion_output.strip()
    # 1) å°è¯•æ•´ä½“è§£æ
    try:
        obj = json.loads(s)
        if isinstance(obj, dict):
            return [obj]
        if isinstance(obj, list):
            return [x for x in obj if isinstance(x, dict)]
    except json.JSONDecodeError:
        pass

    # 2) å°è¯•æŒ‰è¡Œè§£æ jsonl
    results = []
    for raw in s.splitlines():
        line = raw.strip()
        if not line or not line.startswith("{"):
            continue
        try:
            d = json.loads(line)
            if isinstance(d, dict):
                results.append(d)
        except json.JSONDecodeError:
            continue

    return results

######################è§‚ç‚¹å›æº¯è‡³top5åšæœ€åè¾“å‡º##############
def _norm_text(s: str) -> str:
    """æ–‡æœ¬å½’ä¸€åŒ–ï¼šå»ç©ºæ ¼å’Œå¸¸è§æ ‡ç‚¹ï¼Œç”¨äºå®½æ¾åŒ¹é…è®¨è®ºç‚¹."""
    if not s:
        return ""
    s = str(s)
    s = re.sub(r"\s+", "", s)
    s = re.sub(r"[ï¼Œ,ã€‚\.ï¼ï¼Ÿ!ï¼š:ã€ï¼›;ï¼ˆï¼‰()\[\]ã€ã€‘\"'â€œâ€â€˜â€™]", "", s)
    return s


def merge_top5_with_opinions_numbered(
    top5_results: List[Dict[str, Any]],
    opinions: List[Dict[str, Any]],
) -> List[Dict[str, Any]]:
    """
    ç”¨â€œè®¨è®ºç‚¹â€æŠŠæ¨¡å‹4çš„è§‚ç‚¹ç»“æœåˆå¹¶å› top5ã€‚

    - ä»¥ è®¨è®ºç‚¹ æ–‡æœ¬åš keyï¼ŒæŠŠ opinions ä¸­å¯¹åº”çš„è§‚ç‚¹å¯¹è±¡æŒ‚å› top5ã€‚
    - è¾“å‡ºï¼š
        * ä¿ç•™åŸ top5 å­—æ®µï¼Œä½†ç§»é™¤é¡¶å±‚ "è®¨è®ºç‚¹" / "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"
        * æ–°å¢ "è®¨è®ºç‚¹åˆ—è¡¨"ï¼š
            [
              {
                "è®¨è®ºç‚¹1": "xxx",
                "ç©å®¶è§‚ç‚¹": [...],
                "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": [...]
              },
              ...
            ]
    """

    # 1) opinions -> mapï¼šå½’ä¸€åŒ–è®¨è®ºç‚¹ => opinion å¯¹è±¡
    op_map: Dict[str, Dict[str, Any]] = {}
    for op in opinions:
        dp = (op.get("è®¨è®ºç‚¹") or "").strip()
        if not dp:
            continue
        op_map[_norm_text(dp)] = op

    merged: List[Dict[str, Any]] = []

    # 2) åˆå¹¶å› top5
    for row in top5_results:
        dps = row.get("è®¨è®ºç‚¹") or []
        if isinstance(dps, str):
            dps = [dps]

        discussion_list: List[Dict[str, Any]] = []

        for idx, dp in enumerate(dps, start=1):
            raw_dp = (dp or "").strip()
            if not raw_dp:
                continue

            op = op_map.get(_norm_text(raw_dp))
            numbered_key = f"è®¨è®ºç‚¹{idx}"

            if not op:
                # æ²¡æ‰¾åˆ°å¯¹åº”è§‚ç‚¹ç»“æœï¼Œç•™ä¸ªæ ‡è®°æ–¹ä¾¿æ’æŸ¥
                discussion_list.append({
                    numbered_key: raw_dp,
                    "ç©å®¶è§‚ç‚¹": [],
                    "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": [],
                    "_missing_opinion": True,   # è°ƒè¯•ç”¨
                })
                continue

            examples = op.get("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹") or []
            if not isinstance(examples, list):
                examples = [str(examples)]

            viewpoints = op.get("ç©å®¶è§‚ç‚¹") or []
            if not isinstance(viewpoints, list):
                viewpoints = [str(viewpoints)]

            discussion_list.append({
                numbered_key: raw_dp,
                "ç©å®¶è§‚ç‚¹": viewpoints,
                "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": examples,
            })

        # 3) ç»„è£…è¾“å‡ºï¼šç§»é™¤é¡¶å±‚â€œè®¨è®ºç‚¹/ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹â€ï¼Œåªä¿ç•™è®¨è®ºç‚¹åˆ—è¡¨
        new_row = dict(row)
        new_row.pop("è®¨è®ºç‚¹", None)
        new_row.pop("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹", None)
        new_row["è®¨è®ºç‚¹åˆ—è¡¨"] = discussion_list

        merged.append(new_row)

    return merged

#####æ—¶é—´è½´æ ¡æ­£###
import re
from typing import Any, Dict, List, Optional, Tuple

def _pick_time_axis_value(c: Dict[str, Any]) -> Tuple[Optional[str], Optional[str]]:
    """
    åœ¨ dict c ä¸­æŒ‘ä¸€ä¸ªæœ€åƒâ€œæ—¶é—´è½´â€çš„å­—æ®µå€¼ã€‚
    è¿”å› (value, source_key)
    """
    # å€™é€‰ key çš„ä¼˜å…ˆçº§ï¼šè¶Šé å‰è¶Šåƒæ—¶é—´è½´
    priority_patterns = [
        r"^æ—¶é—´è½´$", r"æ—¶é—´è½´",
        r"æ—¶é—´æ®µ", r"æ—¶é—´èŒƒå›´", r"æ—¶æ®µ", r"èŒƒå›´",
        r"time[_\s-]*axis", r"axis",
        r"æè½´", r"æ—¶é—´æ"
    ]

    # 1) å…ˆæŒ‰ä¼˜å…ˆçº§æ‰¾ key
    keys = list(c.keys())
    for pat in priority_patterns:
        for k in keys:
            if re.search(pat, str(k), flags=re.IGNORECASE):
                v = c.get(k)
                if isinstance(v, str) and v.strip():
                    return v.strip(), str(k)
                # æœ‰äº›æ¨¡å‹ä¼šç»™ list[str]ï¼Œæ‹¼ä¸€ä¸‹
                if isinstance(v, list) and v:
                    s = "ã€".join([str(x).strip() for x in v if str(x).strip()])
                    if s:
                        return s, str(k)

    # 2) å†æ‰«æ‰€æœ‰ keyï¼šåªè¦ key å«â€œæ—¶é—´/æ—¶æ®µ/èŒƒå›´/time/axis/æè½´â€
    for k in keys:
        kk = str(k)
        if any(x in kk.lower() for x in ["æ—¶é—´", "æ—¶æ®µ", "èŒƒå›´", "time", "axis", "æè½´"]):
            v = c.get(k)
            if isinstance(v, str) and v.strip():
                return v.strip(), str(k)
            if isinstance(v, list) and v:
                s = "ã€".join([str(x).strip() for x in v if str(x).strip()])
                if s:
                    return s, str(k)

    return None, None


def ensure_time_axis_key(c: Dict[str, Any], verbose: bool = True) -> bool:
    """
    å¦‚æœ c ä¸­æ²¡æœ‰ key == 'æ—¶é—´è½´' æˆ–å…¶å€¼ä¸ºç©ºï¼Œå°±ç”¨åˆ«åå­—æ®µè¦†ç›–ç”Ÿæˆ c['æ—¶é—´è½´']ã€‚
    è¿”å›ï¼šæ˜¯å¦å‘ç”Ÿäº†ä¿®å¤ï¼ˆTrue/Falseï¼‰
    """
    # å·²ç»æœ‰æ—¶é—´è½´ä¸”éç©º -> ä¸åŠ¨
    ta = c.get("æ—¶é—´è½´")
    if isinstance(ta, str) and ta.strip():
        return False

    # æ²¡æœ‰â€œæ—¶é—´è½´â€ä¸‰ä¸ªå­—çš„ keyï¼ˆä½ è¦æ±‚ï¼šåªè¦æ²¡å‡ºç°æ—¶é—´è½´ä¸‰ä¸ªå­—å°±è¦†ç›–ï¼‰
    has_time_axis_key = any("æ—¶é—´è½´" in str(k) for k in c.keys())
    if not has_time_axis_key:
        v, src = _pick_time_axis_value(c)
        if v:
            c["æ—¶é—´è½´"] = v
            c["_time_axis_from"] = src  # è®°å½•æ¥æºï¼Œæ–¹ä¾¿æ’æŸ¥
            if verbose:
                print(f"âœ… å·²è¡¥é½ æ—¶é—´è½´: {v}  (from: {src}) | èšåˆè¯é¢˜ç°‡={c.get('èšåˆè¯é¢˜ç°‡')}")
            return True
        else:
            if verbose:
                print(f"âš  æ‰¾ä¸åˆ°å¯ç”¨æ—¶é—´å­—æ®µæ¥è¡¥é½ æ—¶é—´è½´ | èšåˆè¯é¢˜ç°‡={c.get('èšåˆè¯é¢˜ç°‡')} keys={list(c.keys())}")
            return False

    # æœ‰ key å«â€œæ—¶é—´è½´â€ä½†å€¼ä¸ºç©ºï¼šä¹Ÿå°è¯•ç”¨åˆ«åè¡¥é½
    v, src = _pick_time_axis_value(c)
    if v:
        c["æ—¶é—´è½´"] = v
        c["_time_axis_from"] = src
        if verbose:
            print(f"âœ… å·²ä¿®å¤ ç©ºæ—¶é—´è½´: {v} (from: {src}) | èšåˆè¯é¢˜ç°‡={c.get('èšåˆè¯é¢˜ç°‡')}")
        return True

    if verbose:
        print(f"âš  æœ‰æ—¶é—´è½´keyä½†æ— æ³•ä¿®å¤ï¼ˆæ— å¯ç”¨å€¼ï¼‰| èšåˆè¯é¢˜ç°‡={c.get('èšåˆè¯é¢˜ç°‡')}")
    return False

def ensure_subcluster_list_key(c: Dict[str, Any]) -> bool:
    if isinstance(c.get("å­è¯é¢˜ç°‡åˆ—è¡¨"), list) and c["å­è¯é¢˜ç°‡åˆ—è¡¨"]:
        return False

    candidates = ["å­è¯é¢˜ç°‡åˆ—è¡¨", "å­è¯æç°‡åˆ—è¡¨", "å­è¯é¢˜ç°‡IDåˆ—è¡¨", "å­ç°‡åˆ—è¡¨", "å­è¯é¢˜ç°‡"]
    for k in candidates:
        v = c.get(k)
        if v is None:
            continue
        if isinstance(v, list):
            c["å­è¯é¢˜ç°‡åˆ—è¡¨"] = v
            c["_sub_list_from"] = k
            return True
        if isinstance(v, str) and v.strip():
            c["å­è¯é¢˜ç°‡åˆ—è¡¨"] = [x.strip() for x in v.split("ã€") if x.strip()]
            c["_sub_list_from"] = k
            return True
    return False
