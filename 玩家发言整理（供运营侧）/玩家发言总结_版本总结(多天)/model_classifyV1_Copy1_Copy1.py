
from __future__ import annotations
import json, time, typing as T
import pandas as pd
import requests
import math
from pathlib import Path
from typing import List, Dict, Any,Optional
import re, json, unicodedata
from datetime import datetime
import json
from json import JSONDecodeError
# --- openpyxl æ ·å¼/å·¥å…· ---
from openpyxl import Workbook, load_workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, NamedStyle

# --- docx æ ·å¼/å·¥å…· ---
from docx.oxml import OxmlElement
from docx import Document
from docx.shared import Pt
from docx.oxml.ns import qn


################æ¨¡å‹è°ƒç”¨ï¼Œå‡ºç»“æœ###################

def load_system_prompt(path: Path) -> str:
    if not path.exists():
        raise FileNotFoundError(f"Prompt file not found: {path}")
    return path.read_text(encoding="utf-8")

def build_user_prompt_filter(json_lines: T.List[str]) -> str:
    # æ¨¡å‹#1ï¼šç­›æ‰éæ¸¸æˆç›¸å…³ï¼Œåªè¾“å‡ºç›¸å…³ JSON è¡Œï¼ˆåŸæ ·ï¼‰
    return (
        "ä»¥ä¸‹æ˜¯è‹¥å¹²ç©å®¶/å®¢æœ/ç ”å‘çš„å‘è¨€è®°å½•ï¼Œè¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­è§„åˆ™ï¼Œ"
        "åˆ¤æ–­å“ªäº›æ˜¯ã€ä¸æ¸¸æˆå†…å®¹ç›¸å…³ã€‘çš„å‘è¨€ï¼Œä¿ç•™è¿™äº› JSON è¡Œï¼Œä¸ç›¸å…³çš„å¿½ç•¥ã€‚"
        "è¯·ä»…è¾“å‡ºã€ç›¸å…³å‘è¨€çš„åŸå§‹ JSON è¡Œã€‘ï¼Œä¸¥æ ¼ä¿æŒæ ¼å¼ä¸å˜ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + "\n".join(json_lines)
    )

def build_user_prompt_clsuter(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )

def build_user_prompt_cluster_agg(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )
def build_user_prompt_version_agg(jsonl_block: str) -> str:
    return (
        "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·å…ˆå®Œæ•´é˜…è¯»å…¨éƒ¨è¾“å…¥ï¼Œç„¶åæŒ‰ç³»ç»Ÿæç¤ºä¸­çš„è¯é¢˜ç°‡è§„åˆ™è¿›è¡Œåˆ’åˆ†ã€‚\n"
        "ã€è¾“å‡ºè¦æ±‚ã€‘åªè¾“å‡ºè‹¥å¹² JSON å¯¹è±¡ï¼Œæ¯ä¸ªè¯é¢˜ç°‡ä¸€ä¸ª JSONï¼›"
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )

from typing import List, Dict, Any
import json

def build_user_prompt_subcluster_opinion(
    topic_id: str,
    discussion_point: str,
    dialogs: List[Dict[str, Any]],
) -> str:
    lines = []

    meta = {
        "è¯é¢˜ç°‡ID": topic_id,
        "è®¨è®ºç‚¹": discussion_point,
    }
    lines.append(json.dumps(meta, ensure_ascii=False))

    for row in dialogs:
        lines.append(json.dumps(row, ensure_ascii=False))

    jsonl_block = "\n".join(lines)

    return (
        "ç¦æ­¢ä½¿ç”¨ ```json æˆ– ``` ç­‰ Markdown ä»£ç å—ï¼Œç¦æ­¢è¾“å‡ºè§£é‡Šæ–‡å­—ã€‚\n\n"
        "ã€è¾“å…¥ã€‘æœ¬æ¬¡æ‰€æœ‰æ•°æ®ï¼ˆJSONLï¼Œç¬¬ä¸€è¡Œä¸ºè¯é¢˜ç°‡ä¿¡æ¯ï¼Œå…¶ä½™ä¸ºå‘è¨€ï¼‰ï¼š\n"
        + jsonl_block +
        "\n\nä»…æ ¹æ®ä»¥ä¸Šå†…å®¹ï¼Œå¹¶éµå¾ªç³»ç»Ÿæç¤ºè¯ä¸­çš„è§„åˆ™å’Œè¾“å‡ºæ ¼å¼ï¼Œ"
        "ç›´æ¥è¾“å‡º 1 ä¸ª JSON å¯¹è±¡ä½œä¸ºæœ€ç»ˆç»“æœã€‚"
    )


def call_ark_chat_completions(
    api_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    temperature: float = 0.3,
    max_tokens: int = 32700,
    timeout: int = 600,
    retries: int = 2,
) -> str:
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    last_err = None
    for attempt in range(retries + 1):
        try:
            resp = requests.post(api_url, headers=headers, json=payload, timeout=timeout)
            if resp.status_code != 200:
                raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            last_err = e
            time.sleep(1.2 * (attempt + 1))
    raise RuntimeError(f"Ark API è°ƒç”¨å¤±è´¥: {last_err}")

def extract_valid_json_lines(text: str) -> T.List[str]:
    """
    æŠŠæ¨¡å‹è¾“å‡ºé‡Œçš„çº¯ JSON è¡Œæå–å‡ºæ¥ï¼ˆé²æ£’å¤„ç†ï¼‰ï¼š
    - é€è¡Œåˆ¤æ–­ï¼šä»¥ { å¼€å¤´ ä¸” ä»¥ } ç»“å°¾ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ª JSON å¯¹è±¡è¡Œ
    - ä¹Ÿèƒ½å®¹å¿å‰åå¤šä½™ç©ºè¡Œæˆ–è§£é‡Šæ–‡å­—ï¼ˆä¼šè¢«å¿½ç•¥ï¼‰
    """
    lines = []
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith("{") and s.endswith("}"):
            lines.append(s)
    return lines

def add_index_to_jsonl_lines(jsonl_lines):
    """
    ç»™åŸå§‹ jsonl æ¯æ¡å‘è¨€åŠ ä¸Šä¸€ä¸ªå”¯ä¸€è¡Œå·å­—æ®µ `_idx`ï¼Œ
    è¿”å›æ–°çš„ List[str]ï¼Œæ¯ä¸ªå…ƒç´ ä»ç„¶æ˜¯ä¸€è¡Œ JSON å­—ç¬¦ä¸²ã€‚
    """
    new_lines = []
    for idx, line in enumerate(jsonl_lines, start=1):
        line = line.strip()
        if not line:
            continue
        obj = json.loads(line)
        obj["_idx"] = idx  # æ–°å¢è¡Œå·
        new_lines.append(json.dumps(obj, ensure_ascii=False))
    return new_lines


def count_output_filter_stats(output_filter: str):
    """
    ç»Ÿè®¡æ¨¡å‹#1 è¾“å‡ºä¸­çš„ï¼š
    - æ€»è¡Œæ•° total_linesï¼ˆç©å®¶+å®¢æœ+ç ”å‘ï¼‰
    - ç©å®¶å‘è¨€è¡Œæ•° player_linesï¼ˆç®€å•ç”¨ å‘è¨€äººID é‡Œçš„å…³é”®è¯åŒºåˆ†ï¼‰
    """
    total_lines = 0
    player_lines = 0

    for line in output_filter.splitlines():
        line = line.strip()
        if not line:
            continue
        total_lines += 1

        try:
            obj = json.loads(line)
        except Exception:
            # å¦‚æœè¿™ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±åªèƒ½ç®—è¿› total_linesï¼Œæ— æ³•ç»†åˆ†
            continue

        speaker = (
            obj.get("ç©å®¶ID")
            or obj.get("å‘è¨€äººID")
            or obj.get("è§’è‰²ID")
            or ""
        )

        # ç®€å•æ’é™¤å®¢æœ/GM/å®˜æ–¹/è¿è¥/ç ”å‘ï¼Œå…¶ä½™è§†ä¸ºç©å®¶
        if any(key in str(speaker) for key in ["å®¢æœ", "GM", "å®˜æ–¹", "è¿è¥", "ç ”å‘"]):
            continue

        player_lines += 1

    return total_lines, player_lines


def get_covered_indices_from_cluster_output(output_cluster: str):
    """
    ä»æ¨¡å‹#2 çš„è‡ªç„¶è¯­è¨€è¾“å‡ºä¸­ï¼Œè§£ææ‰€æœ‰ â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[...æ•°å­—...]â€ é‡Œçš„æ•°å­—ï¼Œ
    æ”¶é›†æˆä¸€ä¸ª set è¿”å›ã€‚

    é€‚é…ç±»ä¼¼æ ¼å¼ï¼ˆä½ ç°åœ¨çš„è¾“å‡ºå°±æ˜¯è¿™æ ·ï¼‰ï¼š
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[88, 90]
        å‘è¨€è¡Œå·åˆ—è¡¨ï¼š[124,125,126]

    æ”¯æŒï¼š
    - ä¸­æ–‡/è‹±æ–‡å†’å·ï¼ˆ: / ï¼šï¼‰
    - ä¸­æ–‡/è‹±æ–‡ä¸­æ‹¬å·ï¼ˆ[] / ã€ã€‘ï¼‰
    - é€—å·å¯ä¸º , æˆ– ï¼Œ
    """
    covered = set()

    # æ­£åˆ™åŒ¹é… â€œå‘è¨€è¡Œå·åˆ—è¡¨ï¼š[1, 2, 3]â€
    pattern = r"å‘è¨€è¡Œå·åˆ—è¡¨[:ï¼š]\s*[\[\ã€]([0-9,\sï¼Œ]+)[\]\ã€‘]"

    for m in re.finditer(pattern, output_cluster):
        nums_str = m.group(1)  # é‡Œé¢æ˜¯ç±»ä¼¼ "1, 2, 3" æˆ– "124,125,126"
        # æ›¿æ¢ä¸­æ–‡é€—å·ï¼ŒæŒ‰é€—å·åˆ‡åˆ†
        nums_str = nums_str.replace("ï¼Œ", ",")
        for part in nums_str.split(","):
            p = part.strip()
            if not p:
                continue
            try:
                covered.add(int(p))
            except ValueError:
                continue

    return covered


################################ä¿®è¡¥bugè¯æç°‡åˆ’åˆ†è§£æ#########################

def _normalize_json_text(text: str) -> str:
    """
    å¯¹æ¨¡å‹è¾“å‡ºåšä¸€äº›å°ä¿®å¤ï¼Œä»¥ä¾¿ json.loads æ­£å¸¸è§£æï¼š
    1ï¼‰å»æ‰æ•´æ®µæœ«å°¾å¤šä½™çš„é€—å·ï¼š{"a":1},
    2ï¼‰å»æ‰ å±æ€§/å…ƒç´  åç´§è·Ÿ } æˆ– ] çš„éæ³•é€—å·ï¼š
        ä¾‹å¦‚ï¼š
            "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "xxx",
        }
        â†’  "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "xxx"
           }
    """
    text = text.rstrip()

    # 1) æ•´ä¸ªå¯¹è±¡æœ«å°¾å¤šæ‰“ä¸€é¢—é€—å·ï¼š{"a":1},
    if text.endswith(","):
        text = text[:-1].rstrip()

    # 2) å±æ€§/å…ƒç´ åç›´æ¥è·Ÿ } æˆ– ] çš„é€—å·ï¼š
    #    æŠŠ ",\n  }" æˆ– ",\n]" å˜æˆ "\n  }" æˆ– "\n]"
    text = re.sub(r",\s*(\n\s*[}\]])", r"\1", text)

    return text

def parse_model2_output_to_json_list(output_cluster: str, batch_idx: int = 0) -> List[Dict[str, Any]]:
    """
    æŠŠæ¨¡å‹#2çš„åŸå§‹è¾“å‡ºè§£ææˆ list[dict]ï¼š
    - æ”¯æŒä¸€è¡Œä¸€ä¸ª JSONï¼š
        {"è¯é¢˜ç°‡1": "...", "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "..."}
    - ä¹Ÿæ”¯æŒå¤šè¡Œæ¼‚äº® JSONï¼š
        {
          "è¯é¢˜ç°‡1": "...",
          "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶": "..."
        }
    - è‡ªåŠ¨å¿½ç•¥è§£é‡Šæ€§æ–‡å­—ã€markdown åˆ—è¡¨ "- {...}"
    - è°ƒç”¨ _normalize_json_text ä¿®å¤å°¾é€—å·ç­‰æ ¼å¼é—®é¢˜
    - è§£æå¤±è´¥æ—¶æ‰“å°å®Œæ•´å¯¹è±¡åŸæ–‡ï¼Œä½†ä¸ä¸­æ–­æ•´æ‰¹
    """
    objs: List[Dict[str, Any]] = []
    cur_lines: List[str] = []
    depth = 0  # å½“å‰å¤§æ‹¬å·åµŒå¥—å±‚æ•°

    lines = output_cluster.strip().splitlines()

    for idx, raw in enumerate(lines, start=1):
        line = (raw or "").rstrip()
        if not line.strip():
            continue

        # å»æ‰ markdown åˆ—è¡¨å‰ç¼€ï¼ˆå¸¸è§æ ¼å¼ï¼š"- {...}"ï¼‰
        if line.lstrip().startswith("- "):
            line = line.lstrip()[2:].lstrip()

        # å½“å‰è¿˜æ²¡è¿›å…¥ä»»ä½• JSON å¯¹è±¡ï¼Œå¹¶ä¸”è¿™ä¸€è¡Œä¹Ÿçœ‹ä¸åˆ° "{"
        if depth == 0 and "{" not in line:
            # å¤§æ¦‚ç‡æ˜¯è§£é‡Šæ€§æ–‡å­—ï¼Œæ¯”å¦‚ "ä»¥ä¸‹æ˜¯è¯é¢˜ç°‡..."
            # å¦‚æœæƒ³çœ‹ï¼Œå¯æ‰“å¼€ä¸‹é¢è¿™è¡Œï¼š
            # tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] â© è·³è¿‡éJSONè¡Œ #{idx}: {line[:80]}")
            continue

        # ç»Ÿè®¡è¿™ä¸€è¡Œçš„å¤§æ‹¬å·æ•°é‡
        open_cnt = line.count("{")
        close_cnt = line.count("}")

        if depth == 0:
            # åˆšåˆšè¿›å…¥ä¸€ä¸ªæ–°çš„å¯¹è±¡
            cur_lines = [line]
            depth = open_cnt - close_cnt

            # depth <= 0 è¯´æ˜è¿™æ˜¯â€œå•è¡Œå¯¹è±¡â€ï¼š{"a":1} æˆ– {"a":1},
            if depth <= 0:
                text = "\n".join(cur_lines)
                text = _normalize_json_text(text)
                try:
                    obj = json.loads(text)
                except JSONDecodeError as e:
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] âŒ JSONè§£æå¤±è´¥ï¼ˆè¡Œ#{idx}é™„è¿‘ï¼‰ï¼š{e}")
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] è¯¥å¯¹è±¡åŸæ–‡ï¼š{text}")
                else:
                    # ç»Ÿä¸€ "è¯é¢˜ç°‡1"/"è¯é¢˜ç°‡2"... -> "è¯é¢˜ç°‡"
                    for key in list(obj.keys()):
                        if key.startswith("è¯é¢˜ç°‡") and key != "è¯é¢˜ç°‡":
                            obj["è¯é¢˜ç°‡"] = obj.pop(key)
                    objs.append(obj)
                cur_lines = []
                depth = 0
        else:
            # å·²ç»åœ¨ä¸€ä¸ª JSON å¯¹è±¡å†…éƒ¨ï¼ˆå¤šè¡Œåœºæ™¯ï¼‰
            cur_lines.append(line)
            depth += open_cnt - close_cnt

            # depth å›åˆ° 0ï¼Œè¡¨ç¤ºä¸€ä¸ªå¯¹è±¡ç»“æŸ
            if depth <= 0:
                text = "\n".join(cur_lines)
                text = _normalize_json_text(text)

                try:
                    obj = json.loads(text)
                except JSONDecodeError as e:
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] âŒ JSONè§£æå¤±è´¥ï¼ˆè¡Œ#{idx}é™„è¿‘ï¼‰ï¼š{e}")
                    tqdm.write(f"[æ‰¹æ¬¡ {batch_idx}] è¯¥å¯¹è±¡åŸæ–‡ï¼š{text}")
                else:
                    for key in list(obj.keys()):
                        if key.startswith("è¯é¢˜ç°‡") and key != "è¯é¢˜ç°‡":
                            obj["è¯é¢˜ç°‡"] = obj.pop(key)
                    objs.append(obj)

                cur_lines = []
                depth = 0

    return objs


def parse_jsonl_text_safe(text: str, label: str = "æ¨¡å‹#3èšåˆè¾“å‡º") -> List[Dict[str, Any]]:
    """
    å°è¯•æŠŠ text æŒ‰è¡Œè§£æä¸º JSON å¯¹è±¡ï¼š
    - æ¯è¡Œç‹¬ç«‹å°è¯• json.loads
    - è§£æå‰å…ˆå¯¹â€œæè½´â€ç­‰å¸¸è§é”™è¯¯åšä¸€æ¬¡æ­£åˆ™ä¿®å¤
    - è§£æå¤±è´¥æ—¶ä¸ä¼šæŠ›å¼‚å¸¸ï¼Œè€Œæ˜¯æ‰“å°å‡ºå…·ä½“è¡Œå·å’ŒåŸæ–‡ï¼Œæ–¹ä¾¿æ’æŸ¥
    """
    objs: List[Dict[str, Any]] = []
    lines = text.strip().splitlines()

    for idx, raw in enumerate(lines, start=1):
        s = (raw or "").strip()
        if not s:
            continue

        # è·³è¿‡ ```json / ``` ç­‰ä»£ç å—æ ‡è®°
        if s.startswith("```"):
            continue

        # â­ å…ˆä¿®å¤â€œæè½´â€ç›¸å…³æ ¼å¼é”™è¯¯
        s_fixed = fix_model3_line_extreme_axis(s)

        try:
            obj = json.loads(s_fixed)
        except JSONDecodeError as e:
            print(f"[{label}] âŒ JSONè§£æå¤±è´¥ï¼šè¡Œ#{idx} -> {e}")
            print(f"[{label}] è¯¥è¡ŒåŸæ–‡(ä¿®å¤å)ï¼š{s_fixed}")
            continue

        objs.append(obj)

    return objs

#################################è¯æç°‡å”¯ä¸€id#################################

###1. clusteridçš„æ—¥æœŸæ‰’å–##
def infer_date_for_batch(
    cluster_json_list: List[Dict[str, Any]],
    batch_lines: List[str]
) -> str:
    """
    ä¼˜å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜ä¸­æŠ½å–æ—¥æœŸï¼ˆå½¢å¦‚ï¼šxxxxï¼ˆ2025-12-02 16:30:01-17:42:41ï¼‰ï¼‰
    è‹¥å¤±è´¥ï¼Œåˆ™å›é€€åˆ°åŸå§‹å‘è¨€ä¸­çš„ `å‘è¨€æ—¥æœŸ` / `æ—¥æœŸ` å­—æ®µã€‚
    """
    # 1ï¼‰å…ˆä»è¯é¢˜ç°‡æ ‡é¢˜é‡Œæ‰¾ YYYY-MM-DD
    for obj in cluster_json_list:
        title = str(
            obj.get("è¯é¢˜ç°‡")
            or obj.get("èšåˆè¯é¢˜ç°‡")
            or ""
        )
        m = re.search(r"(\d{4}-\d{2}-\d{2})", title)
        if m:
            return m.group(1)

    # 2ï¼‰å¦‚æœæ ‡é¢˜é‡Œæ²¡æœ‰ï¼Œå°±ä»åŸå§‹å‘è¨€é‡Œæ‹¿
    for line in batch_lines:
        line = line.strip()
        if not line:
            continue
        try:
            msg = json.loads(line)
        except Exception:
            continue

        date_str = msg.get("å‘è¨€æ—¥æœŸ") or msg.get("æ—¥æœŸ")
        if date_str:
            return date_str

    # 3ï¼‰å†ä¸è¡Œå°±æŠ¥é”™ï¼Œè®©ä½ æ„è¯†åˆ°æ•°æ®æ ¼å¼æœ‰é—®é¢˜
    raise ValueError("æ— æ³•ä»è¯é¢˜ç°‡æ ‡é¢˜æˆ–åŸå§‹å‘è¨€ä¸­æ¨æ–­å‡ºæ—¥æœŸï¼Œè¯·æ£€æŸ¥æ•°æ®æ ¼å¼ã€‚")

##2.ç”Ÿæˆè¯æç°‡clusterid##
def assign_global_cluster_ids(cluster_list, date_str, batch_id):
    """
    ä¸ºæ¯ä¸ªè¯é¢˜ç°‡ç”Ÿæˆå…¨å±€å”¯ä¸€IDå­—æ®µ `_cluster_id`
    æ ¼å¼ï¼šYYYY-MM-DD_BX_XXï¼Œå¦‚ 2025-11-20_B2_03
    """
    for idx, cluster in enumerate(cluster_list, start=1):
        cluster["_cluster_id"] = f"{date_str}_{batch_id}_{idx:02d}"
    return cluster_list

#################################èšåˆæ¯å¤©çš„è¯æç°‡åˆ†æ‰¹è¾“å‡º#################################
def aggregate_cluster_outputs(batch_outputs: List[str]) -> str:
    all_lines = []

    for batch_id, text in enumerate(batch_outputs, start=1):
        if not text:
            continue

        for line in text.strip().splitlines():
            line = line.strip()
            if not line:
                continue

            try:
                obj = json.loads(line)
            except Exception:
                # å¦‚æœæœ‰ä¸€è¡Œä¸æ˜¯åˆæ³• JSONï¼Œå°±è·³è¿‡ï¼ˆä¹Ÿå¯ä»¥æ”¹æˆ raiseï¼Œçœ‹ä½ éœ€æ±‚ï¼‰
                continue

            clean_line = json.dumps(obj, ensure_ascii=False)
            all_lines.append(clean_line)

    # èšåˆä¸ºä¸€ä¸ªå¤§çš„ JSONL å­—ç¬¦ä¸²
    return "\n".join(all_lines)

################èšåˆä¿®å¤â€œæâ€#############################
def fix_model3_line_extreme_axis(s: str) -> str:
    """
    ä¸“é—¨ä¿®å¤æ¨¡å‹#3è¾“å‡ºä¸­â€œæè½´â€ç›¸å…³çš„åæ ¼å¼ï¼š
    é’ˆå¯¹ä»¥ä¸‹å‡ ç§æƒ…å†µï¼š
    1) "æ—¥æœŸ": "æè½´": "2025-12-06"  =>  "æ—¥æœŸ": "2025-12-06"
    2) "æ—¶é—´è½´": "æè½´": "22:30:57-22:33:57"  =>  "æ—¶é—´è½´": "22:30:57-22:33:57"
    3) "æ—¶é—´è½´": "22:34:24-æè½´": "22:38:33" => "æ—¶é—´è½´": "22:34:24-22:38:33"
    """
    s = re.sub(r'"æ—¥æœŸ"\s*:\s*"æè½´"\s*:\s*"([^"]+)"', r'"æ—¥æœŸ": "\1"', s)
    s = re.sub(r'"æ—¶é—´è½´"\s*:\s*"æè½´"\s*:\s*"([^"]+)"', r'"æ—¶é—´è½´": "\1"', s)
    s = re.sub(r'"æ—¶é—´è½´"\s*:\s*"([^"]*?)-æè½´"\s*:\s*"([^"]+)"', r'"æ—¶é—´è½´": "\1-\2"', s)
    return s

def _strip_fences(s: str) -> str:
    s = (s or "").strip()
    s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
    s = re.sub(r"\s*```$", "", s)
    return s.strip()

def _as_list(v):
    if v is None:
        return []
    if isinstance(v, list):
        return v
    return [v]

def _merge_time_axes(time_axes: list[str]) -> str:
    seen, out = set(), []
    for t in time_axes:
        t = (t or "").strip()
        if not t or t in seen:
            continue
        seen.add(t)
        out.append(t)
    return "ã€".join(out)

def normalize_model3_clusters(output_text: str, parsed_subclusters: list[dict]) -> list[dict]:
    # 0) å…ˆè®©å­ç°‡å…·å¤‡ æ—¥æœŸ/æ—¶é—´è½´
    parsed_subclusters = enrich_subclusters_with_datetime(parsed_subclusters)

    # 1) å»ºç´¢å¼•ï¼š_cluster_id -> å­ç°‡(å«æ—¥æœŸ/æ—¶é—´è½´)
    sub_by_id = {}
    for sc in parsed_subclusters:
        cid = (sc.get("_cluster_id") or "").strip()
        if cid:
            sub_by_id[cid] = sc

    # 2) é€è¡Œä¿®å¤â€œæè½´â€æ±¡æŸ“å† parse
    raw = _strip_fences(output_text)
    lines = [ln.strip() for ln in raw.splitlines() if ln.strip()]
    lines = [fix_model3_line_extreme_axis(ln) for ln in lines]

    objs = []
    for ln in lines:
        try:
            objs.append(json.loads(ln))
        except Exception:
            continue

    normalized = []
    for obj in objs:
        topic = (obj.get("è¯é¢˜ç°‡") or obj.get("èšåˆè¯é¢˜ç°‡") or obj.get("è¯é¢˜ç°‡3") or "").strip()

        sub_list = (
            obj.get("å­è¯é¢˜ç°‡åˆ—è¡¨")
            or obj.get("å­è¯é¢˜ç°‡")
            or obj.get("å­è¯é¢˜ç°‡idåˆ—è¡¨")
            or obj.get("å­è¯é¢˜ç°‡IDåˆ—è¡¨")
            or []
        )
        sub_list = [str(x).strip() for x in _as_list(sub_list) if str(x).strip()]

        date = (obj.get("æ—¥æœŸ") or "").strip()
        time_axis = (obj.get("æ—¶é—´è½´") or "").strip()

        # 3) ç¼ºå¤±å›å¡«ï¼šä»å­ç°‡å–æ—¥æœŸ/æ—¶é—´è½´
        if (not date) or (not time_axis):
            sub_dates, sub_axes = [], []
            for cid in sub_list:
                sc = sub_by_id.get(cid)
                if not sc:
                    continue
                d = (sc.get("æ—¥æœŸ") or "").strip()
                ta = (sc.get("æ—¶é—´è½´") or "").strip()
                if d:
                    sub_dates.append(d)
                if ta:
                    sub_axes.append(ta)
            if not date and sub_dates:
                date = sub_dates[0]
            if not time_axis and sub_axes:
                time_axis = _merge_time_axes(sub_axes)

        # 4) å¼ºåˆ¶ schemaï¼šç¼ºå…³é”®å­—æ®µå°±ä¸¢å¼ƒï¼Œé¿å…åç»­ split(None) å´©
        if not topic or not sub_list or not date or not time_axis:
            continue

        normalized.append({
            "è¯é¢˜ç°‡": topic,
            "å­è¯é¢˜ç°‡åˆ—è¡¨": sub_list,
            "æ—¥æœŸ": date,
            "æ—¶é—´è½´": time_axis
        })

    return normalized

######åŒ¹é…all_CLUSTERæ—¶é—´

# åŒ¹é…ï¼šâ€¦â€¦ï¼ˆ2025-12-07 22:40:36-22:41:27ï¼‰ æˆ– â€¦â€¦(2025-12-07 22:40:36-22:41:27)
SUB_TIME_RE = re.compile(
    r"[ï¼ˆ(]\s*(\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2}:\d{2}-\d{2}:\d{2}:\d{2})\s*[ï¼‰)]"
)

def enrich_subclusters_with_datetime(parsed_subclusters: list[dict]) -> list[dict]:
    """
    ç»™æ¯æ¡å­ç°‡è¡¥å­—æ®µï¼š
    - æ—¥æœŸ: YYYY-MM-DD
    - æ—¶é—´è½´: HH:MM:SS-HH:MM:SS
    ä» å­ç°‡["è¯é¢˜ç°‡"] å°¾éƒ¨æ‹¬å·é‡Œæå–ã€‚
    """
    out = []
    for sc in parsed_subclusters:
        sc2 = dict(sc)
        title = sc2.get("è¯é¢˜ç°‡") or ""
        m = SUB_TIME_RE.search(title)
        if m:
            sc2["æ—¥æœŸ"] = m.group(1)
            sc2["æ—¶é—´è½´"] = m.group(2)
        else:
            # å…œåº•ï¼šæ—¥æœŸå¯ä» _cluster_id é‡Œå–åˆ°ï¼ˆæ—¶é—´è½´å–ä¸åˆ°ï¼‰
            cid = (sc2.get("_cluster_id") or "").strip()
            if cid and re.match(r"^\d{4}-\d{2}-\d{2}_", cid) is None:
                # ä½ çš„ _cluster_id æ˜¯ 2025-12-07_B4_06ï¼Œå‰10ä½å°±æ˜¯æ—¥æœŸ
                sc2["æ—¥æœŸ"] = cid[:10]
            sc2.setdefault("æ—¶é—´è½´", "")
        out.append(sc2)
    return out
################top5ç­›é€‰#################################


# -------------------------------
# ğŸ”¹ 1. åŒ¹é…åŸå§‹å‘è¨€
# -------------------------------
from datetime import datetime
import json
from typing import List, Dict

def parse_time_range(date_str, range_str):
    # å¦‚æœåªæœ‰ä¸€ä¸ªæ—¶é—´ç‚¹è€Œä¸æ˜¯èŒƒå›´ï¼Œè·³è¿‡
    if "-" not in range_str:
        return None, None
    start_str, end_str = range_str.split("-")
    start_dt = datetime.strptime(f"{date_str} {start_str.strip()}", "%Y-%m-%d %H:%M:%S")
    end_dt = datetime.strptime(f"{date_str} {end_str.strip()}", "%Y-%m-%d %H:%M:%S")
    return start_dt, end_dt


def match_dialogs_by_time(messages, date_str, time_axis_str):
    time_ranges = time_axis_str.split("ã€")
    matched = []
    for tr in time_ranges:
        start, end = parse_time_range(date_str, tr)
        if not start or not end:
            continue  # è·³è¿‡æ— æ³•è§£æçš„æ—¶é—´æ®µ
        for row in messages:
            ts = datetime.strptime(f"{row['å‘è¨€æ—¥æœŸ']} {row['å‘è¨€æ—¶é—´']}", "%Y-%m-%d %H:%M:%S")
            if start <= ts <= end:
                matched.append(row)
    return matched


def extract_cluster_stats(èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict], åŸå§‹å‘è¨€: List[str]) -> List[Dict]:
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    results = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = cluster.get("æ—¶é—´è½´")
        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)
        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        result = {
            "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡"),
            "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
            "å‘è¨€ç©å®¶æ€»æ•°": len(players),
            "å‘è¨€æ€»æ•°": len(matched)
        }
        results.append(result)

    return results

# -------------------------------
# ğŸ”¹ 2. çƒ­åº¦è¯†åˆ«ä¸»å‡½æ•°
# -------------------------------
def compute_heat_score(U: int, M: int) -> float:
    if U == 0 or M == 0:
        return 0.0
    return round(U * math.sqrt(M), 2)

def extract_top5_heat_clusters(èšåˆè¯é¢˜ç°‡åˆ—è¡¨: List[Dict], åŸå§‹å‘è¨€: List[str], top_k=5) -> List[Dict]:
    parsed_msgs = [json.loads(line.strip()) for line in åŸå§‹å‘è¨€ if line.strip()]
    enriched = []

    for cluster in èšåˆè¯é¢˜ç°‡åˆ—è¡¨:
        date = cluster.get("æ—¥æœŸ")
        time_axis = cluster.get("æ—¶é—´è½´")
        matched = match_dialogs_by_time(parsed_msgs, date, time_axis)

        players = {msg.get("ç©å®¶ID") for msg in matched if msg.get("ç©å®¶ID")}
        U = len(players)
        M = len(matched)
        heat = compute_heat_score(U, M)

        enriched.append({
        "èšåˆè¯é¢˜ç°‡": cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥",
        "å­è¯é¢˜ç°‡åˆ—è¡¨": cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨"),
        "æ—¥æœŸ": date,
        "æ—¶é—´è½´": time_axis,
        "å‘è¨€ç©å®¶æ€»æ•°": U,
        "å‘è¨€æ€»æ•°": M,
        "çƒ­åº¦è¯„åˆ†": heat
         })


    # æŒ‰çƒ­åº¦è¯„åˆ†æ’åºï¼Œå– TopK
    enriched.sort(key=lambda x: x["çƒ­åº¦è¯„åˆ†"], reverse=True)
    return enriched[:top_k]

# -------------------------------
# ğŸ”¹ 3. æ·»åŠ è®¨è®ºç‚¹å­—æ®µ
# -------------------------------
def attach_discussion_points(top_clusters: List[Dict], subclusters: List[Dict]) -> List[Dict]:
    """
    å°† top_clusters ä¸­æ¯ä¸ªèšåˆç°‡çš„å­è¯é¢˜ç°‡åˆ—è¡¨ï¼Œä¸ subclusters ä¸­çš„ _cluster_id åŒ¹é…ï¼Œ
    æ‹¼å‡ºæ ¸å¿ƒæœºåˆ¶æè¿°ï¼Œå¹¶æ ¼å¼åŒ–ä¸ºåˆ—ç‚¹ + ç©ºè¡Œã€‚ç§»é™¤å­è¯é¢˜ç°‡åˆ—è¡¨å­—æ®µã€‚
    """
    # æ„å»º _cluster_id â†’ æ ¸å¿ƒæœºåˆ¶ æ˜ å°„
    cluster_mechanism_map = {
        row["_cluster_id"]: row["æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶"]
        for row in subclusters
        if "_cluster_id" in row and "æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶" in row
    }

    result = []
    for cluster in top_clusters:
        ids = cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨", [])
        mechanisms = [cluster_mechanism_map.get(cid) for cid in ids if cid in cluster_mechanism_map]

        # æ ¼å¼åŒ–ä¸ºåˆ—ç‚¹ + ç©ºè¡Œ
        discussion_point = [m for m in mechanisms if m]

        # ä¿®å¤èšåˆè¯é¢˜ç°‡åç§°å­—æ®µä¸ºç©ºçš„é—®é¢˜
        cluster_name = cluster.get("è¯é¢˜ç°‡") or cluster.get("èšåˆè¯é¢˜ç°‡") or "æœªçŸ¥"

        enriched_cluster = {
            "èšåˆè¯é¢˜ç°‡": cluster_name,
            "æ—¥æœŸ": cluster.get("æ—¥æœŸ"),
            "æ—¶é—´è½´": cluster.get("æ—¶é—´è½´"),
            "å‘è¨€ç©å®¶æ€»æ•°": cluster.get("å‘è¨€ç©å®¶æ€»æ•°"),
            "å‘è¨€æ€»æ•°": cluster.get("å‘è¨€æ€»æ•°"),
            "çƒ­åº¦è¯„åˆ†": cluster.get("çƒ­åº¦è¯„åˆ†"),
            "è®¨è®ºç‚¹": discussion_point
        }

        result.append(enriched_cluster)

    return result



####################### å­˜å…¥æ¯æ—¥å‘è¨€ top5 ########################
from pathlib import Path
from typing import List, Dict
import json

####################### å­˜å…¥æ¯æ—¥å‘è¨€ top5 ########################
from pathlib import Path
from typing import List, Dict
import json

def append_daily_top5_to_version_jsonl(
    final_result: List[Dict],
    version_jsonl_path: str = "daily_top5.jsonl",
):
    """
    å°†å½“æ—¥ Top5 è¿½åŠ å†™å…¥æŸä¸ªã€ç‰ˆæœ¬ç´¯è®¡ jsonl æ–‡ä»¶ã€‘ä¸­ã€‚
    
    åŒæ—¶ä¸ºæ¯æ¡è®°å½•è¡¥å……ä¸¤ä¸ªå­—æ®µï¼š
    1ï¼‰_idxï¼šåœ¨ version_jsonl_path ä¸­çš„å…¨å±€é€’å¢è¡Œå·ï¼ˆä»1å¼€å§‹ï¼‰
    2ï¼‰_daily_top_idï¼šå½“å¤©å†…çš„ Top è¯é¢˜ç°‡IDï¼Œå½¢å¼ï¼šYYYY-MM-DD_TXXï¼Œä¾‹å¦‚ 2025-12-03_T01
    
    è¯´æ˜ï¼š
    - å‡è®¾ final_result å†…æ‰€æœ‰è®°å½•çš„ "æ—¥æœŸ" ç›¸åŒï¼ˆå³åŒä¸€å¤©çš„ top5ï¼‰
    - å¦‚æœæ–‡ä»¶ä¸­å·²å­˜åœ¨åŒä¸€æ—¥æœŸçš„æ•°æ®ï¼Œä¼šåœ¨åŸæœ‰åŸºç¡€ä¸Šç»§ç»­ç´¯åŠ  _daily_top_id çš„ç¼–å·
    - ä¸ä¼šä¿®æ”¹ä»»ä½•å·²æœ‰çš„ `_cluster_id` å­—æ®µï¼ˆå­è¯é¢˜ç°‡ä»ç„¶ç”¨å®ƒï¼‰
    """
    if not final_result:
        print("âš  final_result ä¸ºç©ºï¼Œä»Šæ—¥æ—  Top5 å¯å†™å…¥ã€‚")
        return

    # å–å½“å¤©æ—¥æœŸï¼ˆå‡è®¾ final_result åŒä¸€æ‰¹éƒ½æ˜¯åŒä¸€å¤©ï¼‰
    date_str = final_result[0].get("æ—¥æœŸ")
    if not date_str:
        raise ValueError("final_result ä¸­ç¼ºå°‘ 'æ—¥æœŸ' å­—æ®µï¼Œæ— æ³•ç”Ÿæˆ _daily_top_idã€‚")

    path = Path(version_jsonl_path)

    # ---------- 1. è®¡ç®—å½“å‰æ–‡ä»¶ä¸­çš„æœ€å¤§ _idxï¼ˆå…¨å±€è¡Œå·ï¼‰ ----------
    global_max_idx = 0
    # åŒæ—¶ç»Ÿè®¡â€œä»Šå¤©â€å·²ç»æœ‰å¤šå°‘æ¡ï¼Œç”¨äº _daily_top_id ç»­ç¼–å·
    existing_count_for_day = 0

    if path.exists():
        with path.open("r", encoding="utf-8") as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue
                try:
                    obj = json.loads(line)
                except json.JSONDecodeError:
                    continue

                # ç»Ÿè®¡å…¨å±€ _idx
                if isinstance(obj.get("_idx"), int):
                    global_max_idx = max(global_max_idx, obj["_idx"])
                else:
                    # æ—§æ•°æ®æ²¡æœ‰ _idxï¼Œå°±ç®€å•å½“ä½œä¸€è¡Œï¼Œæ–°è¡Œå·å¾€åæ¨
                    global_max_idx += 1

                # ç»Ÿè®¡å½“å¤©çš„æ¡æ•°ï¼ˆç”¨äº _daily_top_idï¼‰
                if obj.get("æ—¥æœŸ") == date_str:
                    existing_count_for_day += 1

    # ---------- 2. è¿½åŠ å†™å…¥ï¼Œå¹¶ä¸ºæ–°è®°å½•ç”Ÿæˆ _idx + _daily_top_id ----------
    mode = "a" if path.exists() else "w"
    with path.open(mode, encoding="utf-8") as f:
        for offset, row in enumerate(final_result, start=1):
            row = dict(row)  # é¿å…ä¿®æ”¹åŸå¯¹è±¡å¼•ç”¨

            # 2.1 å…¨å±€é€’å¢ _idxï¼ˆç‰ˆæœ¬æ‰€æœ‰å¤©å…±ç”¨ä¸€ä¸ªåºå·ï¼‰
            global_max_idx += 1
            row["_idx"] = global_max_idx

            # 2.2 ç”Ÿæˆå½“å¤©çš„ top idï¼ˆä¸ç¢° `_cluster_id`ï¼‰
            # å½“å¤©å·²æœ‰ existing_count_for_day æ¡ï¼Œè¿™æ‰¹ä» +1 å¼€å§‹æ¥ç€æ’
            idx_for_day = existing_count_for_day + offset
            row["_daily_top_id"] = f"{date_str}_T{idx_for_day:02d}"

            f.write(json.dumps(row, ensure_ascii=False) + "\n")

    print(f"âœ… å·²å°†å½“æ—¥ Top5ï¼ˆå« _idx å’Œ _daily_top_idï¼‰è¿½åŠ å†™å…¥: {path}")


def extract_time_axis_from_title(title: str) -> str:
    """
    ä»è¯é¢˜ç°‡æ ‡é¢˜ä¸­æå–æ—¶é—´è½´éƒ¨åˆ†ï¼Œå¦‚ï¼š
    'ç»´æŠ¤è¡¥å¿è¯‰æ±‚è®¨è®ºï¼ˆ2025-11-19 14:21:57-14:34:08ï¼‰'
    æˆ– 'ç»´æŠ¤è¡¥å¿è¯‰æ±‚è®¨è®º(2025-11-19 14:21:57-14:34:08)'
    -> '14:21:57-14:34:08'
    """
    if not title:
        return ""
    title = str(title)

    # æ”¯æŒå…¨è§’/åŠè§’æ‹¬å·ï¼šï¼ˆï¼‰ã€()
    pattern = r"[ï¼ˆ(](\d{4}-\d{2}-\d{2})\s+(\d{2}:\d{2}:\d{2}-\d{2}:\d{2}:\d{2})[ï¼‰)]"
    m = re.search(pattern, title)
    if m:
        # ä½ çš„ match_dialogs_by_time æ˜¯æ—¥æœŸ+æ—¶é—´æ®µåˆ†å¼€ä¼ çš„ï¼Œæ‰€ä»¥åªè¦ HH:MM:SS-HH:MM:SS å³å¯
        return m.group(2)
    return ""
#-------è§£ææ¨¡å‹#4çš„è¾“å‡ºæ–‡æœ¬----------------
def parse_and_normalize_opinion_output(
    opinion_output: str,
    topic_id: str,
    discussion_point: str,
) -> Optional[Dict[str, Any]]:
    """
    è§£ææ¨¡å‹#4è¾“å‡ºï¼Œå¹¶åšè½»åº¦è§„èŒƒåŒ–ã€‚
    å‰æï¼šopinion_output æ˜¯ä¸€ä¸ªå•ç‹¬çš„ JSON å¯¹è±¡å­—ç¬¦ä¸²ã€‚

    è¿”å›ï¼šè§„èŒƒåŒ–åçš„ dictï¼›
         å¦‚æœè§£æå¤±è´¥åˆ™è¿”å› Noneã€‚
    """
    if not opinion_output or not opinion_output.strip():
        return None

    s = opinion_output.strip()

    # 1ï¼‰æœ€ç›´æ¥æƒ…å†µï¼šæ•´æ®µå°±æ˜¯ä¸€ä¸ª JSON å¯¹è±¡
    try:
        obj = json.loads(s)
    except Exception:
        # 2ï¼‰å…œåº•ï¼šå¦‚æœä¸Šæ¸¸å“ªå¤©åˆåœ¨å‰ååŠ äº†è§£é‡Šæ–‡å­—ï¼Œ
        #    å°è¯•æˆªå–ç¬¬ä¸€å¯¹ { ... } ä¸­é—´çš„å†…å®¹å†è§£æä¸€æ¬¡
        start = s.find("{")
        end = s.rfind("}")
        if start == -1 or end == -1 or start >= end:
            return None
        try:
            obj = json.loads(s[start : end + 1])
        except Exception:
            return None

    # â€”â€” ç”¨æˆ‘ä»¬è‡ªå·±ä¼ è¿›æ¥çš„ä¿¡æ¯è¦†ç›–ï¼Œç¡®ä¿å¯¹å¾—ä¸Š top5_results â€”â€” 
    obj["è¯é¢˜ç°‡ID"] = topic_id          # é€šå¸¸æ˜¯ _cluster_idï¼Œæ¯”å¦‚ "2025-12-02_B2_01"
    obj["è®¨è®ºç‚¹"] = discussion_point    # ä¸Šæ¸¸â€œæ ¸å¿ƒå¯¹è±¡/æœºåˆ¶â€å­—æ®µ

    # â€”â€” ç»Ÿä¸€â€œä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹â€ä¸º list[str] â€”â€” 
    ex = obj.get("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹")
    if isinstance(ex, str):
        obj["ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"] = [ex.strip()] if ex.strip() else []
    elif isinstance(ex, list):
        obj["ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"] = [str(x).strip() for x in ex if str(x).strip()]
    else:
        obj["ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹"] = []

    # â€”â€” åˆ†æ­§ç‚¹åšä¸ª stripï¼Œæ–¹ä¾¿åé¢åˆ¤æ–­â€œæ— æ˜æ˜¾åˆ†æ­§â€ â€”â€” 
    diff = obj.get("ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹")
    if isinstance(diff, str):
        obj["ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹"] = diff.strip()

    return obj

#--------------final_top5_result---------------------

def build_daily_top5_opinion_records(
    top5_results: List[Dict[str, Any]],
    sub_opinion_map: Dict[str, Dict[str, Any]],
) -> List[Dict[str, Any]]:
    final_records: List[Dict[str, Any]] = []

    for cluster in top5_results:
        date = cluster.get("æ—¥æœŸ")
        cluster_name = cluster.get("èšåˆè¯é¢˜ç°‡") or cluster.get("è¯é¢˜ç°‡") or "æœªçŸ¥"

        base = {
            "æ—¥æœŸ": date,
            "èšåˆè¯é¢˜ç°‡": cluster_name,
            "çƒ­åº¦è¯„åˆ†": cluster.get("çƒ­åº¦è¯„åˆ†"),
            "å‘è¨€ç©å®¶æ•°": cluster.get("å‘è¨€ç©å®¶æ€»æ•°"),
            "å‘è¨€æ€»æ•°": cluster.get("å‘è¨€æ€»æ•°"),
            "æ—¶é—´è½´": cluster.get("æ—¶é—´è½´"),
        }

        cid_list = cluster.get("å­è¯é¢˜ç°‡åˆ—è¡¨", []) or []

        discussion_items: List[Dict[str, Any]] = []
        all_examples: List[str] = []
        seen_examples = set()  # é˜²æ­¢åŒä¸€å¥åŸè¯é‡å¤

        for idx, cid in enumerate(cid_list, start=1):
            op = sub_opinion_map.get(cid)
            if not op:
                print(f"âš  å­è¯é¢˜ç°‡ {cid} æœªåœ¨ sub_opinion_map ä¸­æ‰¾åˆ°æ¨¡å‹#4ç»“æœï¼Œè·³è¿‡ã€‚")
                continue

            # åŠ¨æ€å­—æ®µåï¼šè®¨è®ºç‚¹1 / è®¨è®ºç‚¹2 / ...
            key_discussion = f"è®¨è®ºç‚¹{idx}"

            item: Dict[str, Any] = {
                key_discussion: op.get("è®¨è®ºç‚¹", ""),
                "ç©å®¶å…±è¯†": op.get("ç©å®¶å…±è¯†", ""),
            }

            # åªåœ¨ã€æœ‰çœŸå®åˆ†æ­§ã€‘æ—¶å†™å…¥â€œç©å®¶ä¸»è¦åˆ†æ­§ç‚¹â€
            diff = op.get("ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹", "")
            if isinstance(diff, str):
                diff_clean = diff.strip()
                if diff_clean and diff_clean != "æ— æ˜æ˜¾åˆ†æ­§":
                    item["ç©å®¶ä¸»è¦åˆ†æ­§ç‚¹"] = diff_clean

            discussion_items.append(item)

            # æ”¶é›†å…¸å‹å‘è¨€
            examples = op.get("ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹", [])
            if isinstance(examples, list):
                for ex in examples:
                    if not ex:
                        continue
                    s = str(ex)
                    if s in seen_examples:
                        continue
                    seen_examples.add(s)
                    all_examples.append(s)

        record = {
            **base,
            "è®¨è®ºç‚¹åˆ—è¡¨": discussion_items,
            "ä»£è¡¨æ€§ç©å®¶å‘è¨€ç¤ºä¾‹": all_examples,
        }
        final_records.append(record)

    return final_records
#------------------------ç‰ˆæœ¬èšåˆè¯æç°‡å¼•å…¥-------------------------
def read_jsonl_file(path: str | Path) -> list[dict]:
    path = Path(path)
    rows = []
    if not path.exists():
        return rows
    for i, line in enumerate(path.read_text(encoding="utf-8").splitlines(), start=1):
        line = line.strip()
        if not line:
            continue
        try:
            rows.append(json.loads(line))
        except Exception as e:
            print(f"[read_jsonl_file] line {i} JSONè§£æå¤±è´¥ï¼š{e}\nåŸæ–‡ï¼š{line[:200]}...")
    return rows


#####å¤šæ—¥è¾“å‡ºæçº¯åªè¦idã€æ—¥æœŸè¯æç°‡ã€è®¨è®ºç‚¹################################
import json

def build_version_agg_input_jsonl_text(
    daily_top5_rows: list[dict],
    max_points_per_row: int = 3,
) -> str:
    """
    ä» daily_TOP5ï¼ˆå¤šæ—¥æ··åœ¨ä¸€ä¸ªæ–‡ä»¶ï¼‰æŠ½å–æœ€å°å­—æ®µï¼Œç”Ÿæˆç»™æ¨¡å‹#4çš„ jsonl æ–‡æœ¬ã€‚
    æ¯è¡Œç»“æ„ï¼š
    {"id": "...", "æ—¥æœŸ": "...", "è¯é¢˜ç°‡": "...", "è®¨è®ºç‚¹": [...]}
    """
    out_lines = []
    seen = set()

    for r in daily_top5_rows:
        _id = (r.get("_daily_top_id") or r.get("id") or "").strip()
        date = (r.get("æ—¥æœŸ") or "").strip()
        topic = (r.get("èšåˆè¯é¢˜ç°‡") or r.get("è¯é¢˜ç°‡") or r.get("è¯é¢˜ç°‡3") or "").strip()

        points = r.get("è®¨è®ºç‚¹") or []
        if isinstance(points, str):
            points = [points]
        points = [p.strip() for p in points if isinstance(p, str) and p.strip()]

        if max_points_per_row and len(points) > max_points_per_row:
            points = points[:max_points_per_row]

        # æ²¡ id çš„è¡Œè·³è¿‡ï¼ˆå»ºè®®ä¿è¯æ¯è¡Œéƒ½æœ‰ _daily_top_idï¼‰
        if not _id:
            continue

        # å»é‡
        if _id in seen:
            continue
        seen.add(_id)

        obj = {
            "è¯é¢˜ç°‡": topic,
            "æ—¥æœŸ": date,
            "è®¨è®ºç‚¹": points,
            "id": _id,
            
        }
        out_lines.append(json.dumps(obj, ensure_ascii=False))

    return "\n".join(out_lines)

####################ç‰ˆæœ¬çƒ­åº¦top5è®¡ç®—#################
def compute_version_heat_topk(
    version_clusters: List[Dict[str, Any]],
    daily_top5_rows: List[Dict[str, Any]],
    top_k: int = 5,
) -> List[Dict[str, Any]]:
    """
    version_clusters: æ¨¡å‹#4 çš„ç‰ˆæœ¬èšåˆè¾“å‡ºï¼ˆæ¯è¡Œæœ‰ è¯é¢˜ç°‡ / è®¨è®ºç‚¹ / æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨ï¼‰
    daily_top5_rows:  daily_top5.jsonl è§£æç»“æœï¼ˆæœ‰ å‘è¨€æ€»æ•° / å‘è¨€ç©å®¶æ€»æ•° / _daily_top_idï¼‰

    ç‰ˆæœ¬çº§å‘è¨€çƒ­åº¦å…¬å¼ï¼š
      ç‰ˆæœ¬_å‘è¨€çƒ­åº¦ = ç‰ˆæœ¬_å‘è¨€ç©å®¶æ€»æ•° Ã— sqrt(ç‰ˆæœ¬_å‘è¨€æ€»æ•°)
    å…¶ä¸­ï¼š
      ç‰ˆæœ¬_å‘è¨€æ€»æ•°     = è¯¥ç‰ˆæœ¬çº§ç°‡ä¸‹æ‰€æœ‰ daily å­ç°‡ å‘è¨€æ€»æ•° ä¹‹å’Œ
      ç‰ˆæœ¬_å‘è¨€ç©å®¶æ€»æ•° = è¯¥ç‰ˆæœ¬çº§ç°‡ä¸‹æ‰€æœ‰ daily å­ç°‡ å‘è¨€ç©å®¶æ€»æ•° ä¹‹å’Œï¼ˆè¿‘ä¼¼ï¼‰
    """

    # 1) å»º daily_top5 çš„ç´¢å¼•ï¼šid -> {å‘è¨€æ€»æ•°, å‘è¨€ç©å®¶æ€»æ•°}
    id2metrics: Dict[str, Dict[str, int]] = {}
    for r in daily_top5_rows:
        _id = (r.get("_daily_top_id") or r.get("id") or "").strip()
        if not _id:
            continue
        total_msgs = int(r.get("å‘è¨€æ€»æ•°") or 0)
        total_players = int(r.get("å‘è¨€ç©å®¶æ€»æ•°") or 0)
        id2metrics[_id] = {
            "å‘è¨€æ€»æ•°": total_msgs,
            "å‘è¨€ç©å®¶æ€»æ•°": total_players,
        }

    enriched: List[Dict[str, Any]] = []

    # 2) å¯¹æ¯ä¸ªç‰ˆæœ¬çº§èšåˆè¯é¢˜ç°‡ï¼Œç´¯åŠ æ——ä¸‹æ‰€æœ‰ id çš„ U / M
    for vc in version_clusters:
        dt_list = vc.get("æ—¥æœŸæ—¶é—´è½´åˆ—è¡¨") or []
        version_total_msgs = 0
        version_total_players = 0
        used_ids = set()

        for item in dt_list:
            did = (item.get("id") or "").strip()
            if not did or did in used_ids:
                continue
            used_ids.add(did)

            metrics = id2metrics.get(did)
            if not metrics:
                # daily_top5.jsonl é‡Œæ‰¾ä¸åˆ°è¿™ä¸ª idï¼Œå¯ä»¥æ‰“å°å‡ºæ¥æ’æŸ¥
                # print(f"[WARN] æ‰¾ä¸åˆ° daily è®°å½•ï¼š{did}")
                continue

            version_total_msgs += metrics["å‘è¨€æ€»æ•°"]
            version_total_players += metrics["å‘è¨€ç©å®¶æ€»æ•°"]

        # æ²¡æœ‰ä»»ä½•æœ‰æ•ˆæ•°æ®ï¼Œè·³è¿‡
        if version_total_msgs <= 0 or version_total_players <= 0:
            continue

        # ğŸ” è¿™é‡Œç›´æ¥å¤ç”¨ä½ å·²æœ‰çš„ compute_heat_score
        version_heat = compute_heat_score(version_total_players, version_total_msgs)

        vc_enriched = dict(vc)
        vc_enriched["ç‰ˆæœ¬_å‘è¨€æ€»æ•°"] = version_total_msgs
        vc_enriched["ç‰ˆæœ¬_å‘è¨€ç©å®¶æ€»æ•°"] = version_total_players
        vc_enriched["ç‰ˆæœ¬_å‘è¨€çƒ­åº¦"] = version_heat

        enriched.append(vc_enriched)

    # 3) æŒ‰ç‰ˆæœ¬_å‘è¨€çƒ­åº¦æ’åºï¼Œå– TopK
    enriched.sort(key=lambda x: x.get("ç‰ˆæœ¬_å‘è¨€çƒ­åº¦", 0.0), reverse=True)
    return enriched[:top_k]

