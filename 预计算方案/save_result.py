"""
ä¿å­˜åˆ†æç»“æœè„šæœ¬
è¿è¡Œ top5_Q2.ipynb çš„é€»è¾‘å¹¶å°†ç»“æœä¿å­˜ä¸º JSON æ–‡ä»¶
"""
import sys
import os
import json
import time
import argparse
from pathlib import Path
from datetime import datetime

# æ·»åŠ æºä»£ç è·¯å¾„
SOURCE_DIR = Path(__file__).parent.parent / "ç©å®¶å‘è¨€æ•´ç†ï¼ˆä¾›è¿è¥ä¾§ï¼‰" / "ç©å®¶å‘è¨€æ€»ç»“_ç‰ˆæœ¬æ€»ç»“V2-Copy1.0(å•æ—¥ï¼‰"
if SOURCE_DIR.exists():
    sys.path.insert(0, str(SOURCE_DIR))
    print(f"âœ… å·²åŠ è½½æºä»£ç è·¯å¾„: {SOURCE_DIR}")
else:
    print(f"âŒ æºä»£ç è·¯å¾„ä¸å­˜åœ¨: {SOURCE_DIR}")
    sys.exit(1)

# å¯¼å…¥åˆ†ææ¨¡å—
from data_processing import build_jsonl_for_range
from model_classifyV1_Copy1_Copy1 import (
    load_system_prompt,
    build_user_prompt_filter,
    build_user_prompt_clsuter,
    build_user_prompt_cluster_agg,
    build_user_prompt_subcluster_opinion,
    call_ark_chat_completions,
    parse_model2_output_to_json_list,
    infer_date_for_batch,
    assign_global_cluster_ids,
    aggregate_cluster_outputs,
    parse_jsonl_text_safe,
    parse_jsonl_text,
    ensure_time_axis_key,
    ensure_subcluster_list_key,
    extract_top5_heat_clusters,
    attach_discussion_points,
    print_mech_time_from_top5,
    get_dialogs_lines_by_fayan_time_debug,
    merge_top5_with_opinions_numbered,
    parse_opinion_output_to_list,
)

# ==================== é…ç½® ====================
API_URL = "https://ark.cn-beijing.volces.com/api/v3/chat/completions"
API_KEY = "de91deb0-aae6-46cb-bac0-17ac3b6107f5"
V3_MODEL_ID = "ep-20251020160142-5d7hp"
V3_1_MODEL_ID = "ep-20251020160025-9p5tj"

BATCH_SIZE = 300
TEMPERATURE = 0.20
MAX_TOKENS = 16384
TIMEOUT_SEC = 600
RETRIES = 2

SPEAKER_MAP = {
    "16186514": "peteræœ¬å°Š",
    "1655611808": "è¿è¥ç»¾ç»¾",
    "2073820674": "æ²™åˆ©æ–‡è€å¸ˆ",
    "2726067525": "milissa",
}

# ç»“æœä¿å­˜ç›®å½•
RESULTS_DIR = Path(__file__).parent / "results"
RESULTS_DIR.mkdir(exist_ok=True)


def run_analysis(txt_path: str, mapping_path: str, 
                 start_time: str, end_time: str) -> dict:
    """
    è¿è¡Œåˆ†æï¼ˆä¸ top5_Q2.ipynb é€»è¾‘ä¸€è‡´ï¼‰
    """
    print(f"\n{'='*60}")
    print(f"ğŸ“Š å¼€å§‹åˆ†æ: {start_time} ~ {end_time}")
    print(f"{'='*60}")
    
    # åŠ è½½æç¤ºè¯
    prompt_dir = SOURCE_DIR
    system_prompt01 = load_system_prompt(prompt_dir / "æç¤ºè¯1.md")
    system_prompt02 = load_system_prompt(prompt_dir / "2è¯é¢˜åˆ†ç±».md")
    system_prompt03 = load_system_prompt(prompt_dir / "3æ—¥èšåˆ.md")
    system_prompt04 = load_system_prompt(prompt_dir / "2è¯é¢˜åˆ†ç±»å’Œæ€»ç»“.md")
    
    # Step 1: æ•°æ®é¢„å¤„ç†
    print("\n[1/6] åŠ è½½èŠå¤©è®°å½•...")
    jsonl_lines01 = build_jsonl_for_range(
        pathtxt=txt_path,
        mapping_file=mapping_path,
        speaker_map=SPEAKER_MAP,
        start_time=start_time,
        end_time=end_time,
        return_str=False,
    )
    
    total_messages = len(jsonl_lines01)
    print(f"  â†’ å…± {total_messages} æ¡æ¶ˆæ¯")
    
    if total_messages == 0:
        return {
            "status": "no_data",
            "error": "æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰èŠå¤©è®°å½•",
            "date": start_time.split(" ")[0],
            "time_range": f"{start_time} ~ {end_time}",
            "total_messages": 0,
            "filtered_messages": 0,
            "top5_clusters": []
        }
    
    # Step 2: æ¨¡å‹#1 + æ¨¡å‹#2 æ‰¹å¤„ç†
    print("\n[2/6] è¯é¢˜ç°‡åˆ†æ...")
    batch_cluster_outputs = []
    total_batches = (total_messages + BATCH_SIZE - 1) // BATCH_SIZE
    written_total = 0
    
    for b in range(total_batches):
        start_idx = b * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, total_messages)
        batch_lines = jsonl_lines01[start_idx:end_idx]
        
        print(f"  æ‰¹æ¬¡ {b+1}/{total_batches}...", end=" ")
        
        try:
            # æ¨¡å‹ #1ï¼šç­›é€‰
            user_prompt1 = build_user_prompt_filter(batch_lines)
            output_filter = call_ark_chat_completions(
                api_url=API_URL,
                api_key=API_KEY,
                model=V3_MODEL_ID,
                system_prompt=system_prompt01,
                user_prompt=user_prompt1,
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                timeout=TIMEOUT_SEC,
                retries=RETRIES,
            )
            
            if not output_filter:
                print("âŒ æ¨¡å‹#1 æ— è¾“å‡º")
                continue
            
            filter_count = sum(1 for line in output_filter.splitlines() if line.strip())
            written_total += filter_count
            
            # æ¨¡å‹ #2ï¼šè¯é¢˜ç°‡
            user_prompt2 = build_user_prompt_clsuter(output_filter)
            output_cluster = call_ark_chat_completions(
                api_url=API_URL,
                api_key=API_KEY,
                model=V3_1_MODEL_ID,
                system_prompt=system_prompt02,
                user_prompt=user_prompt2,
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                timeout=TIMEOUT_SEC,
                retries=RETRIES,
            )
            
            if not output_cluster:
                print("âŒ æ¨¡å‹#2 æ— è¾“å‡º")
                continue
            
            # è§£æå¹¶æ·»åŠ  ID
            cluster_json_list = parse_model2_output_to_json_list(output_cluster, batch_idx=b+1)
            if not cluster_json_list:
                print("âš ï¸ æ— æœ‰æ•ˆç°‡")
                continue
            
            date_str = infer_date_for_batch(cluster_json_list, batch_lines)
            batch_id = f"B{b+1}"
            cluster_json_list = assign_global_cluster_ids(cluster_json_list, date_str, batch_id)
            
            output_cluster_with_ids = "\n".join(
                json.dumps(c, ensure_ascii=False) for c in cluster_json_list
            )
            batch_cluster_outputs.append(output_cluster_with_ids)
            print(f"âœ… ç­›é€‰ {filter_count} æ¡")
            
        except Exception as e:
            print(f"âŒ å‡ºé”™: {e}")
            continue
        
        time.sleep(1)
    
    # Step 3: æ¨¡å‹#3 èšåˆ
    print("\n[3/6] èšåˆè¯é¢˜ç°‡...")
    all_cluster = aggregate_cluster_outputs(batch_cluster_outputs)
    
    user_prompt3 = build_user_prompt_cluster_agg(all_cluster)
    output_cluster_agg = call_ark_chat_completions(
        api_url=API_URL,
        api_key=API_KEY,
        model=V3_1_MODEL_ID,
        system_prompt=system_prompt03,
        user_prompt=user_prompt3,
        temperature=TEMPERATURE,
        max_tokens=MAX_TOKENS,
        timeout=TIMEOUT_SEC,
        retries=RETRIES,
    )
    
    parsed_clusters = parse_jsonl_text_safe(output_cluster_agg, label="æ¨¡å‹#3èšåˆè¾“å‡º")
    parsed_subclusters = parse_jsonl_text(all_cluster)
    
    for c in parsed_clusters:
        ensure_time_axis_key(c, verbose=False)
        ensure_subcluster_list_key(c)
    
    # Step 4: è®¡ç®—çƒ­åº¦ Top5
    print("\n[4/6] è®¡ç®—çƒ­åº¦æ’å...")
    top5_results = extract_top5_heat_clusters(parsed_clusters, jsonl_lines01, top_k=5)
    final_result = attach_discussion_points(top5_results, parsed_subclusters)
    
    # Step 5: æ¨¡å‹#4 è§‚ç‚¹åˆ†æ
    print("\n[5/6] åˆ†æç©å®¶è§‚ç‚¹...")
    rows = print_mech_time_from_top5(final_result, all_cluster)
    
    all_opinions = []
    for idx, r in enumerate(rows, start=1):
        mech = r.get("æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶") or ""
        full_time = (r.get("å‘è¨€æ—¶é—´") or "").strip()
        
        if not mech or not full_time or " " not in full_time:
            continue
        
        fayan_date, fayan_time = full_time.split(" ", 1)
        
        dialogs_lines = get_dialogs_lines_by_fayan_time_debug(
            jsonl_lines01, fayan_date, fayan_time, debug=False
        )
        
        if not dialogs_lines:
            continue
        
        user_prompt4 = build_user_prompt_subcluster_opinion(
            discussion_point=mech,
            json_lines=dialogs_lines,
        )
        
        try:
            opinion_output = call_ark_chat_completions(
                api_url=API_URL,
                api_key=API_KEY,
                model=V3_1_MODEL_ID,
                system_prompt=system_prompt04,
                user_prompt=user_prompt4,
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                timeout=TIMEOUT_SEC,
                retries=RETRIES,
            )
            
            opinions_this_mech = parse_opinion_output_to_list(opinion_output)
            all_opinions.extend(opinions_this_mech)
            print(f"  âœ… è§‚ç‚¹ {idx}: {mech[:30]}...")
        except Exception as e:
            print(f"  âŒ è§‚ç‚¹ {idx} å‡ºé”™: {e}")
            continue
    
    # Step 6: åˆå¹¶ç»“æœ
    print("\n[6/6] ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    merged_top5 = merge_top5_with_opinions_numbered(final_result, all_opinions)
    
    return {
        "status": "success",
        "date": start_time.split(" ")[0],
        "time_range": f"{start_time} ~ {end_time}",
        "total_messages": total_messages,
        "filtered_messages": written_total,
        "top5_clusters": merged_top5,
        "generated_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    }


def save_result(result: dict, output_path: Path = None):
    """ä¿å­˜ç»“æœåˆ° JSON æ–‡ä»¶"""
    if output_path is None:
        date_str = result.get("date", datetime.now().strftime("%Y-%m-%d"))
        output_path = RESULTS_DIR / f"{date_str}.json"
    
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nâœ… ç»“æœå·²ä¿å­˜åˆ°: {output_path}")
    return output_path


def main():
    parser = argparse.ArgumentParser(description="è¿è¡Œåˆ†æå¹¶ä¿å­˜ç»“æœ")
    parser.add_argument("--txt", required=True, help="èŠå¤©è®°å½• txt æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--mapping", required=True, help="å®¢æœæ˜ å°„ xlsx æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--start", required=True, help="å¼€å§‹æ—¶é—´ (æ ¼å¼: YYYY-MM-DD HH:MM:SS)")
    parser.add_argument("--end", required=True, help="ç»“æŸæ—¶é—´ (æ ¼å¼: YYYY-MM-DD HH:MM:SS)")
    parser.add_argument("--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (å¯é€‰)")
    
    args = parser.parse_args()
    
    # è¿è¡Œåˆ†æ
    result = run_analysis(
        txt_path=args.txt,
        mapping_path=args.mapping,
        start_time=args.start,
        end_time=args.end
    )
    
    # ä¿å­˜ç»“æœ
    output_path = Path(args.output) if args.output else None
    save_result(result, output_path)
    
    print("\n" + "="*60)
    print("ğŸ‰ åˆ†æå®Œæˆï¼")
    print("="*60)
    print("\nä¸‹ä¸€æ­¥ï¼š")
    print("1. git add results/")
    print('2. git commit -m "æ·»åŠ åˆ†æç»“æœ"')
    print("3. git push")


if __name__ == "__main__":
    main()


