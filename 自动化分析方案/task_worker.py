"""
æœ¬åœ°ä»»åŠ¡ç›‘å¬è„šæœ¬
åå°è¿è¡Œï¼Œè‡ªåŠ¨æ£€æµ‹å¹¶å¤„ç†åˆ†æä»»åŠ¡
"""
import sys
import time
import os
import traceback
from datetime import datetime
from pathlib import Path

# æ·»åŠ æºä»£ç è·¯å¾„
try:
    from config import SOURCE_CODE_DIR, PROMPTS_DIR, TEMP_DIR, POLL_INTERVAL
    from config import API_URL, API_KEY, V3_MODEL_ID, V3_1_MODEL_ID
    from config import BATCH_SIZE, TEMPERATURE, MAX_TOKENS, TIMEOUT_SEC, RETRIES
    from config import SPEAKER_MAP
except ImportError:
    print("âŒ è¯·å…ˆå¤åˆ¶ config.example.py ä¸º config.py å¹¶å¡«å…¥é…ç½®ä¿¡æ¯")
    sys.exit(1)

# æ·»åŠ æºä»£ç åˆ°è·¯å¾„
if os.path.exists(SOURCE_CODE_DIR):
    sys.path.insert(0, SOURCE_CODE_DIR)
    print(f"âœ… å·²åŠ è½½æºä»£ç è·¯å¾„: {SOURCE_CODE_DIR}")
else:
    print(f"âŒ æºä»£ç è·¯å¾„ä¸å­˜åœ¨: {SOURCE_CODE_DIR}")
    sys.exit(1)

# å¯¼å…¥åˆ†ææ¨¡å—
from data_processing import build_jsonl_for_range
from model_classifyV1_Copy1_Copy1 import (
    load_system_prompt,
    build_user_prompt_filter,
    build_user_prompt_clsuter,
    build_user_prompt_cluster_agg,
    build_user_prompt_subcluster_opinion,
    call_ark_chat_completions,
    parse_model2_output_to_json_list,
    infer_date_for_batch,
    assign_global_cluster_ids,
    aggregate_cluster_outputs,
    parse_jsonl_text_safe,
    parse_jsonl_text,
    ensure_time_axis_key,
    ensure_subcluster_list_key,
    extract_top5_heat_clusters,
    attach_discussion_points,
    print_mech_time_from_top5,
    get_dialogs_lines_by_fayan_time_debug,
    merge_top5_with_opinions_numbered,
    parse_opinion_output_to_list,
)

from supabase_client import get_client
import json


def run_analysis(txt_path: str, mapping_path: str, 
                 start_time: str, end_time: str) -> dict:
    """
    è¿è¡Œåˆ†æï¼ˆä¸ top5_Q2.ipynb é€»è¾‘ä¸€è‡´ï¼‰
    """
    print(f"ğŸ“Š å¼€å§‹åˆ†æ: {start_time} ~ {end_time}")
    
    # åŠ è½½æç¤ºè¯
    prompt_dir = Path(PROMPTS_DIR)
    system_prompt01 = load_system_prompt(prompt_dir / "æç¤ºè¯1.md")
    system_prompt02 = load_system_prompt(prompt_dir / "2è¯é¢˜åˆ†ç±».md")
    system_prompt03 = load_system_prompt(prompt_dir / "3æ—¥èšåˆ.md")
    system_prompt04 = load_system_prompt(prompt_dir / "2è¯é¢˜åˆ†ç±»å’Œæ€»ç»“.md")
    
    # Step 1: æ•°æ®é¢„å¤„ç†
    print("  [1/6] åŠ è½½èŠå¤©è®°å½•...")
    jsonl_lines01 = build_jsonl_for_range(
        pathtxt=txt_path,
        mapping_file=mapping_path,
        speaker_map=SPEAKER_MAP,
        start_time=start_time,
        end_time=end_time,
        return_str=False,
    )
    
    total_messages = len(jsonl_lines01)
    print(f"  â†’ å…± {total_messages} æ¡æ¶ˆæ¯")
    
    if total_messages == 0:
        return {
            "status": "no_data",
            "error": "æŒ‡å®šæ—¶é—´èŒƒå›´å†…æ²¡æœ‰èŠå¤©è®°å½•",
            "total_messages": 0,
            "filtered_messages": 0,
            "top5_clusters": []
        }
    
    # Step 2: æ¨¡å‹#1 + æ¨¡å‹#2 æ‰¹å¤„ç†
    print("  [2/6] è¯é¢˜ç°‡åˆ†æ...")
    batch_cluster_outputs = []
    total_batches = (total_messages + BATCH_SIZE - 1) // BATCH_SIZE
    written_total = 0
    
    for b in range(total_batches):
        start_idx = b * BATCH_SIZE
        end_idx = min(start_idx + BATCH_SIZE, total_messages)
        batch_lines = jsonl_lines01[start_idx:end_idx]
        
        print(f"    æ‰¹æ¬¡ {b+1}/{total_batches}...")
        
        try:
            # æ¨¡å‹ #1ï¼šç­›é€‰
            user_prompt1 = build_user_prompt_filter(batch_lines)
            output_filter = call_ark_chat_completions(
                api_url=API_URL,
                api_key=API_KEY,
                model=V3_MODEL_ID,
                system_prompt=system_prompt01,
                user_prompt=user_prompt1,
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                timeout=TIMEOUT_SEC,
                retries=RETRIES,
            )
            
            if not output_filter:
                continue
            
            filter_count = sum(1 for line in output_filter.splitlines() if line.strip())
            written_total += filter_count
            
            # æ¨¡å‹ #2ï¼šè¯é¢˜ç°‡
            user_prompt2 = build_user_prompt_clsuter(output_filter)
            output_cluster = call_ark_chat_completions(
                api_url=API_URL,
                api_key=API_KEY,
                model=V3_1_MODEL_ID,
                system_prompt=system_prompt02,
                user_prompt=user_prompt2,
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                timeout=TIMEOUT_SEC,
                retries=RETRIES,
            )
            
            if not output_cluster:
                continue
            
            # è§£æå¹¶æ·»åŠ  ID
            cluster_json_list = parse_model2_output_to_json_list(output_cluster, batch_idx=b+1)
            if not cluster_json_list:
                continue
            
            date_str = infer_date_for_batch(cluster_json_list, batch_lines)
            batch_id = f"B{b+1}"
            cluster_json_list = assign_global_cluster_ids(cluster_json_list, date_str, batch_id)
            
            output_cluster_with_ids = "\n".join(
                json.dumps(c, ensure_ascii=False) for c in cluster_json_list
            )
            batch_cluster_outputs.append(output_cluster_with_ids)
            
        except Exception as e:
            print(f"    âŒ æ‰¹æ¬¡ {b+1} å‡ºé”™: {e}")
            continue
        
        time.sleep(1)  # é˜²æ­¢ QPS é™åˆ¶
    
    # Step 3: æ¨¡å‹#3 èšåˆ
    print("  [3/6] èšåˆè¯é¢˜ç°‡...")
    all_cluster = aggregate_cluster_outputs(batch_cluster_outputs)
    
    user_prompt3 = build_user_prompt_cluster_agg(all_cluster)
    output_cluster_agg = call_ark_chat_completions(
        api_url=API_URL,
        api_key=API_KEY,
        model=V3_1_MODEL_ID,
        system_prompt=system_prompt03,
        user_prompt=user_prompt3,
        temperature=TEMPERATURE,
        max_tokens=MAX_TOKENS,
        timeout=TIMEOUT_SEC,
        retries=RETRIES,
    )
    
    parsed_clusters = parse_jsonl_text_safe(output_cluster_agg, label="æ¨¡å‹#3èšåˆè¾“å‡º")
    parsed_subclusters = parse_jsonl_text(all_cluster)
    
    for c in parsed_clusters:
        ensure_time_axis_key(c, verbose=False)
        ensure_subcluster_list_key(c)
    
    # Step 4: è®¡ç®—çƒ­åº¦ Top5
    print("  [4/6] è®¡ç®—çƒ­åº¦æ’å...")
    top5_results = extract_top5_heat_clusters(parsed_clusters, jsonl_lines01, top_k=5)
    final_result = attach_discussion_points(top5_results, parsed_subclusters)
    
    # Step 5: æ¨¡å‹#4 è§‚ç‚¹åˆ†æ
    print("  [5/6] åˆ†æç©å®¶è§‚ç‚¹...")
    rows = print_mech_time_from_top5(final_result, all_cluster)
    
    all_opinions = []
    for idx, r in enumerate(rows, start=1):
        mech = r.get("æ ¸å¿ƒå¯¹è±¡/æœºåˆ¶") or ""
        full_time = (r.get("å‘è¨€æ—¶é—´") or "").strip()
        
        if not mech or not full_time or " " not in full_time:
            continue
        
        fayan_date, fayan_time = full_time.split(" ", 1)
        
        dialogs_lines = get_dialogs_lines_by_fayan_time_debug(
            jsonl_lines01, fayan_date, fayan_time, debug=False
        )
        
        if not dialogs_lines:
            continue
        
        user_prompt4 = build_user_prompt_subcluster_opinion(
            discussion_point=mech,
            json_lines=dialogs_lines,
        )
        
        try:
            opinion_output = call_ark_chat_completions(
                api_url=API_URL,
                api_key=API_KEY,
                model=V3_1_MODEL_ID,
                system_prompt=system_prompt04,
                user_prompt=user_prompt4,
                temperature=TEMPERATURE,
                max_tokens=MAX_TOKENS,
                timeout=TIMEOUT_SEC,
                retries=RETRIES,
            )
            
            opinions_this_mech = parse_opinion_output_to_list(opinion_output)
            all_opinions.extend(opinions_this_mech)
        except Exception as e:
            print(f"    âŒ è§‚ç‚¹åˆ†æå‡ºé”™: {e}")
            continue
    
    # Step 6: åˆå¹¶ç»“æœ
    print("  [6/6] ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    merged_top5 = merge_top5_with_opinions_numbered(final_result, all_opinions)
    
    return {
        "status": "success",
        "date": start_time.split(" ")[0],
        "time_range": f"{start_time} ~ {end_time}",
        "total_messages": total_messages,
        "filtered_messages": written_total,
        "top5_clusters": merged_top5
    }


def process_task(task: dict):
    """å¤„ç†å•ä¸ªä»»åŠ¡"""
    task_id = task["id"]
    client = get_client()
    
    print(f"\n{'='*50}")
    print(f"ğŸ“‹ å¤„ç†ä»»åŠ¡: {task_id}")
    print(f"   æ—¶é—´èŒƒå›´: {task['start_time']} ~ {task['end_time']}")
    print(f"{'='*50}")
    
    # æ›´æ–°çŠ¶æ€ä¸ºå¤„ç†ä¸­
    client.set_task_processing(task_id)
    
    start_time_proc = time.time()
    
    try:
        # ä¸‹è½½æ–‡ä»¶
        temp_dir = os.path.join(TEMP_DIR, task_id)
        txt_path, mapping_path = client.download_task_files(task, temp_dir)
        
        if not txt_path:
            raise Exception("txt æ–‡ä»¶ä¸‹è½½å¤±è´¥")
        if not mapping_path:
            raise Exception("mapping æ–‡ä»¶ä¸‹è½½å¤±è´¥")
        
        # è¿è¡Œåˆ†æ
        result = run_analysis(
            txt_path=txt_path,
            mapping_path=mapping_path,
            start_time=task["start_time"],
            end_time=task["end_time"]
        )
        
        processing_time = time.time() - start_time_proc
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if result["status"] == "success":
            client.set_task_completed(
                task_id,
                result=result,
                total_messages=result.get("total_messages", 0),
                filtered_messages=result.get("filtered_messages", 0),
                processing_time=processing_time
            )
            print(f"âœ… ä»»åŠ¡å®Œæˆï¼è€—æ—¶ {processing_time:.1f} ç§’")
        else:
            client.set_task_failed(task_id, result.get("error", "æœªçŸ¥é”™è¯¯"))
            print(f"âš ï¸ ä»»åŠ¡å®Œæˆä½†æ— æ•°æ®: {result.get('error')}")
        
    except Exception as e:
        error_msg = f"{str(e)}\n{traceback.format_exc()}"
        client.set_task_failed(task_id, error_msg)
        print(f"âŒ ä»»åŠ¡å¤±è´¥: {e}")


def main():
    """ä¸»å¾ªç¯"""
    print("="*60)
    print("ğŸš€ è‡ªåŠ¨åŒ–åˆ†æä»»åŠ¡ç›‘å¬å™¨")
    print("="*60)
    print(f"è½®è¯¢é—´éš”: {POLL_INTERVAL} ç§’")
    print(f"æºä»£ç ç›®å½•: {SOURCE_CODE_DIR}")
    print("æŒ‰ Ctrl+C åœæ­¢")
    print("="*60)
    
    client = get_client()
    
    while True:
        try:
            # è·å–å¾…å¤„ç†ä»»åŠ¡
            tasks = client.get_pending_tasks()
            
            if tasks:
                print(f"\nğŸ“¬ å‘ç° {len(tasks)} ä¸ªå¾…å¤„ç†ä»»åŠ¡")
                for task in tasks:
                    process_task(task)
            else:
                print(".", end="", flush=True)
            
            time.sleep(POLL_INTERVAL)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å·²åœæ­¢ç›‘å¬")
            break
        except Exception as e:
            print(f"\nâŒ ç›‘å¬å‡ºé”™: {e}")
            time.sleep(POLL_INTERVAL)


if __name__ == "__main__":
    main()


