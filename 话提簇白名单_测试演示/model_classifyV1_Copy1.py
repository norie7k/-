
from __future__ import annotations
import json, time, typing as T
import pandas as pd
import requests
from pathlib import Path
from typing import List, Dict, Any
import re, json, unicodedata
import json
# --- openpyxl æ ·å¼/å·¥å…· ---
from openpyxl import Workbook, load_workbook
from openpyxl.utils import get_column_letter
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, NamedStyle

import re

################æ¨¡å‹è°ƒç”¨ï¼Œå‡ºç»“æœ###################

def load_system_prompt(path: Path) -> str:
    if not path.exists():
        raise FileNotFoundError(f"Prompt file not found: {path}")
    return path.read_text(encoding="utf-8")

def build_user_prompt_filter(json_lines: T.List[str]) -> str:
    # æ¨¡å‹#1ï¼šç­›æ‰éæ¸¸æˆç›¸å…³ï¼Œåªè¾“å‡ºç›¸å…³ JSON è¡Œï¼ˆåŸæ ·ï¼‰
    return (
        "ä»¥ä¸‹æ˜¯è‹¥å¹²ç©å®¶/å®¢æœ/ç ”å‘çš„å‘è¨€è®°å½•ï¼Œè¯·æ ¹æ®ç³»ç»Ÿæç¤ºä¸­è§„åˆ™ï¼Œ"
        "åˆ¤æ–­å“ªäº›æ˜¯ã€ä¸æ¸¸æˆå†…å®¹ç›¸å…³ã€‘çš„å‘è¨€ï¼Œä¿ç•™è¿™äº› JSON è¡Œï¼Œä¸ç›¸å…³çš„å¿½ç•¥ã€‚"
        "è¯·ä»…è¾“å‡ºã€ç›¸å…³å‘è¨€çš„åŸå§‹ JSON è¡Œã€‘ï¼Œä¸¥æ ¼ä¿æŒæ ¼å¼ä¸å˜ã€‚\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + "\n".join(json_lines)
    )

def build_user_prompt_classify2(jsonl_block: str) -> str:
    # æ¨¡å‹#3ï¼šå¯¹å·²ç­›é€‰çš„ç›¸å…³ JSON è¡Œï¼ˆåŸæ ·ï¼‰è¿›è¡ŒäºŒçº§æ ‡ç­¾åˆ†ç±»
    return (
       "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·ä»…è¾“å‡ºã€ JSON è¡Œã€‘ï¼Œ\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )
    
def build_user_prompt_classify(jsonl_block: str) -> str:
    # æ¨¡å‹#2ï¼šå¯¹å·²ç­›é€‰çš„ç›¸å…³ JSON è¡Œï¼ˆåŸæ ·ï¼‰è¿›è¡Œåˆ†ç±»ï¼Œè¿½åŠ â€œæ„å›¾åˆ†ç±»â€é”®
    return (
       "ä»¥ä¸‹æ˜¯è¾“å…¥æ•°æ®ï¼ˆJSONL æ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªå‘è¨€å¯¹è±¡ï¼‰ï¼š\n\n"
        "è¯·ä»…è¾“å‡ºã€ JSON è¡Œã€‘ï¼Œ\n\n"
        "ã€è¾“å…¥ã€‘ï¼š\n" + jsonl_block
    )

def build_user_prompt_cluster_correct(clustered_jsonl: str, whitelist: List[Dict]) -> str:
    whitelist_text = "[å½“å‰ç™½åå•ä¸ºç©ºï¼Œæš‚æ— å‚è€ƒå‘½å]" if not whitelist else "\n".join(
        json.dumps(x, ensure_ascii=False) for x in whitelist
    )
    return (
        "ä½ æ˜¯ä¸€ä½â€œè¯é¢˜ç°‡å‘½åæ ¡æ­£ä¸“å®¶â€ã€‚å¯¹ç…§ç™½åå•ç»Ÿä¸€å‘½åï¼›ä¸åŒ¹é…åˆ™ä¿ç•™åŸåã€‚\n"
        "ä»…è¾“å‡ºå­—æ®µï¼šå‘è¨€æ—¥æœŸã€å‘è¨€æ—¶é—´ã€ç©å®¶IDã€ç©å®¶æ¶ˆæ¯ã€åˆ†ç±»æ ‡ç­¾ã€è¯é¢˜ç°‡ã€è¯é¢˜ç°‡æè¿°ã€‚\n"
        "å‘½ä¸­ç™½åå•æ—¶ï¼šè¯é¢˜ç°‡ = ç™½åå•åç§°ï¼›è¯é¢˜ç°‡æè¿° = ç™½åå•ç›¸å…³æè¿°ã€‚\n"
        "æœªå‘½ä¸­ç™½åå•æ—¶ï¼šè¯é¢˜ç°‡ä¿æŒè¾“å…¥å€¼ï¼›è¯é¢˜ç°‡æè¿°æ²¿ç”¨è¾“å…¥ï¼ˆè‹¥æ— åˆ™è¾“å‡ºç©ºå­—ç¬¦ä¸²ï¼‰ã€‚\n\n"
        "ã€å‘è¨€ã€‘ï¼š\n" + clustered_jsonl + "\n\n"
        "ã€ç™½åå•ã€‘ï¼š\n" + whitelist_text + "\n"
    )


def call_ark_chat_completions(
    api_url: str,
    api_key: str,
    model: str,
    system_prompt: str,
    user_prompt: str,
    temperature: float = 0.3,
    max_tokens: int = 32700,
    timeout: int = 600,
    retries: int = 2,
) -> str:
    headers = {"Content-Type": "application/json", "Authorization": f"Bearer {api_key}"}
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        "temperature": temperature,
        "max_tokens": max_tokens,
    }
    last_err = None
    for attempt in range(retries + 1):
        try:
            resp = requests.post(api_url, headers=headers, json=payload, timeout=timeout)
            if resp.status_code != 200:
                raise RuntimeError(f"HTTP {resp.status_code}: {resp.text}")
            data = resp.json()
            return data["choices"][0]["message"]["content"]
        except Exception as e:
            last_err = e
            time.sleep(1.2 * (attempt + 1))
    raise RuntimeError(f"Ark API è°ƒç”¨å¤±è´¥: {last_err}")

def extract_valid_json_lines(text: str) -> T.List[str]:
    """
    æŠŠæ¨¡å‹è¾“å‡ºé‡Œçš„çº¯ JSON è¡Œæå–å‡ºæ¥ï¼ˆé²æ£’å¤„ç†ï¼‰ï¼š
    - é€è¡Œåˆ¤æ–­ï¼šä»¥ { å¼€å¤´ ä¸” ä»¥ } ç»“å°¾ï¼Œåˆ™è®¤ä¸ºæ˜¯ä¸€ä¸ª JSON å¯¹è±¡è¡Œ
    - ä¹Ÿèƒ½å®¹å¿å‰åå¤šä½™ç©ºè¡Œæˆ–è§£é‡Šæ–‡å­—ï¼ˆä¼šè¢«å¿½ç•¥ï¼‰
    """
    lines = []
    for line in text.splitlines():
        s = line.strip()
        if not s:
            continue
        if s.startswith("{") and s.endswith("}"):
            lines.append(s)
    return lines






def jsonl_to_dataframe_with_intent(jsonl_text: str) -> pd.DataFrame:
    """
    å°†æ¨¡å‹#3è¾“å‡ºï¼ˆJSONLï¼Œæ¯è¡Œä¸€ä¸ªJSONï¼‰è½¬ dfã€‚
    ç›®æ ‡åˆ—ï¼š["è¯é¢˜ç°‡","å‘è¨€æ—¶é—´","ç©å®¶ID","ç©å®¶æ¶ˆæ¯","ä¸€çº§åˆ†ç±»"]

    æ–°å¢èƒ½åŠ›ï¼š
    - æ”¯æŒè¾“å…¥åŒæ—¶åŒ…å«ã€Œå‘è¨€æ—¥æœŸã€ã€Œå‘è¨€æ—¶é—´ã€ä¸¤ä¸ªå­—æ®µï¼›
    - è‡ªåŠ¨åˆå¹¶ä¸ºç»Ÿä¸€çš„ã€Œå‘è¨€æ—¶é—´ã€ï¼ˆæ ¼å¼ï¼š%Y-%m-%d %H:%M:%Sï¼‰ã€‚
    """
    # å–å‡ºçº¯ JSON è¡Œ
    try:
        pure_lines = extract_valid_json_lines(jsonl_text)
    except NameError:
        pure_lines = [ln.strip() for ln in (jsonl_text or "").splitlines() if ln.strip()]

    if not pure_lines:
        return pd.DataFrame(columns=["è¯é¢˜ç°‡","å‘è¨€æ—¶é—´","ç©å®¶ID","ç©å®¶æ¶ˆæ¯","ä¸€çº§åˆ†ç±»"])

    rows = []
    for line in pure_lines:
        try:
            rows.append(json.loads(line))
        except Exception:
            continue
    if not rows:
        return pd.DataFrame(columns=["è¯é¢˜ç°‡","å‘è¨€æ—¶é—´","ç©å®¶ID","ç©å®¶æ¶ˆæ¯","ä¸€çº§åˆ†ç±»"])

    df = pd.DataFrame(rows)

    # é”®åå…¼å®¹
    df = df.rename(columns={
        "ç©å®¶ ID": "ç©å®¶ID",
        "æ„å›¾åˆ†ç±»": "ä¸€çº§åˆ†ç±»",
        "åˆ†ç±»æ ‡ç­¾": "ä¸€çº§åˆ†ç±»",
    })

    # è¡¥åˆ—
    for c in ["å‘è¨€æ—¥æœŸ","å‘è¨€æ—¶é—´","ç©å®¶ID","ç©å®¶æ¶ˆæ¯","ä¸€çº§åˆ†ç±»","è¯é¢˜ç°‡"]:
        if c not in df.columns:
            df[c] = pd.NA

    # ä¸€çº§åˆ†ç±»æ•°å€¼åŒ–ï¼ˆå¯é€‰ï¼‰
    df["ä¸€çº§åˆ†ç±»"] = pd.to_numeric(df["ä¸€çº§åˆ†ç±»"], errors="coerce")

    # è¯é¢˜ç°‡ç»Ÿä¸€ä¸ºå­—ç¬¦ä¸²ï¼ˆå¦‚æœæ˜¯åˆ—è¡¨åˆ™ç”¨â€œã€â€æ‹¼æ¥ï¼‰
    def _topic_to_str(x):
        if x is None or (isinstance(x, float) and pd.isna(x)):
            return ""
        if isinstance(x, list):
            return "ã€".join(str(i) for i in x if i is not None)
        return str(x)
    df["è¯é¢˜ç°‡"] = df["è¯é¢˜ç°‡"].apply(_topic_to_str).str.strip()

    # === åˆå¹¶â€œå‘è¨€æ—¥æœŸ + å‘è¨€æ—¶é—´â€ => æ ‡å‡†â€œå‘è¨€æ—¶é—´â€ ===
    def _combine_dt(row):
        d = str(row.get("å‘è¨€æ—¥æœŸ") or "").strip()
        t = str(row.get("å‘è¨€æ—¶é—´") or "").strip()

        # åªæœ‰æ—¥æœŸæˆ–åªæœ‰æ—¶é—´çš„æƒ…å†µä¹Ÿå®¹é”™
        if d and t:
            s = f"{d} {t}"
        elif d:
            s = d
        else:
            s = t

        # ç»Ÿä¸€åˆ†éš”ç¬¦ï¼Œä¿®æ­£ä»…åˆ°åˆ†é’Ÿçš„æ—¶é—´è¡¥ç§’
        s = re.sub(r"[/.]", "-", s)
        if re.fullmatch(r"\d{1,2}:\d{2}", t):  # e.g. 14:03
            s = f"{d} {t}:00"

        ts = pd.to_datetime(s, errors="coerce")
        return ts

    ts = df.apply(_combine_dt, axis=1)

    # è¾“å‡ºä¸ºæ ‡å‡†æ ¼å¼å­—ç¬¦ä¸²ï¼›è§£æå¤±è´¥ä¿æŒ NaN
    df["å‘è¨€æ—¶é—´"] = ts.dt.strftime("%Y-%m-%d %H:%M:%S")
    df.loc[ts.isna(), "å‘è¨€æ—¶é—´"] = pd.NA

    # åªä¿ç•™ç›®æ ‡åˆ—é¡ºåº
    return df[["è¯é¢˜ç°‡","å‘è¨€æ—¶é—´","ç©å®¶ID","ç©å®¶æ¶ˆæ¯","ä¸€çº§åˆ†ç±»"]]

##########################è¯æç°‡æ•°æ®åº“################################
def load_whitelist(path: Path) -> list[dict]:
    if not path.exists():
        return []
    with open(path, "r", encoding="utf-8") as f:
        return [json.loads(line) for line in f if line.strip()]

def extract_clusters_from_output(output_text: str) -> list[dict]:
    import json, re
    if not output_text:
        return []
    s = output_text.strip()
    s = re.sub(r"^```[a-zA-Z0-9]*\s*", "", s)
    s = re.sub(r"\s*```$", "", s)
    results, seen = [], set()
    def _push(obj):
        cluster = (obj.get("è¯é¢˜ç°‡") or "").strip()
        desc = (obj.get("è¯é¢˜ç°‡æè¿°") or "").strip()
        if cluster and cluster not in seen:
            results.append({"è¯é¢˜ç°‡åç§°": cluster, "ç›¸å…³æè¿°": desc})
            seen.add(cluster)
    try:
        whole = json.loads(s)
        if isinstance(whole, dict):
            _push(whole); return results
        if isinstance(whole, list):
            for it in whole:
                if isinstance(it, dict): _push(it)
            return results
    except Exception:
        pass
    buf, depth = [], 0
    for ch in s:
        if ch == '{': depth += 1
        if depth > 0: buf.append(ch)
        if ch == '}':
            depth -= 1
            if depth == 0 and buf:
                block = ''.join(buf).strip(); buf = []
                try:
                    obj = json.loads(block)
                    if isinstance(obj, dict): _push(obj)
                except Exception:
                    pass
    return results


def update_and_save_whitelist(path: Path, current: list[dict], new_items: list[dict]) -> list[dict]:
    existing_names = {item["è¯é¢˜ç°‡åç§°"] for item in current}
    added = [item for item in new_items if item["è¯é¢˜ç°‡åç§°"] not in existing_names]

    if added:
        with open(path, "a", encoding="utf-8") as f:
            for item in added:
                f.write(json.dumps(item, ensure_ascii=False) + "\n")
        current.extend(added)
        print(f"âœ… æ–°å¢ {len(added)} æ¡è¯é¢˜ç°‡è‡³ç™½åå•")
    else:
        print("âšª æ— æ–°å¢è¯é¢˜ç°‡")

    return current


##########################å¯¼å…¥Excelæ ¼å¼è¦æ±‚###########################

from pathlib import Path
from typing import List, Dict, Any
import pandas as pd
import unicodedata
from openpyxl import Workbook, load_workbook
from openpyxl.styles import Font, Alignment, PatternFill, Border, Side, NamedStyle
from openpyxl.utils import get_column_letter

########################## Excel ç»“æ„ä¸æ ·å¼ ###########################
SHEET_NAMES = ["ä½“éªŒåé¦ˆ", "ç–‘æƒ‘è¯¢é—®", "å»ºè®®çµæ„Ÿ", "æƒ…ç»ªè¾“å‡º", "é—®é¢˜åé¦ˆ"]
# æ–°å¢â€œäºŒçº§æ ‡ç­¾â€åˆ—
HEADERS = [ "è¯é¢˜ç°‡","å‘è¨€æ—¶é—´", "ç©å®¶ID", "ç©å®¶æ¶ˆæ¯"]

# æ ·å¼é…ç½®
FONT_NAME = "å¾®é›…è½¯é»‘"         # è‹¥æœ¬æœºæ²¡æœ‰ï¼Œå¯æ”¹ "å¾®è½¯é›…é»‘"
HEADER_FONT_SIZE = 16
BODY_FONT_SIZE   = 11
HEADER_FILL      = "FFDDEBF7"     # è“åº•
HEADER_ROW_HEIGHT = 24
COL_WIDTHS = [ 16,21, 30, 95]   # æ—¶é—´ / ç©å®¶ID / ç©å®¶æ¶ˆæ¯ / äºŒçº§æ ‡ç­¾

CA_TO_SHEET = {1:"ä½“éªŒåé¦ˆ", 2:"ç–‘æƒ‘è¯¢é—®", 3:"å»ºè®®çµæ„Ÿ", 4:"æƒ…ç»ªè¾“å‡º", 5:"é—®é¢˜åé¦ˆ"}


def _ensure_named_style(wb) -> str:
    """ç¡®ä¿æ­£æ–‡ NamedStyle å­˜åœ¨ï¼›è¿”å› style åç§°ã€‚"""
    style_name = "BodyStyle"
    if style_name in wb.named_styles:
        return style_name
    thin = Side(style="thin", color="000000")
    body_style = NamedStyle(name=style_name)
    body_style.font = Font(name=FONT_NAME, size=BODY_FONT_SIZE)
    body_style.alignment = Alignment(vertical="center", wrap_text=True)
    body_style.border = Border(left=thin, right=thin, top=thin, bottom=thin)
    wb.add_named_style(body_style)
    return style_name


def create_intent_excel_styled(filename: str):
    """åˆå§‹åŒ–å·¥ä½œç°¿ï¼š5 ä¸ª sheetï¼Œè¡¨å¤´+åˆ—å®½+å†»ç»“+è¡¨å¤´æ ·å¼+æ­£æ–‡æ ·å¼æ³¨å†Œ"""
    wb = Workbook()
    ws0 = wb.active
    ws0.title = SHEET_NAMES[0]

    for idx, name in enumerate(SHEET_NAMES):
        ws = wb[name] if idx == 0 else wb.create_sheet(title=name)

        # è¡¨å¤´
        ws.append(HEADERS)

        # åˆ—å®½
        for col_idx, w in enumerate(COL_WIDTHS, start=1):
            ws.column_dimensions[get_column_letter(col_idx)].width = w

        # å†»ç»“é¦–è¡Œ
        ws.freeze_panes = "A2"

        # è¡¨å¤´æ ·å¼
        for c in range(1, len(HEADERS)+1):
            cell = ws.cell(row=1, column=c)
            cell.font = Font(name=FONT_NAME, size=HEADER_FONT_SIZE, bold=True)
            cell.alignment = Alignment(horizontal="center", vertical="center", wrap_text=True)
            cell.fill = PatternFill("solid", fgColor=HEADER_FILL)

        ws.row_dimensions[1].height = HEADER_ROW_HEIGHT

    _ensure_named_style(wb)
    wb.save(filename)
    print(f"âœ… åˆ›å»ºå¹¶å¥—æ ·å¼ï¼š{filename}")


def _open_or_create_excel(excel_path: str):
    """æ²¡æœ‰æ–‡ä»¶å°±åˆ›å»ºå¹¶é¢„ç½®æ ¼å¼ï¼›è¿”å› (wb, created_flag)"""
    p = Path(excel_path)
    if not p.exists():
        create_intent_excel_styled(excel_path)
        wb = load_workbook(excel_path)
        return wb, True
    wb = load_workbook(excel_path)
    _ensure_named_style(wb)
    return wb, False


########################## æ•°æ®è§„èŒƒåŒ– ###########################
def _normalize_records(records: List[Dict[str, Any]]) -> pd.DataFrame:
    """
    è¾“å…¥ï¼šå½¢å¦‚
    [{"ä¸€çº§åˆ†ç±»":2,"äºŒçº§æ ‡ç­¾":["æ®–è£…ä¿ç•™","ä¿ç•™çŒœæµ‹"],"å‘è¨€æ—¶é—´":"2025-08-06 17:25:36",
      "ç©å®¶ ID":"ï¼Œ(1272414483)","ç©å®¶æ¶ˆæ¯":"èƒ½ä¿ç•™æ®–è£…åº”è¯¥"}, ...]
    è¾“å‡ºï¼šDataFrameï¼Œåˆ— = ["ä¸€çº§åˆ†ç±»","å‘è¨€æ—¶é—´","ç©å®¶ID","ç©å®¶æ¶ˆæ¯","äºŒçº§æ ‡ç­¾"]ï¼Œ
    å…¶ä¸­â€œäºŒçº§æ ‡ç­¾â€ä¸ºå•ä¸ªå­—ç¬¦ä¸²ï¼ˆè‹¥åŸæœ¬æ˜¯åˆ—è¡¨åˆ™ explodeï¼‰
    """
    if not records:
        return pd.DataFrame(columns=["ä¸€çº§åˆ†ç±»", "å‘è¨€æ—¶é—´", "ç©å®¶ID", "ç©å®¶æ¶ˆæ¯", "è¯é¢˜ç°‡"])

    df = pd.DataFrame(records)

    # å…¼å®¹ é”®åå·®å¼‚ï¼šç©å®¶ ID vs ç©å®¶ID
    if "ç©å®¶ID" not in df.columns and "ç©å®¶ ID" in df.columns:
        df = df.rename(columns={"ç©å®¶ ID": "ç©å®¶ID"})

    # ç¼ºåˆ—è¡¥ç©º
    for col in ["ä¸€çº§åˆ†ç±»", "å‘è¨€æ—¶é—´", "ç©å®¶ID", "ç©å®¶æ¶ˆæ¯", "è¯é¢˜ç°‡"]:
        if col not in df.columns:
            df[col] = pd.NA

    # ç»Ÿä¸€â€œäºŒçº§æ ‡ç­¾â€ä¸ºåˆ—è¡¨
    def _to_list(x):
        if x is None or (isinstance(x, float) and pd.isna(x)):
            return []
        if isinstance(x, list):
            return x
        # å­—ç¬¦ä¸²å°±å½“æˆå•æ ‡ç­¾
        return [str(x)]

    df["è¯é¢˜ç°‡"] = df["è¯é¢˜ç°‡"].apply(_to_list)

    # explode æˆå•æ ‡ç­¾ä¸€è¡Œï¼›è‹¥åŸæœ¬ä¸ºç©ºåˆ—è¡¨ï¼Œå°†ç”Ÿæˆç©ºè¡Œï¼Œå…ˆè¿‡æ»¤
    df = df.explode("è¯é¢˜ç°‡", ignore_index=True)
    df["è¯é¢˜ç°‡"] = df["è¯é¢˜ç°‡"].fillna("").astype(str)

    # ä¸€çº§åˆ†ç±»æ•°å€¼åŒ– & è¿‡æ»¤æœ‰æ•ˆ sheet
    df["ä¸€çº§åˆ†ç±»"] = pd.to_numeric(df["ä¸€çº§åˆ†ç±»"], errors="coerce")
    df = df[df["ä¸€çº§åˆ†ç±»"].isin(CA_TO_SHEET.keys())]
    # df = df.rename(columns={"äºŒçº§æ ‡ç­¾": "è¯é¢˜ç°‡"})

    # åªä¿ç•™éœ€è¦åˆ—ã€å¹¶æŒ‰æ—¢å®šé¡ºåº
    return df[["è¯é¢˜ç°‡","ä¸€çº§åˆ†ç±»", "å‘è¨€æ—¶é—´", "ç©å®¶ID", "ç©å®¶æ¶ˆæ¯" ]]


########################## å†™å…¥ + æŒ‰äºŒçº§æ ‡ç­¾çƒ­åº¦æ’åº ###########################
def _rewrite_sheet_sorted_by_tag(wb, sheet_name: str, new_rows_df: pd.DataFrame):
    """
    åˆå¹¶å†å²+æ–°å¢åï¼ŒæŒ‰ï¼š
      1) äºŒçº§æ ‡ç­¾ å‡åºï¼ˆç©ºæ ‡ç­¾æ”¾æœ€åï¼‰
      2) å‘è¨€æ—¶é—´ å‡åº
    é‡å†™æ­£æ–‡åŒºåŸŸï¼ˆA2:D*ï¼‰ã€‚
    """
    ws = wb[sheet_name]
    body_style_name = _ensure_named_style(wb)

    # è¯»å–æ—¢æœ‰æ­£æ–‡ï¼ˆA2:D*ï¼‰
    existing = []
    if ws.max_row >= 2:
        for r in ws.iter_rows(min_row=2, max_row=ws.max_row,
                              min_col=1, max_col=len(HEADERS), values_only=True):
            if r is None or all(x in (None, "") for x in r):
                continue
            existing.append(r)

    cols = HEADERS  # ["å‘è¨€æ—¶é—´","ç©å®¶ID","ç©å®¶æ¶ˆæ¯","äºŒçº§æ ‡ç­¾"]
    df_exist = pd.DataFrame(existing, columns=cols) if existing else pd.DataFrame(columns=cols)

    # åˆå¹¶å†å²+æ–°å¢
    df_all = pd.concat([df_exist, new_rows_df[cols]], ignore_index=True)
    if df_all.empty:
        if ws.max_row > 1:
            ws.delete_rows(2, ws.max_row - 1)
        return

    # è§„èŒƒåŒ–ï¼šæ ‡ç­¾å»é¦–å°¾ç©ºç™½ï¼›æ„é€ â€œæ˜¯å¦ç©ºæ ‡ç­¾â€æ ‡è®°ï¼›è§£ææ—¶é—´
    df_all["è¯é¢˜ç°‡"] = df_all["è¯é¢˜ç°‡"].astype(str).str.strip()
    df_all["_tag_blank"] = df_all["è¯é¢˜ç°‡"].eq("") | df_all["è¯é¢˜ç°‡"].isna()
    df_all["_ts"] =pd.to_datetime(df_all["å‘è¨€æ—¶é—´"], format="%Y-%m-%d %H:%M:%S", errors="coerce")

    # å…³é”®æ’åºï¼šéç©ºæ ‡ç­¾åœ¨å‰ -> æ ‡ç­¾å‡åº -> æ—¶é—´å‡åºï¼ˆæ— æ³•è§£æçš„æ—¶é—´æ’åï¼Œé€šè¿‡åŸå­—ç¬¦ä¸²å…œåº•ï¼‰
    df_all = df_all.sort_values(
        by=["_tag_blank", "è¯é¢˜ç°‡", "_ts", "å‘è¨€æ—¶é—´"],
        ascending=[True, True, True, True],
        kind="mergesort"  # ç¨³å®šæ’åº
    ).drop(columns=["_tag_blank", "_ts"])

    # æ¸…æ­£æ–‡å¹¶å†™å›
    if ws.max_row > 1:
        ws.delete_rows(2, ws.max_row - 1)
    for row in df_all.itertuples(index=False, name=None):
        ws.append(row)

    # å¥—æ­£æ–‡æ ·å¼
    if ws.max_row >= 2:
        for r in ws.iter_rows(min_row=2, max_row=ws.max_row,
                              min_col=1, max_col=len(HEADERS)):
            for cell in r:
                cell.style = body_style_name



def append_json_to_excel_by_cat_and_tag(records: List[Dict[str, Any]], excel_path: str):
    """
    ä¸»å…¥å£ï¼š
    - è§£æ/è§„èŒƒåŒ–è¾“å…¥è®°å½•
    - æŒ‰â€œä¸€çº§åˆ†ç±»â€åˆ†å‘åˆ° sheet
    - æ¯ä¸ª sheet å†™å…¥åï¼Œå¯¹â€œæ‰€æœ‰å†…å®¹ï¼ˆå«å†å²+æ–°å¢ï¼‰â€æŒ‰â€œäºŒçº§æ ‡ç­¾â€çƒ­åº¦é™åºé‡æ’
    """
    df = _normalize_records(records)
    if df.empty:
        return

    wb, _ = _open_or_create_excel(excel_path)

    # æŒ‰ä¸€çº§åˆ†ç±»åˆ†ç»„ï¼Œæ¯ä¸ªç»„åœ¨å„è‡ª sheet å†…å®Œæˆâ€œåˆå¹¶ + é‡æ’ + é‡å†™â€
    for ca, grp in df.groupby("ä¸€çº§åˆ†ç±»", sort=False):
        sheet = CA_TO_SHEET.get(int(ca))
        if not sheet or sheet not in wb.sheetnames:
            continue

        # ä»…ä¿ç•™å†™å…¥åˆ—é¡ºåº
        body_df = grp[["è¯é¢˜ç°‡","å‘è¨€æ—¶é—´", "ç©å®¶ID", "ç©å®¶æ¶ˆæ¯" ]].copy()

        _rewrite_sheet_sorted_by_tag(wb, sheet, body_df)

    wb.save(excel_path)





# ----------------------------Excelå¤„ç†---------------------



# ========== åˆ—å·å¸¸é‡ï¼ˆ1-basedï¼Œå¯¹åº”ä½ çš„æ–°è¡¨å¤´é¡ºåºï¼‰ ==========
TOPIC_COL = 1      # A åˆ—ï¼šè¯é¢˜ç°‡
TIME_COL  = 2      # B åˆ—ï¼šå‘è¨€æ—¶é—´
USER_COL  = 3      # C åˆ—ï¼šç©å®¶ID
MSG_COL   = 4      # D åˆ—ï¼šç©å®¶æ¶ˆæ¯

# ========== è¡¨æ ¼æ ·å¼ï¼ˆå¦‚æœä½ éœ€è¦åˆå§‹åŒ–/ä¿è¯æ­£æ–‡æ ·å¼å­˜åœ¨ï¼‰ ==========
FONT_NAME = "å¾®é›…è½¯é»‘"      # æ²¡æœ‰å°±æ¢æˆâ€œå¾®è½¯é›…é»‘â€
BODY_FONT_SIZE = 11

def _ensure_named_style(wb) -> str:
    """ç¡®ä¿æ­£æ–‡ NamedStyle å­˜åœ¨ï¼›è¿”å› style åç§°ã€‚"""
    style_name = "BodyStyle"
    if style_name in wb.named_styles:
        return style_name
    thin = Side(style="thin", color="000000")
    body_style = NamedStyle(name=style_name)
    body_style.font = Font(name=FONT_NAME, size=BODY_FONT_SIZE)
    body_style.alignment = Alignment(vertical="center", wrap_text=True)
    body_style.border = Border(left=thin, right=thin, top=thin, bottom=thin)
    wb.add_named_style(body_style)
    return style_name

# ========== è¯»å‰ï¼šæ‹†å¹¶å¹¶å›å¡«â€œè¯é¢˜ç°‡â€åˆ— ==========
def _pre_unmerge_and_fill_topic(ws):
    to_process = []
    for rng in list(ws.merged_cells.ranges):
        if rng.min_col == TOPIC_COL and rng.max_col == TOPIC_COL:
            to_process.append(rng)
    for rng in to_process:
        top_val = ws.cell(row=rng.min_row, column=TOPIC_COL).value
        for r in range(rng.min_row, rng.max_row + 1):
            ws.cell(row=r, column=TOPIC_COL).value = top_val
        ws.unmerge_cells(range_string=str(rng))

# ========== æ–‡æœ¬è§„èŒƒåŒ–ï¼ˆå»é›¶å®½/å…¨è§’ç©ºæ ¼/å¤šç©ºç™½ï¼‰ ==========
_ZW_RE = re.compile(r'[\u200b-\u200f\u202a-\u202e\u2060-\u206f\ufeff]')
def _norm_topic(s: str) -> str:
    if s is None:
        return ""
    s = unicodedata.normalize("NFKC", str(s))
    s = _ZW_RE.sub("", s).replace("\u3000", " ").strip()
    s = re.sub(r"\s+", " ", s)
    return s

# ========== å½»åº•åˆ é™¤å›¾ç‰‡/å›¾è¡¨/ç»˜å›¾å…³ç³» ==========
def _remove_all_drawings(ws):
    # æ¸…ç©ºå›¾ç‰‡ä¸å›¾è¡¨é›†åˆ
    if hasattr(ws, "_images"):
        ws._images = []
    if hasattr(ws, "_charts"):
        ws._charts = []
    # åˆ é™¤ drawing å…³ç³»ï¼ˆé¿å…æ®‹ç•™ï¼‰
    try:
        rels = getattr(ws, "_rels", None)
        if rels:
            to_del = [rid for rid, rel in rels.items()
                      if "drawing" in getattr(rel, "type", "")]
            for rid in to_del:
                del rels[rid]
    except Exception:
        pass
    # æ¸…ç©º _drawing å¥æŸ„
    if hasattr(ws, "_drawing"):
        ws._drawing = None

# ========== æ¸…æ—§åˆå¹¶ï¼ˆä»…è¯é¢˜ç°‡åˆ—ï¼‰ ==========
def _clear_topic_merges(ws):
    try:
        for rng in list(ws.merged_cells.ranges):
            if rng.min_col == TOPIC_COL and rng.max_col == TOPIC_COL:
                ws.unmerge_cells(str(rng))
    except Exception:
        pass

# ========== çº¯å›¾ç‰‡è¡Œåˆ¤å®š & å»é‡è¡¨æƒ… ==========
_IMG_PAT = re.compile(r'^\s*(?:\[(?:å›¾ç‰‡|è¡¨æƒ…|å›¾åƒ)[^\]]*\]\s*)+$')
def _is_pure_image_msg(text: str) -> bool:
    if text is None:
        return False
    return bool(_IMG_PAT.match(str(text).strip()))

def _strip_trailing_flag(s: str) -> str:
    if s is None:
        return ""
    return re.sub(r'(?:\s*ğŸ–¼ï¸)+$', '', str(s).rstrip())

# ========== åˆ†æ®µï¼šåŒç°‡ä¸”ç›¸é‚»æ—¶é—´é—´éš”â‰¤gapï¼Œè¿”å› df è¡Œå·æ®µ ==========
def _iter_topic_runs(df: pd.DataFrame, gap_minutes: int, nat_policy: str = "skip"):
    """
    df: å¿…é¡»åŒ…å«åˆ— ["å‘è¨€æ—¶é—´","è¯é¢˜ç°‡"]ï¼Œä¸”å·²æŒ‰ã€è¯é¢˜ç°‡â†’æ—¶é—´ã€æ’è¿‡åºã€‚
    è¿”å›: [(start_idx, end_idx, topic_norm), ...] â€”â€” æ³¨æ„æ˜¯ df çš„ 0-based è¡Œå·ã€‚
    """
    def _to_ts(x):
        try:
            return pd.to_datetime(x, errors="coerce")
        except Exception:
            return pd.NaT

    n = len(df)
    if n == 0:
        return []

    segs = []
    start = 0
    topic = str(df.iloc[0]["è¯é¢˜ç°‡"] or "")
    last_ts = _to_ts(df.iloc[0]["å‘è¨€æ—¶é—´"])

    for i in range(1, n):
        cur_topic = str(df.iloc[i]["è¯é¢˜ç°‡"] or "")
        cur_ts = _to_ts(df.iloc[i]["å‘è¨€æ—¶é—´"])

        same_topic = (cur_topic == topic and cur_topic != "")
        cont = False

        if same_topic:
            if pd.notna(cur_ts) and pd.notna(last_ts):
                cont = (cur_ts - last_ts) <= pd.Timedelta(minutes=gap_minutes)
                if cont:
                    last_ts = cur_ts
            elif pd.isna(cur_ts):
                # NaTï¼šskip=å¹¶å…¥ä½†ä¸æ›´æ–°åŸºå‡†ï¼›break=ç›´æ¥æ–­æ®µ
                cont = (nat_policy != "break")
            else:  # last_ts NaT, cur_ts å¯è§£æ
                if nat_policy == "break":
                    cont = False
                else:
                    cont = True
                    last_ts = cur_ts
        else:
            cont = False

        if not cont:
            segs.append((start, i - 1, topic))
            start = i
            topic = cur_topic
            last_ts = cur_ts if pd.notna(cur_ts) else pd.NaT

    segs.append((start, n - 1, topic))
    # è¿‡æ»¤ç©º topic
    return [(s, e, t) for (s, e, t) in segs if (t or "").strip()]

          
# ========== åˆ†æ®µï¼šåŒç°‡ä¸”ç›¸é‚»æ—¶é—´é—´éš”â‰¤gapï¼Œè¿”å› df è¡Œå·æ®µ ==========
def _iter_topic_runs(df: pd.DataFrame, gap_minutes: int, nat_policy: str = "skip"):
    """
    df: å¿…é¡»åŒ…å«åˆ— ["å‘è¨€æ—¶é—´","è¯é¢˜ç°‡"]ï¼Œä¸”å·²æŒ‰ã€è¯é¢˜ç°‡â†’æ—¶é—´ã€æ’è¿‡åºã€‚
    è¿”å›: [(start_idx, end_idx, topic_norm), ...] â€”â€” æ³¨æ„æ˜¯ df çš„ 0-based è¡Œå·ã€‚
    """
    def _to_ts(x):
        try:
            return pd.to_datetime(x, errors="coerce")
        except Exception:
            return pd.NaT

    n = len(df)
    if n == 0:
        return []

    segs = []
    start = 0
    topic = str(df.iloc[0]["è¯é¢˜ç°‡"] or "")
    last_ts = _to_ts(df.iloc[0]["å‘è¨€æ—¶é—´"])

    for i in range(1, n):
        cur_topic = str(df.iloc[i]["è¯é¢˜ç°‡"] or "")
        cur_ts = _to_ts(df.iloc[i]["å‘è¨€æ—¶é—´"])

        same_topic = (cur_topic == topic and cur_topic != "")
        cont = False

        if same_topic:
            if pd.notna(cur_ts) and pd.notna(last_ts):
                cont = (cur_ts - last_ts) <= pd.Timedelta(minutes=gap_minutes)
                if cont:
                    last_ts = cur_ts
            elif pd.isna(cur_ts):
                # NaTï¼šskip=å¹¶å…¥ä½†ä¸æ›´æ–°åŸºå‡†ï¼›break=ç›´æ¥æ–­æ®µ
                cont = (nat_policy != "break")
            else:  # last_ts NaT, cur_ts å¯è§£æ
                if nat_policy == "break":
                    cont = False
                else:
                    cont = True
                    last_ts = cur_ts
        else:
            cont = False

        if not cont:
            segs.append((start, i - 1, topic))
            start = i
            topic = cur_topic
            last_ts = cur_ts if pd.notna(cur_ts) else pd.NaT

    segs.append((start, n - 1, topic))
    # è¿‡æ»¤ç©º topic
    return [(s, e, t) for (s, e, t) in segs if (t or "").strip()]

# ========== æ ¸å¿ƒï¼šæ’åºâ†’é‡å†™â†’åˆ‡æ®µâ†’æ®µå†…æ’åºâ†’æ®µé—´æ’åºâ†’åˆå¹¶â†’æ ‡ğŸ–¼ï¸â†’æ¸…å›¾ ==========
def _sort_merge_flag(
    ws,
    gap_minutes: int = 15,
    nat_policy: str = "skip",          # "skip": NaT å¹¶å…¥ä¸æ›´æ–°åŸºå‡†ï¼›"break": NaT ç›´æ¥æ–­æ®µ
    fill_nat_in_topic: bool = True,    # åŒç°‡å†…å¯¹å°‘é‡ NaT åšå‰åå¡«è¡¥
    dump_bad_ts: bool = False
):
    _pre_unmerge_and_fill_topic(ws)

    # è¯»æ­£æ–‡
    rows = []
    if ws.max_row >= 2:
        for r in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=4, values_only=True):
            if r is None or all(x in (None, "") for x in r):
                continue
            rows.append(r)
    df = pd.DataFrame(rows, columns=["è¯é¢˜ç°‡","å‘è¨€æ—¶é—´","ç©å®¶ID","ç©å®¶æ¶ˆæ¯"])
    if df.empty:
        if ws.max_row > 1:
            ws.delete_rows(2, ws.max_row - 1)
        _clear_topic_merges(ws); _remove_all_drawings(ws)
        return

    # è§„èŒƒ & å¼ºéŸ§è§£ææ—¶é—´
    df["è¯é¢˜ç°‡"] = df["è¯é¢˜ç°‡"].apply(_norm_topic)
    col = df["å‘è¨€æ—¶é—´"].astype(str).str.replace(r"[/.]", "-", regex=True).str.strip()
    ts  = pd.to_datetime(col, errors="coerce")
    bad = ts.isna()
    if bad.any():
        ts2 = pd.to_datetime(df.loc[bad, "å‘è¨€æ—¶é—´"], format="%Y-%m-%d %H:%M:%S", errors="coerce")
        ts.loc[bad] = ts2
    df["_ts"] = ts

    if dump_bad_ts and df["_ts"].isna().any():
        df.loc[df["_ts"].isna(), ["è¯é¢˜ç°‡","å‘è¨€æ—¶é—´","ç©å®¶ID","ç©å®¶æ¶ˆæ¯"]].to_excel(
            f"bad_ts_{ws.title}.xlsx", index=False
        )

    if fill_nat_in_topic:
        def _safe_fill(s: pd.Series) -> pd.Series:
            if s.notna().any():
                return s.ffill().bfill()
            return s
        df["_ts"] = df.groupby("è¯é¢˜ç°‡")["_ts"].transform(_safe_fill)

    # <<< æ–°å¢ï¼šè‹¥åŸâ€œå‘è¨€æ—¶é—´â€ä¸ºç©ºä¸” _ts æœ‰å€¼ï¼Œç”¨ _ts å›å¡«ä¸ºæ ‡å‡†å­—ç¬¦ä¸²ï¼Œé¿å…å†™è¡¨ç©ºç™½
    _time_is_empty = df["å‘è¨€æ—¶é—´"].isna() | (df["å‘è¨€æ—¶é—´"].astype(str).str.strip() == "")
    mask_fill = _time_is_empty & df["_ts"].notna()
    if mask_fill.any():
        df.loc[mask_fill, "å‘è¨€æ—¶é—´"] = df.loc[mask_fill, "_ts"].dt.strftime("%Y-%m-%d %H:%M:%S")

    # <<< æ–°å¢ï¼šç©å®¶æ¶ˆæ¯ NaN â†’ ""ï¼ˆé¿å… openpyxl å†™ None â†’ Excel ç©ºï¼‰
    df["ç©å®¶æ¶ˆæ¯"] = df["ç©å®¶æ¶ˆæ¯"].astype(object).where(df["ç©å®¶æ¶ˆæ¯"].notna(), "")

    # åŸºç¡€æ’åºï¼šè¯é¢˜ç°‡â†’æ—¶é—´ï¼Œä¾¿äºåˆ‡æ®µç¨³å®šï¼ˆåŒç°‡å†…æ—¶é—´å‡åºï¼‰
    df = df.sort_values(by=["è¯é¢˜ç°‡", "_ts", "å‘è¨€æ—¶é—´"],
                        ascending=[True, True, True],
                        kind="mergesort")

    # â‘  åˆ‡æ®µ
    runs = _iter_topic_runs(df[["å‘è¨€æ—¶é—´","è¯é¢˜ç°‡"]].copy(), gap_minutes, nat_policy)

    # â‘¡ æ®µå†…æ’åºï¼›â‘¢ æ®µä¸æ®µä¹‹é—´æŒ‰â€œè¯é¢˜ç°‡ + æ®µé¦–æ—¶é—´â€æ’åºï¼ˆç¡®ä¿åŒç°‡æ•´ä½“æ—¶é—´å‡åºï¼‰
    parts = []
    run_meta = []   # <<< ä¿®æ”¹ï¼š[(idx_in_parts, topic, start_ts)]
    for (s, e, t) in runs:
        seg = df.iloc[s:e+1].copy()
        seg = seg.sort_values(by=["_ts","å‘è¨€æ—¶é—´"], ascending=[True, True], kind="mergesort")
        parts.append(seg)
        seg_ts = pd.to_datetime(seg["å‘è¨€æ—¶é—´"], errors="coerce")
        start_ts = seg_ts.iloc[0] if not seg_ts.empty else pd.NaT
        run_meta.append((len(parts)-1, t, start_ts))  # <<< è®°å½•è¯é¢˜ç°‡

    # <<< ä¿®æ”¹ï¼šæ®µé—´æ’åºé”® (topic, isNaT, start_ts)ï¼›NaT æ®µæ”¾æœ€å
    run_meta.sort(key=lambda x: (x[1], pd.isna(x[2]), x[2]))
    df_out = pd.concat([parts[i] for (i, _topic, _ts0) in run_meta], ignore_index=True)

    # é‡å†™ Excelï¼ˆæŒ‰é‡æ’åçš„é¡ºåºï¼‰
    if ws.max_row > 1:
        ws.delete_rows(2, ws.max_row - 1)
    style = _ensure_named_style(ws.parent)
    for row in df_out.drop(columns=["_ts"]).itertuples(index=False, name=None):
        ws.append(row)
    for rr in ws.iter_rows(min_row=2, max_row=ws.max_row, min_col=1, max_col=4):
        for cell in rr:
            cell.style = style

    # åŸºäºæ–°é¡ºåºå†åˆ‡ä¸€æ¬¡æ®µ â†’ ç”¨äºåˆå¹¶ä¸æ ‡è®°
    runs_new = _iter_topic_runs(
        pd.DataFrame({
            "å‘è¨€æ—¶é—´": [ws.cell(row=r, column=TIME_COL).value  for r in range(2, ws.max_row+1)],
            "è¯é¢˜ç°‡":   [ws.cell(row=r, column=TOPIC_COL).value  for r in range(2, ws.max_row+1)],
        }),
        gap_minutes, nat_policy
    )

    # åˆå¹¶ A åˆ—ï¼ˆè¯é¢˜ç°‡ï¼‰
    _clear_topic_merges(ws)
    for (s, e, _t) in runs_new:
        r1, r2 = s + 2, e + 2
        if r2 > r1:
            ws.merge_cells(start_row=r1, start_column=TOPIC_COL, end_row=r2, end_column=TOPIC_COL)

    # æ ‡è®° ğŸ–¼ï¸ï¼šæ®µå†…å‡ºç°çº¯å›¾ç‰‡è¡Œ â†’ D åˆ—å°¾éƒ¨ + æ®µé¦– A åˆ— + ğŸ–¼ï¸
    for (s, e, _t) in runs_new:
        r1, r2 = s + 2, e + 2
        has_img_only = False
        for r in range(r1, r2 + 1):
            msg = ws.cell(row=r, column=MSG_COL).value
            if _is_pure_image_msg(msg):
                has_img_only = True
                ws.cell(row=r, column=MSG_COL).value = _strip_trailing_flag(str(msg)) + " ğŸ–¼ï¸"  # <<< str()
        if has_img_only:
            tval = ws.cell(row=r1, column=TOPIC_COL).value
            ws.cell(row=r1, column=TOPIC_COL).value = _strip_trailing_flag(str(tval)) + " ğŸ–¼ï¸"   # <<< str()

    # ç§»é™¤æ‰€æœ‰ shapeï¼ˆå°å›¾ç‰‡å›¾æ ‡ï¼‰
    _remove_all_drawings(ws)



# ========== å…¥å£ï¼šå¤„ç†æ•´ä¸ªå·¥ä½œç°¿ ==========
def postprocess_excel_by_topic(excel_path: str, gap_minutes: int = 15, nat_policy: str = "skip"):
    wb = load_workbook(excel_path, data_only=True)
    _ensure_named_style(wb)
    for name in wb.sheetnames:
        _sort_merge_flag(wb[name], gap_minutes=gap_minutes, nat_policy=nat_policy)
    wb.save(excel_path)
    print(f"âœ… å·²å®Œæˆåå¤„ç†ï¼š{excel_path}ï¼ˆgap={gap_minutes}min, NaTç­–ç•¥={nat_policy}ï¼‰")

